2023-07-22 00:41:23,031:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 00:41:24,793:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 00:41:24,793:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 00:41:24,793:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 00:41:24,793:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 00:41:24,810:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 00:41:24,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 00:41:24,814:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 00:41:25,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning:

This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0


2023-07-22 00:41:25,729:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning:

'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680


2023-07-22 01:03:14,274:INFO:PyCaret ClassificationExperiment
2023-07-22 01:03:14,274:INFO:Logging name: balanceado
2023-07-22 01:03:14,274:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:03:14,274:INFO:version 3.0.4
2023-07-22 01:03:14,274:INFO:Initializing setup()
2023-07-22 01:03:14,274:INFO:self.USI: 3b3c
2023-07-22 01:03:14,274:INFO:self._variable_keys: {'X_train', 'exp_name_log', 'memory', 'gpu_n_jobs_param', 'html_param', 'target_param', 'gpu_param', 'log_plots_param', 'idx', 'is_multiclass', 'pipeline', '_ml_usecase', 'fold_shuffle_param', 'n_jobs_param', '_available_plots', 'X_test', 'fold_generator', 'USI', 'data', 'X', 'exp_id', 'y', 'fix_imbalance', 'logging_param', 'y_train', 'fold_groups_param', 'seed', 'y_test'}
2023-07-22 01:03:14,274:INFO:Checking environment
2023-07-22 01:03:14,274:INFO:python_version: 3.10.8
2023-07-22 01:03:14,274:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:03:14,275:INFO:machine: AMD64
2023-07-22 01:03:14,275:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:03:14,282:INFO:Memory: svmem(total=16505966592, available=3937013760, percent=76.1, used=12568952832, free=3937013760)
2023-07-22 01:03:14,282:INFO:Physical Core: 6
2023-07-22 01:03:14,282:INFO:Logical Core: 12
2023-07-22 01:03:14,282:INFO:Checking libraries
2023-07-22 01:03:14,282:INFO:System:
2023-07-22 01:03:14,282:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:03:14,282:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:03:14,282:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:03:14,282:INFO:PyCaret required dependencies:
2023-07-22 01:03:14,282:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 01:03:14,285:INFO:                 pip: 22.2.2
2023-07-22 01:03:14,285:INFO:          setuptools: 63.2.0
2023-07-22 01:03:14,285:INFO:             pycaret: 3.0.4
2023-07-22 01:03:14,285:INFO:             IPython: 8.11.0
2023-07-22 01:03:14,285:INFO:          ipywidgets: 8.0.7
2023-07-22 01:03:14,285:INFO:                tqdm: 4.64.1
2023-07-22 01:03:14,285:INFO:               numpy: 1.23.5
2023-07-22 01:03:14,285:INFO:              pandas: 1.5.3
2023-07-22 01:03:14,285:INFO:              jinja2: 3.1.2
2023-07-22 01:03:14,285:INFO:               scipy: 1.9.3
2023-07-22 01:03:14,285:INFO:              joblib: 1.2.0
2023-07-22 01:03:14,285:INFO:             sklearn: 1.2.2
2023-07-22 01:03:14,285:INFO:                pyod: 1.1.0
2023-07-22 01:03:14,285:INFO:            imblearn: 0.10.1
2023-07-22 01:03:14,285:INFO:   category_encoders: 2.6.1
2023-07-22 01:03:14,285:INFO:            lightgbm: 3.3.5
2023-07-22 01:03:14,285:INFO:               numba: 0.57.0
2023-07-22 01:03:14,285:INFO:            requests: 2.28.2
2023-07-22 01:03:14,285:INFO:          matplotlib: 3.7.1
2023-07-22 01:03:14,286:INFO:          scikitplot: 0.3.7
2023-07-22 01:03:14,286:INFO:         yellowbrick: 1.5
2023-07-22 01:03:14,286:INFO:              plotly: 5.15.0
2023-07-22 01:03:14,286:INFO:    plotly-resampler: Not installed
2023-07-22 01:03:14,286:INFO:             kaleido: 0.2.1
2023-07-22 01:03:14,286:INFO:           schemdraw: 0.15
2023-07-22 01:03:14,286:INFO:         statsmodels: 0.13.5
2023-07-22 01:03:14,286:INFO:              sktime: 0.21.0
2023-07-22 01:03:14,286:INFO:               tbats: 1.1.3
2023-07-22 01:03:14,286:INFO:            pmdarima: 2.0.3
2023-07-22 01:03:14,286:INFO:              psutil: 5.9.4
2023-07-22 01:03:14,286:INFO:          markupsafe: 2.1.2
2023-07-22 01:03:14,286:INFO:             pickle5: Not installed
2023-07-22 01:03:14,286:INFO:         cloudpickle: 2.2.1
2023-07-22 01:03:14,286:INFO:         deprecation: 2.1.0
2023-07-22 01:03:14,286:INFO:              xxhash: 3.2.0
2023-07-22 01:03:14,286:INFO:           wurlitzer: Not installed
2023-07-22 01:03:14,286:INFO:PyCaret optional dependencies:
2023-07-22 01:03:15,464:INFO:                shap: 0.41.0
2023-07-22 01:03:15,464:INFO:           interpret: 0.4.2
2023-07-22 01:03:15,464:INFO:                umap: 0.5.3
2023-07-22 01:03:15,464:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:03:15,464:INFO:  explainerdashboard: Not installed
2023-07-22 01:03:15,464:INFO:             autoviz: Not installed
2023-07-22 01:03:15,464:INFO:           fairlearn: Not installed
2023-07-22 01:03:15,464:INFO:          deepchecks: Not installed
2023-07-22 01:03:15,464:INFO:             xgboost: 1.7.6
2023-07-22 01:03:15,464:INFO:            catboost: Not installed
2023-07-22 01:03:15,464:INFO:              kmodes: Not installed
2023-07-22 01:03:15,464:INFO:             mlxtend: Not installed
2023-07-22 01:03:15,464:INFO:       statsforecast: Not installed
2023-07-22 01:03:15,464:INFO:        tune_sklearn: Not installed
2023-07-22 01:03:15,464:INFO:                 ray: Not installed
2023-07-22 01:03:15,464:INFO:            hyperopt: Not installed
2023-07-22 01:03:15,464:INFO:              optuna: 3.2.0
2023-07-22 01:03:15,464:INFO:               skopt: Not installed
2023-07-22 01:03:15,464:INFO:              mlflow: 2.4.2
2023-07-22 01:03:15,464:INFO:              gradio: Not installed
2023-07-22 01:03:15,464:INFO:             fastapi: 0.95.2
2023-07-22 01:03:15,464:INFO:             uvicorn: 0.22.0
2023-07-22 01:03:15,464:INFO:              m2cgen: Not installed
2023-07-22 01:03:15,464:INFO:           evidently: Not installed
2023-07-22 01:03:15,464:INFO:               fugue: Not installed
2023-07-22 01:03:15,464:INFO:           streamlit: Not installed
2023-07-22 01:03:15,465:INFO:             prophet: Not installed
2023-07-22 01:03:15,465:INFO:None
2023-07-22 01:03:15,465:INFO:Set up GPU usage.
2023-07-22 01:03:15,465:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:15,465:WARNING:cuML is outdated or not found. Required version is >=22.10.
                Please visit https://rapids.ai/ for installation instructions.
2023-07-22 01:03:15,465:INFO:Set up data.
2023-07-22 01:03:15,486:INFO:Set up train/test split.
2023-07-22 01:03:15,486:INFO:Set up data.
2023-07-22 01:03:15,504:INFO:Set up index.
2023-07-22 01:03:15,504:INFO:Set up folding strategy.
2023-07-22 01:03:15,505:INFO:Assigning column types.
2023-07-22 01:03:15,513:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:03:15,514:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:15,559:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:03:15,559:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:15,561:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:15,562:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:03:15,562:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:15,593:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:15,598:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:15,601:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:03:20,179:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:03:20,180:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,242:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:03:20,242:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,242:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,243:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:03:20,243:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,272:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,279:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:03:20,555:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:03:20,556:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:03:20,556:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,615:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,616:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,616:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:03:20,617:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,647:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,653:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,654:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:03:20,786:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:03:20,788:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,849:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,849:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,850:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:03:20,850:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,880:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,886:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:20,888:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:03:21,015:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:03:21,016:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:03:21,016:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,076:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,077:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,078:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,108:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,114:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,115:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:03:21,246:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:03:21,247:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,307:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,307:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,308:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,337:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,343:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,344:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:03:21,472:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:03:21,477:INFO:Preparing preprocessing pipeline...
2023-07-22 01:03:21,481:INFO:Set up simple imputation.
2023-07-22 01:03:21,490:INFO:Set up encoding of categorical features.
2023-07-22 01:03:21,490:INFO:Set up imbalanced handling.
2023-07-22 01:03:21,490:INFO:Set up column transformation.
2023-07-22 01:03:21,490:INFO:Set up feature selection.
2023-07-22 01:03:21,490:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,550:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,550:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,551:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,580:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,586:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:03:21,587:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:03:21,717:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:29,234:INFO:PyCaret ClassificationExperiment
2023-07-22 01:04:29,234:INFO:Logging name: balanceado
2023-07-22 01:04:29,234:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:04:29,234:INFO:version 3.0.4
2023-07-22 01:04:29,234:INFO:Initializing setup()
2023-07-22 01:04:29,234:INFO:self.USI: 14ee
2023-07-22 01:04:29,234:INFO:self._variable_keys: {'X_train', 'exp_name_log', 'memory', 'gpu_n_jobs_param', 'html_param', 'target_param', 'gpu_param', 'log_plots_param', 'idx', 'is_multiclass', 'pipeline', '_ml_usecase', 'fold_shuffle_param', 'n_jobs_param', '_available_plots', 'X_test', 'fold_generator', 'USI', 'data', 'X', 'exp_id', 'y', 'fix_imbalance', 'logging_param', 'y_train', 'fold_groups_param', 'seed', 'y_test'}
2023-07-22 01:04:29,234:INFO:Checking environment
2023-07-22 01:04:29,234:INFO:python_version: 3.10.8
2023-07-22 01:04:29,234:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:04:29,234:INFO:machine: AMD64
2023-07-22 01:04:29,234:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:04:29,241:INFO:Memory: svmem(total=16505966592, available=3828027392, percent=76.8, used=12677939200, free=3828027392)
2023-07-22 01:04:29,241:INFO:Physical Core: 6
2023-07-22 01:04:29,241:INFO:Logical Core: 12
2023-07-22 01:04:29,241:INFO:Checking libraries
2023-07-22 01:04:29,241:INFO:System:
2023-07-22 01:04:29,241:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:04:29,241:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:04:29,241:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:04:29,241:INFO:PyCaret required dependencies:
2023-07-22 01:04:29,242:INFO:                 pip: 22.2.2
2023-07-22 01:04:29,242:INFO:          setuptools: 63.2.0
2023-07-22 01:04:29,242:INFO:             pycaret: 3.0.4
2023-07-22 01:04:29,242:INFO:             IPython: 8.11.0
2023-07-22 01:04:29,242:INFO:          ipywidgets: 8.0.7
2023-07-22 01:04:29,242:INFO:                tqdm: 4.64.1
2023-07-22 01:04:29,242:INFO:               numpy: 1.23.5
2023-07-22 01:04:29,242:INFO:              pandas: 1.5.3
2023-07-22 01:04:29,242:INFO:              jinja2: 3.1.2
2023-07-22 01:04:29,242:INFO:               scipy: 1.9.3
2023-07-22 01:04:29,243:INFO:              joblib: 1.2.0
2023-07-22 01:04:29,243:INFO:             sklearn: 1.2.2
2023-07-22 01:04:29,243:INFO:                pyod: 1.1.0
2023-07-22 01:04:29,243:INFO:            imblearn: 0.10.1
2023-07-22 01:04:29,243:INFO:   category_encoders: 2.6.1
2023-07-22 01:04:29,243:INFO:            lightgbm: 3.3.5
2023-07-22 01:04:29,243:INFO:               numba: 0.57.0
2023-07-22 01:04:29,243:INFO:            requests: 2.28.2
2023-07-22 01:04:29,243:INFO:          matplotlib: 3.7.1
2023-07-22 01:04:29,243:INFO:          scikitplot: 0.3.7
2023-07-22 01:04:29,243:INFO:         yellowbrick: 1.5
2023-07-22 01:04:29,243:INFO:              plotly: 5.15.0
2023-07-22 01:04:29,243:INFO:    plotly-resampler: Not installed
2023-07-22 01:04:29,243:INFO:             kaleido: 0.2.1
2023-07-22 01:04:29,243:INFO:           schemdraw: 0.15
2023-07-22 01:04:29,243:INFO:         statsmodels: 0.13.5
2023-07-22 01:04:29,243:INFO:              sktime: 0.21.0
2023-07-22 01:04:29,243:INFO:               tbats: 1.1.3
2023-07-22 01:04:29,243:INFO:            pmdarima: 2.0.3
2023-07-22 01:04:29,243:INFO:              psutil: 5.9.4
2023-07-22 01:04:29,243:INFO:          markupsafe: 2.1.2
2023-07-22 01:04:29,243:INFO:             pickle5: Not installed
2023-07-22 01:04:29,243:INFO:         cloudpickle: 2.2.1
2023-07-22 01:04:29,243:INFO:         deprecation: 2.1.0
2023-07-22 01:04:29,243:INFO:              xxhash: 3.2.0
2023-07-22 01:04:29,243:INFO:           wurlitzer: Not installed
2023-07-22 01:04:29,243:INFO:PyCaret optional dependencies:
2023-07-22 01:04:29,243:INFO:                shap: 0.41.0
2023-07-22 01:04:29,243:INFO:           interpret: 0.4.2
2023-07-22 01:04:29,243:INFO:                umap: 0.5.3
2023-07-22 01:04:29,243:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:04:29,243:INFO:  explainerdashboard: Not installed
2023-07-22 01:04:29,243:INFO:             autoviz: Not installed
2023-07-22 01:04:29,243:INFO:           fairlearn: Not installed
2023-07-22 01:04:29,243:INFO:          deepchecks: Not installed
2023-07-22 01:04:29,244:INFO:             xgboost: 1.7.6
2023-07-22 01:04:29,244:INFO:            catboost: Not installed
2023-07-22 01:04:29,244:INFO:              kmodes: Not installed
2023-07-22 01:04:29,244:INFO:             mlxtend: Not installed
2023-07-22 01:04:29,244:INFO:       statsforecast: Not installed
2023-07-22 01:04:29,244:INFO:        tune_sklearn: Not installed
2023-07-22 01:04:29,244:INFO:                 ray: Not installed
2023-07-22 01:04:29,244:INFO:            hyperopt: Not installed
2023-07-22 01:04:29,244:INFO:              optuna: 3.2.0
2023-07-22 01:04:29,244:INFO:               skopt: Not installed
2023-07-22 01:04:29,244:INFO:              mlflow: 2.4.2
2023-07-22 01:04:29,244:INFO:              gradio: Not installed
2023-07-22 01:04:29,245:INFO:             fastapi: 0.95.2
2023-07-22 01:04:29,245:INFO:             uvicorn: 0.22.0
2023-07-22 01:04:29,245:INFO:              m2cgen: Not installed
2023-07-22 01:04:29,245:INFO:           evidently: Not installed
2023-07-22 01:04:29,245:INFO:               fugue: Not installed
2023-07-22 01:04:29,245:INFO:           streamlit: Not installed
2023-07-22 01:04:29,245:INFO:             prophet: Not installed
2023-07-22 01:04:29,245:INFO:None
2023-07-22 01:04:29,245:INFO:Set up GPU usage.
2023-07-22 01:04:29,245:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,245:WARNING:cuML is outdated or not found. Required version is >=22.10.
                Please visit https://rapids.ai/ for installation instructions.
2023-07-22 01:04:29,245:INFO:Set up data.
2023-07-22 01:04:29,267:INFO:Set up train/test split.
2023-07-22 01:04:29,267:INFO:Set up data.
2023-07-22 01:04:29,285:INFO:Set up index.
2023-07-22 01:04:29,286:INFO:Set up folding strategy.
2023-07-22 01:04:29,286:INFO:Assigning column types.
2023-07-22 01:04:29,293:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:04:29,293:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,339:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:04:29,339:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,339:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,340:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:04:29,340:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,362:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,368:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,369:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:29,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:29,558:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,633:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:04:29,633:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,633:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,634:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:04:29,634:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,666:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,672:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,673:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:29,827:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:29,828:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:04:29,828:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,895:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,895:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,895:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:04:29,895:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,931:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,937:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:29,938:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:30,104:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:30,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,165:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,165:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,166:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:04:30,166:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,197:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,203:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,204:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:30,452:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:30,452:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:04:30,453:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,520:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,553:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,560:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,562:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:30,714:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:30,715:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,778:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,808:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,814:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,817:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:30,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:30,956:INFO:Preparing preprocessing pipeline...
2023-07-22 01:04:30,958:INFO:Set up iterative imputation.
2023-07-22 01:04:30,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,958:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:04:30,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,964:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:04:30,964:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:30,971:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:04:30,971:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:31,106:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:04:31,106:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:31,156:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:31,156:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:31,157:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:35,147:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:35,148:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,207:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,208:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,208:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,238:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,243:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,245:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:35,385:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:35,422:INFO:Set up encoding of categorical features.
2023-07-22 01:04:35,422:INFO:Set up imbalanced handling.
2023-07-22 01:04:35,422:INFO:Set up column transformation.
2023-07-22 01:04:35,422:INFO:Set up feature selection.
2023-07-22 01:04:35,422:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,482:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,482:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,483:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,506:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,511:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:04:35,512:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:35,641:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:35,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:49,739:INFO:PyCaret ClassificationExperiment
2023-07-22 01:04:49,739:INFO:Logging name: balanceado
2023-07-22 01:04:49,740:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:04:49,740:INFO:version 3.0.4
2023-07-22 01:04:49,740:INFO:Initializing setup()
2023-07-22 01:04:49,740:INFO:self.USI: f7cb
2023-07-22 01:04:49,740:INFO:self._variable_keys: {'X_train', 'exp_name_log', 'memory', 'gpu_n_jobs_param', 'html_param', 'target_param', 'gpu_param', 'log_plots_param', 'idx', 'is_multiclass', 'pipeline', '_ml_usecase', 'fold_shuffle_param', 'n_jobs_param', '_available_plots', 'X_test', 'fold_generator', 'USI', 'data', 'X', 'exp_id', 'y', 'fix_imbalance', 'logging_param', 'y_train', 'fold_groups_param', 'seed', 'y_test'}
2023-07-22 01:04:49,740:INFO:Checking environment
2023-07-22 01:04:49,740:INFO:python_version: 3.10.8
2023-07-22 01:04:49,740:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:04:49,740:INFO:machine: AMD64
2023-07-22 01:04:49,740:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:04:49,747:INFO:Memory: svmem(total=16505966592, available=3881644032, percent=76.5, used=12624322560, free=3881644032)
2023-07-22 01:04:49,747:INFO:Physical Core: 6
2023-07-22 01:04:49,747:INFO:Logical Core: 12
2023-07-22 01:04:49,747:INFO:Checking libraries
2023-07-22 01:04:49,747:INFO:System:
2023-07-22 01:04:49,747:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:04:49,747:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:04:49,747:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:04:49,747:INFO:PyCaret required dependencies:
2023-07-22 01:04:49,747:INFO:                 pip: 22.2.2
2023-07-22 01:04:49,747:INFO:          setuptools: 63.2.0
2023-07-22 01:04:49,747:INFO:             pycaret: 3.0.4
2023-07-22 01:04:49,747:INFO:             IPython: 8.11.0
2023-07-22 01:04:49,747:INFO:          ipywidgets: 8.0.7
2023-07-22 01:04:49,747:INFO:                tqdm: 4.64.1
2023-07-22 01:04:49,747:INFO:               numpy: 1.23.5
2023-07-22 01:04:49,747:INFO:              pandas: 1.5.3
2023-07-22 01:04:49,747:INFO:              jinja2: 3.1.2
2023-07-22 01:04:49,748:INFO:               scipy: 1.9.3
2023-07-22 01:04:49,748:INFO:              joblib: 1.2.0
2023-07-22 01:04:49,748:INFO:             sklearn: 1.2.2
2023-07-22 01:04:49,748:INFO:                pyod: 1.1.0
2023-07-22 01:04:49,748:INFO:            imblearn: 0.10.1
2023-07-22 01:04:49,748:INFO:   category_encoders: 2.6.1
2023-07-22 01:04:49,748:INFO:            lightgbm: 3.3.5
2023-07-22 01:04:49,748:INFO:               numba: 0.57.0
2023-07-22 01:04:49,748:INFO:            requests: 2.28.2
2023-07-22 01:04:49,748:INFO:          matplotlib: 3.7.1
2023-07-22 01:04:49,748:INFO:          scikitplot: 0.3.7
2023-07-22 01:04:49,748:INFO:         yellowbrick: 1.5
2023-07-22 01:04:49,748:INFO:              plotly: 5.15.0
2023-07-22 01:04:49,748:INFO:    plotly-resampler: Not installed
2023-07-22 01:04:49,748:INFO:             kaleido: 0.2.1
2023-07-22 01:04:49,748:INFO:           schemdraw: 0.15
2023-07-22 01:04:49,748:INFO:         statsmodels: 0.13.5
2023-07-22 01:04:49,748:INFO:              sktime: 0.21.0
2023-07-22 01:04:49,748:INFO:               tbats: 1.1.3
2023-07-22 01:04:49,748:INFO:            pmdarima: 2.0.3
2023-07-22 01:04:49,748:INFO:              psutil: 5.9.4
2023-07-22 01:04:49,748:INFO:          markupsafe: 2.1.2
2023-07-22 01:04:49,748:INFO:             pickle5: Not installed
2023-07-22 01:04:49,748:INFO:         cloudpickle: 2.2.1
2023-07-22 01:04:49,748:INFO:         deprecation: 2.1.0
2023-07-22 01:04:49,748:INFO:              xxhash: 3.2.0
2023-07-22 01:04:49,748:INFO:           wurlitzer: Not installed
2023-07-22 01:04:49,748:INFO:PyCaret optional dependencies:
2023-07-22 01:04:49,749:INFO:                shap: 0.41.0
2023-07-22 01:04:49,749:INFO:           interpret: 0.4.2
2023-07-22 01:04:49,749:INFO:                umap: 0.5.3
2023-07-22 01:04:49,749:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:04:49,749:INFO:  explainerdashboard: Not installed
2023-07-22 01:04:49,749:INFO:             autoviz: Not installed
2023-07-22 01:04:49,749:INFO:           fairlearn: Not installed
2023-07-22 01:04:49,749:INFO:          deepchecks: Not installed
2023-07-22 01:04:49,749:INFO:             xgboost: 1.7.6
2023-07-22 01:04:49,749:INFO:            catboost: Not installed
2023-07-22 01:04:49,749:INFO:              kmodes: Not installed
2023-07-22 01:04:49,749:INFO:             mlxtend: Not installed
2023-07-22 01:04:49,749:INFO:       statsforecast: Not installed
2023-07-22 01:04:49,749:INFO:        tune_sklearn: Not installed
2023-07-22 01:04:49,749:INFO:                 ray: Not installed
2023-07-22 01:04:49,749:INFO:            hyperopt: Not installed
2023-07-22 01:04:49,749:INFO:              optuna: 3.2.0
2023-07-22 01:04:49,749:INFO:               skopt: Not installed
2023-07-22 01:04:49,749:INFO:              mlflow: 2.4.2
2023-07-22 01:04:49,749:INFO:              gradio: Not installed
2023-07-22 01:04:49,749:INFO:             fastapi: 0.95.2
2023-07-22 01:04:49,749:INFO:             uvicorn: 0.22.0
2023-07-22 01:04:49,749:INFO:              m2cgen: Not installed
2023-07-22 01:04:49,749:INFO:           evidently: Not installed
2023-07-22 01:04:49,749:INFO:               fugue: Not installed
2023-07-22 01:04:49,749:INFO:           streamlit: Not installed
2023-07-22 01:04:49,749:INFO:             prophet: Not installed
2023-07-22 01:04:49,749:INFO:None
2023-07-22 01:04:49,750:INFO:Set up data.
2023-07-22 01:04:49,775:INFO:Set up train/test split.
2023-07-22 01:04:49,775:INFO:Set up data.
2023-07-22 01:04:49,794:INFO:Set up index.
2023-07-22 01:04:49,794:INFO:Set up folding strategy.
2023-07-22 01:04:49,794:INFO:Assigning column types.
2023-07-22 01:04:49,802:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:04:49,851:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:04:49,852:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:04:49,885:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:49,889:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:49,951:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:04:49,951:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:04:49,989:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:49,992:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:49,992:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:04:50,050:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:04:50,079:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:50,081:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:50,130:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:04:50,159:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:50,161:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:50,162:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:04:50,240:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:50,243:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:50,324:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:50,327:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:50,328:INFO:Preparing preprocessing pipeline...
2023-07-22 01:04:50,330:INFO:Set up iterative imputation.
2023-07-22 01:04:50,330:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:04:50,336:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:04:50,341:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:04:50,405:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:04:50,451:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:50,454:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:50,534:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:50,536:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:50,570:INFO:Set up encoding of categorical features.
2023-07-22 01:04:50,570:INFO:Set up imbalanced handling.
2023-07-22 01:04:50,571:INFO:Set up column transformation.
2023-07-22 01:04:50,571:INFO:Set up feature selection.
2023-07-22 01:04:50,649:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:04:50,652:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:04:50,691:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:50,962:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:51,180:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:51,376:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:51,599:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:51,799:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:51,991:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:52,192:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:52,394:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:52,577:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:52,740:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:52,916:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:53,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:53,249:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:53,436:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:53,610:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:53,786:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:53,933:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:54,142:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:54,319:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:54,507:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:54,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:54,860:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:55,051:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:55,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:55,366:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:55,566:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:55,763:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:55,957:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:56,146:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:56,317:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:56,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:56,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:56,822:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:57,015:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-22 01:04:57,187:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning:

[IterativeImputer] Early stopping criterion not reached.


2023-07-22 01:04:58,882:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning:

overflow encountered in multiply


2023-07-22 01:04:59,313:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:04:59,341:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:04:59,341:INFO:Creating final display dataframe.
2023-07-22 01:05:01,609:INFO:Setup _display_container:                         Description  \
0                        Session id   
1                            Target   
2                       Target type   
3               Original data shape   
4            Transformed data shape   
5       Transformed train set shape   
6        Transformed test set shape   
7                   Ignore features   
8                  Numeric features   
9              Categorical features   
10         Rows with missing values   
11                       Preprocess   
12                  Imputation type   
13  Iterative imputation iterations   
14        Numeric iterative imputer   
15    Categorical iterative imputer   
16         Maximum one-hot encoding   
17                  Encoding method   
18                    Fix imbalance   
19             Fix imbalance method   
20                   Transformation   
21            Transformation method   
22                Feature selection   
23         Feature selection method   
24      Feature selection estimator   
25      Number of features selected   
26                   Fold Generator   
27                      Fold Number   
28                         CPU Jobs   
29                          Use GPU   
30                   Log Experiment   
31                  Experiment Name   
32                              USI   

                                                Value  
0                                                  42  
1                                        credit_score  
2                                              Binary  
3                                         (12500, 22)  
4                                         (16736, 21)  
5                                         (14236, 21)  
6                                          (2500, 21)  
7                                                   1  
8                                                  17  
9                                                   3  
10                                              36.1%  
11                                               True  
12                                          iterative  
13                                                  5  
14                                           lightgbm  
15                                           lightgbm  
16                                                 25  
17                                               None  
18                                               True  
19  SMOTE(k_neighbors=5, n_jobs=None, random_state...  
20                                               True  
21                                        yeo-johnson  
22                                               True  
23                                            classic  
24                                           lightgbm  
25                                                 20  
26                                              KFold  
27                                                 10  
28                                                 -1  
29                                              False  
30                                       MlflowLogger  
31                                         balanceado  
32                                               f7cb  
2023-07-22 01:05:01,717:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:05:01,720:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:05:01,797:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:05:01,800:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:05:01,800:INFO:Logging experiment in loggers
2023-07-22 01:05:02,224:INFO:SubProcess save_model() called ==================================
2023-07-22 01:05:02,289:INFO:Initializing save_model()
2023-07-22 01:05:02,289:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), model_name=C:\TEMP\tmpcpp5r2b7\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2023-07-22 01:05:02,289:INFO:Adding model into prep_pipe
2023-07-22 01:05:02,289:WARNING:Only Model saved as it was a pipeline.
2023-07-22 01:05:02,485:INFO:C:\TEMP\tmpcpp5r2b7\Transformation Pipeline.pkl saved in current working directory
2023-07-22 01:05:02,514:INFO:Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:05:02,514:INFO:save_model() successfully completed......................................
2023-07-22 01:05:02,714:INFO:SubProcess save_model() end ==================================
2023-07-22 01:05:02,830:INFO:setup() successfully completed in 12.06s...............
2023-07-22 01:11:45,571:INFO:Initializing compare_models()
2023-07-22 01:11:45,571:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000240683CBFD0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000240683CBFD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:11:45,571:INFO:Checking exceptions
2023-07-22 01:11:45,580:INFO:Preparing display monitor
2023-07-22 01:11:45,609:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\generic.py:831: DeprecationWarning:

invalid escape sequence '\P'


2023-07-22 01:13:30,017:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x2407078cfa0>


2023-07-22 01:13:30,041:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x2407078c280>


2023-07-22 01:13:30,042:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\IPython\utils\_process_win32.py:124: ResourceWarning:

unclosed file <_io.BufferedWriter name=6>


2023-07-22 01:13:30,042:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\IPython\utils\_process_win32.py:124: ResourceWarning:

unclosed file <_io.BufferedReader name=7>


2023-07-22 01:13:30,042:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\IPython\utils\_process_win32.py:124: ResourceWarning:

unclosed file <_io.BufferedReader name=8>


2023-07-22 01:13:43,112:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x2407078c580>


2023-07-22 01:13:43,115:WARNING:C:\Program Files\Python310\lib\threading.py:957: ResourceWarning:

unclosed file <_io.BufferedWriter name=6>


2023-07-22 01:13:43,116:WARNING:C:\Program Files\Python310\lib\threading.py:957: ResourceWarning:

unclosed file <_io.BufferedReader name=7>


2023-07-22 01:13:43,117:WARNING:C:\Program Files\Python310\lib\threading.py:957: ResourceWarning:

unclosed file <_io.BufferedReader name=8>


2023-07-22 01:13:43,117:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x2407078c100>


2023-07-22 01:13:51,161:INFO:Initializing compare_models()
2023-07-22 01:13:51,161:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000240683CBFD0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000240683CBFD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:13:51,162:INFO:Checking exceptions
2023-07-22 01:13:51,168:INFO:Preparing display monitor
2023-07-22 01:14:52,362:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x240708202e0>


2023-07-22 01:14:52,365:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x24070820340>


2023-07-22 01:14:52,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\IPython\utils\_process_win32.py:124: ResourceWarning:

unclosed file <_io.BufferedWriter name=6>


2023-07-22 01:14:52,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\IPython\utils\_process_win32.py:124: ResourceWarning:

unclosed file <_io.BufferedReader name=7>


2023-07-22 01:14:52,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\IPython\utils\_process_win32.py:124: ResourceWarning:

unclosed file <_io.BufferedReader name=8>


2023-07-22 01:14:56,425:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x24070820460>


2023-07-22 01:15:28,287:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x24070820160>


2023-07-22 01:15:42,957:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x240708206a0>


2023-07-22 01:16:01,326:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x24070820520>


2023-07-22 01:16:01,328:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x240708201c0>


2023-07-22 01:16:05,436:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x24070820760>


2023-07-22 01:16:05,438:WARNING:C:\Program Files\Python310\lib\threading.py:957: ResourceWarning:

unclosed file <_io.BufferedWriter name=6>


2023-07-22 01:16:05,440:WARNING:C:\Program Files\Python310\lib\threading.py:957: ResourceWarning:

unclosed file <_io.BufferedReader name=7>


2023-07-22 01:16:05,441:WARNING:C:\Program Files\Python310\lib\threading.py:957: ResourceWarning:

unclosed file <_io.BufferedReader name=8>


2023-07-22 01:16:05,441:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x240708208e0>


2023-07-22 01:16:10,661:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x24070820400>


2023-07-22 01:16:12,682:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x24070820ac0>


2023-07-22 01:16:12,682:WARNING:sys:1: ResourceWarning:

Unclosed socket <zmq.Socket(zmq.PUSH) at 0x24070820940>


2023-07-22 01:16:17,806:INFO:Initializing compare_models()
2023-07-22 01:16:17,806:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000240683CBFD0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000240683CBFD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:16:17,806:INFO:Checking exceptions
2023-07-22 01:16:17,810:INFO:Preparing display monitor
2023-07-22 01:18:15,745:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:18:15,745:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:18:15,745:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:18:15,745:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:18:41,243:INFO:PyCaret ClassificationExperiment
2023-07-22 01:18:41,243:INFO:Logging name: balanceado
2023-07-22 01:18:41,243:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:18:41,245:INFO:version 3.0.4
2023-07-22 01:18:41,245:INFO:Initializing setup()
2023-07-22 01:18:41,245:INFO:self.USI: b2c6
2023-07-22 01:18:41,245:INFO:self._variable_keys: {'logging_param', 'X_train', 'log_plots_param', '_available_plots', 'y_test', 'seed', 'exp_name_log', 'html_param', 'USI', 'X_test', 'fix_imbalance', 'target_param', 'y_train', 'gpu_n_jobs_param', 'fold_shuffle_param', 'data', 'memory', 'X', 'fold_groups_param', 'fold_generator', 'y', 'is_multiclass', 'n_jobs_param', 'exp_id', 'idx', 'pipeline', 'gpu_param', '_ml_usecase'}
2023-07-22 01:18:41,245:INFO:Checking environment
2023-07-22 01:18:41,245:INFO:python_version: 3.10.8
2023-07-22 01:18:41,245:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:18:41,245:INFO:machine: AMD64
2023-07-22 01:18:41,245:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:18:41,250:INFO:Memory: svmem(total=16505966592, available=2910543872, percent=82.4, used=13595422720, free=2910543872)
2023-07-22 01:18:41,250:INFO:Physical Core: 6
2023-07-22 01:18:41,250:INFO:Logical Core: 12
2023-07-22 01:18:41,250:INFO:Checking libraries
2023-07-22 01:18:41,250:INFO:System:
2023-07-22 01:18:41,251:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:18:41,251:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:18:41,251:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:18:41,251:INFO:PyCaret required dependencies:
2023-07-22 01:18:41,253:INFO:                 pip: 22.2.2
2023-07-22 01:18:41,253:INFO:          setuptools: 63.2.0
2023-07-22 01:18:41,253:INFO:             pycaret: 3.0.4
2023-07-22 01:18:41,253:INFO:             IPython: 8.11.0
2023-07-22 01:18:41,253:INFO:          ipywidgets: 8.0.7
2023-07-22 01:18:41,253:INFO:                tqdm: 4.64.1
2023-07-22 01:18:41,253:INFO:               numpy: 1.23.5
2023-07-22 01:18:41,253:INFO:              pandas: 1.5.3
2023-07-22 01:18:41,253:INFO:              jinja2: 3.1.2
2023-07-22 01:18:41,253:INFO:               scipy: 1.9.3
2023-07-22 01:18:41,254:INFO:              joblib: 1.2.0
2023-07-22 01:18:41,254:INFO:             sklearn: 1.2.2
2023-07-22 01:18:41,254:INFO:                pyod: 1.1.0
2023-07-22 01:18:41,254:INFO:            imblearn: 0.10.1
2023-07-22 01:18:41,254:INFO:   category_encoders: 2.6.1
2023-07-22 01:18:41,254:INFO:            lightgbm: 3.3.5
2023-07-22 01:18:41,254:INFO:               numba: 0.57.0
2023-07-22 01:18:41,254:INFO:            requests: 2.28.2
2023-07-22 01:18:41,254:INFO:          matplotlib: 3.7.1
2023-07-22 01:18:41,254:INFO:          scikitplot: 0.3.7
2023-07-22 01:18:41,254:INFO:         yellowbrick: 1.5
2023-07-22 01:18:41,254:INFO:              plotly: 5.15.0
2023-07-22 01:18:41,254:INFO:    plotly-resampler: Not installed
2023-07-22 01:18:41,254:INFO:             kaleido: 0.2.1
2023-07-22 01:18:41,254:INFO:           schemdraw: 0.15
2023-07-22 01:18:41,254:INFO:         statsmodels: 0.13.5
2023-07-22 01:18:41,254:INFO:              sktime: 0.21.0
2023-07-22 01:18:41,254:INFO:               tbats: 1.1.3
2023-07-22 01:18:41,254:INFO:            pmdarima: 2.0.3
2023-07-22 01:18:41,254:INFO:              psutil: 5.9.4
2023-07-22 01:18:41,254:INFO:          markupsafe: 2.1.2
2023-07-22 01:18:41,254:INFO:             pickle5: Not installed
2023-07-22 01:18:41,254:INFO:         cloudpickle: 2.2.1
2023-07-22 01:18:41,254:INFO:         deprecation: 2.1.0
2023-07-22 01:18:41,254:INFO:              xxhash: 3.2.0
2023-07-22 01:18:41,254:INFO:           wurlitzer: Not installed
2023-07-22 01:18:41,254:INFO:PyCaret optional dependencies:
2023-07-22 01:18:41,496:INFO:                shap: 0.41.0
2023-07-22 01:18:41,497:INFO:           interpret: 0.4.2
2023-07-22 01:18:41,497:INFO:                umap: 0.5.3
2023-07-22 01:18:41,497:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:18:41,497:INFO:  explainerdashboard: Not installed
2023-07-22 01:18:41,497:INFO:             autoviz: Not installed
2023-07-22 01:18:41,497:INFO:           fairlearn: Not installed
2023-07-22 01:18:41,497:INFO:          deepchecks: Not installed
2023-07-22 01:18:41,497:INFO:             xgboost: 1.7.6
2023-07-22 01:18:41,497:INFO:            catboost: Not installed
2023-07-22 01:18:41,497:INFO:              kmodes: Not installed
2023-07-22 01:18:41,497:INFO:             mlxtend: Not installed
2023-07-22 01:18:41,497:INFO:       statsforecast: Not installed
2023-07-22 01:18:41,497:INFO:        tune_sklearn: Not installed
2023-07-22 01:18:41,497:INFO:                 ray: Not installed
2023-07-22 01:18:41,497:INFO:            hyperopt: Not installed
2023-07-22 01:18:41,497:INFO:              optuna: 3.2.0
2023-07-22 01:18:41,497:INFO:               skopt: Not installed
2023-07-22 01:18:41,497:INFO:              mlflow: 2.4.2
2023-07-22 01:18:41,497:INFO:              gradio: Not installed
2023-07-22 01:18:41,497:INFO:             fastapi: 0.95.2
2023-07-22 01:18:41,497:INFO:             uvicorn: 0.22.0
2023-07-22 01:18:41,497:INFO:              m2cgen: Not installed
2023-07-22 01:18:41,497:INFO:           evidently: Not installed
2023-07-22 01:18:41,497:INFO:               fugue: Not installed
2023-07-22 01:18:41,497:INFO:           streamlit: Not installed
2023-07-22 01:18:41,497:INFO:             prophet: Not installed
2023-07-22 01:18:41,497:INFO:None
2023-07-22 01:18:41,497:INFO:Set up data.
2023-07-22 01:18:41,521:INFO:Set up train/test split.
2023-07-22 01:18:41,521:INFO:Set up data.
2023-07-22 01:18:41,539:INFO:Set up index.
2023-07-22 01:18:41,540:INFO:Set up folding strategy.
2023-07-22 01:18:41,540:INFO:Assigning column types.
2023-07-22 01:18:41,546:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:18:41,590:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:18:41,593:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:18:41,629:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:41,631:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:41,678:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:18:41,678:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:18:41,706:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:41,708:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:41,709:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:18:41,754:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:18:41,782:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:41,785:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:41,833:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:18:41,869:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:41,873:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:41,873:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:18:41,948:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:41,951:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:42,023:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:42,026:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:42,027:INFO:Preparing preprocessing pipeline...
2023-07-22 01:18:42,029:INFO:Set up iterative imputation.
2023-07-22 01:18:42,029:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:18:42,033:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:18:42,038:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:18:42,098:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:18:42,143:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:42,145:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:42,219:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:42,221:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:42,255:INFO:Set up encoding of categorical features.
2023-07-22 01:18:42,256:INFO:Set up imbalanced handling.
2023-07-22 01:18:42,256:INFO:Set up column transformation.
2023-07-22 01:18:42,256:INFO:Set up feature selection.
2023-07-22 01:18:42,348:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:42,351:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:42,397:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:42,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:42,721:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:42,853:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:43,033:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:43,202:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:43,350:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:43,505:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:43,676:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:43,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:43,946:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:44,101:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:44,247:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:44,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:44,561:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:44,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:44,894:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:45,032:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:45,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:45,346:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:45,487:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:45,637:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:45,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:46,069:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:46,197:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:46,338:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:46,501:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:46,648:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:46,798:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:46,941:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:47,087:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:47,214:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:47,365:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:47,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:47,677:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:18:47,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:18:49,549:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:18:49,966:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:18:49,997:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:18:49,997:INFO:Creating final display dataframe.
2023-07-22 01:18:52,449:INFO:Setup _display_container:                         Description  \
0                        Session id   
1                            Target   
2                       Target type   
3               Original data shape   
4            Transformed data shape   
5       Transformed train set shape   
6        Transformed test set shape   
7                   Ignore features   
8                  Numeric features   
9              Categorical features   
10         Rows with missing values   
11                       Preprocess   
12                  Imputation type   
13  Iterative imputation iterations   
14        Numeric iterative imputer   
15    Categorical iterative imputer   
16         Maximum one-hot encoding   
17                  Encoding method   
18                    Fix imbalance   
19             Fix imbalance method   
20                   Transformation   
21            Transformation method   
22                Feature selection   
23         Feature selection method   
24      Feature selection estimator   
25      Number of features selected   
26                   Fold Generator   
27                      Fold Number   
28                         CPU Jobs   
29                          Use GPU   
30                   Log Experiment   
31                  Experiment Name   
32                              USI   

                                                Value  
0                                                  42  
1                                        credit_score  
2                                              Binary  
3                                         (12500, 22)  
4                                         (16736, 21)  
5                                         (14236, 21)  
6                                          (2500, 21)  
7                                                   1  
8                                                  17  
9                                                   3  
10                                              36.1%  
11                                               True  
12                                          iterative  
13                                                  5  
14                                           lightgbm  
15                                           lightgbm  
16                                                 25  
17                                               None  
18                                               True  
19  SMOTE(k_neighbors=5, n_jobs=None, random_state...  
20                                               True  
21                                        yeo-johnson  
22                                               True  
23                                            classic  
24                                           lightgbm  
25                                                 20  
26                                              KFold  
27                                                 10  
28                                                 -1  
29                                              False  
30                                       MlflowLogger  
31                                         balanceado  
32                                               b2c6  
2023-07-22 01:18:52,554:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:52,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:52,636:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:52,639:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:52,640:INFO:Logging experiment in loggers
2023-07-22 01:18:52,934:INFO:SubProcess save_model() called ==================================
2023-07-22 01:18:52,975:INFO:Initializing save_model()
2023-07-22 01:18:52,975:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), model_name=C:\TEMP\tmpaoodf2yz\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2023-07-22 01:18:52,976:INFO:Adding model into prep_pipe
2023-07-22 01:18:52,976:WARNING:Only Model saved as it was a pipeline.
2023-07-22 01:18:53,156:INFO:C:\TEMP\tmpaoodf2yz\Transformation Pipeline.pkl saved in current working directory
2023-07-22 01:18:53,184:INFO:Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:18:53,184:INFO:save_model() successfully completed......................................
2023-07-22 01:18:53,284:INFO:SubProcess save_model() end ==================================
2023-07-22 01:18:53,369:INFO:setup() successfully completed in 11.4s...............
2023-07-22 01:18:54,758:INFO:PyCaret ClassificationExperiment
2023-07-22 01:18:54,758:INFO:Logging name: balanceado
2023-07-22 01:18:54,758:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:18:54,758:INFO:version 3.0.4
2023-07-22 01:18:54,758:INFO:Initializing setup()
2023-07-22 01:18:54,758:INFO:self.USI: f037
2023-07-22 01:18:54,759:INFO:self._variable_keys: {'logging_param', 'X_train', 'log_plots_param', '_available_plots', 'y_test', 'seed', 'exp_name_log', 'html_param', 'USI', 'X_test', 'fix_imbalance', 'target_param', 'y_train', 'gpu_n_jobs_param', 'fold_shuffle_param', 'data', 'memory', 'X', 'fold_groups_param', 'fold_generator', 'y', 'is_multiclass', 'n_jobs_param', 'exp_id', 'idx', 'pipeline', 'gpu_param', '_ml_usecase'}
2023-07-22 01:18:54,759:INFO:Checking environment
2023-07-22 01:18:54,759:INFO:python_version: 3.10.8
2023-07-22 01:18:54,759:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:18:54,759:INFO:machine: AMD64
2023-07-22 01:18:54,759:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:18:54,768:INFO:Memory: svmem(total=16505966592, available=2848882688, percent=82.7, used=13657083904, free=2848882688)
2023-07-22 01:18:54,769:INFO:Physical Core: 6
2023-07-22 01:18:54,769:INFO:Logical Core: 12
2023-07-22 01:18:54,769:INFO:Checking libraries
2023-07-22 01:18:54,769:INFO:System:
2023-07-22 01:18:54,769:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:18:54,769:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:18:54,769:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:18:54,769:INFO:PyCaret required dependencies:
2023-07-22 01:18:54,770:INFO:                 pip: 22.2.2
2023-07-22 01:18:54,770:INFO:          setuptools: 63.2.0
2023-07-22 01:18:54,770:INFO:             pycaret: 3.0.4
2023-07-22 01:18:54,770:INFO:             IPython: 8.11.0
2023-07-22 01:18:54,770:INFO:          ipywidgets: 8.0.7
2023-07-22 01:18:54,770:INFO:                tqdm: 4.64.1
2023-07-22 01:18:54,770:INFO:               numpy: 1.23.5
2023-07-22 01:18:54,770:INFO:              pandas: 1.5.3
2023-07-22 01:18:54,770:INFO:              jinja2: 3.1.2
2023-07-22 01:18:54,770:INFO:               scipy: 1.9.3
2023-07-22 01:18:54,770:INFO:              joblib: 1.2.0
2023-07-22 01:18:54,770:INFO:             sklearn: 1.2.2
2023-07-22 01:18:54,770:INFO:                pyod: 1.1.0
2023-07-22 01:18:54,770:INFO:            imblearn: 0.10.1
2023-07-22 01:18:54,770:INFO:   category_encoders: 2.6.1
2023-07-22 01:18:54,770:INFO:            lightgbm: 3.3.5
2023-07-22 01:18:54,771:INFO:               numba: 0.57.0
2023-07-22 01:18:54,771:INFO:            requests: 2.28.2
2023-07-22 01:18:54,771:INFO:          matplotlib: 3.7.1
2023-07-22 01:18:54,771:INFO:          scikitplot: 0.3.7
2023-07-22 01:18:54,771:INFO:         yellowbrick: 1.5
2023-07-22 01:18:54,771:INFO:              plotly: 5.15.0
2023-07-22 01:18:54,771:INFO:    plotly-resampler: Not installed
2023-07-22 01:18:54,771:INFO:             kaleido: 0.2.1
2023-07-22 01:18:54,771:INFO:           schemdraw: 0.15
2023-07-22 01:18:54,771:INFO:         statsmodels: 0.13.5
2023-07-22 01:18:54,771:INFO:              sktime: 0.21.0
2023-07-22 01:18:54,771:INFO:               tbats: 1.1.3
2023-07-22 01:18:54,771:INFO:            pmdarima: 2.0.3
2023-07-22 01:18:54,771:INFO:              psutil: 5.9.4
2023-07-22 01:18:54,771:INFO:          markupsafe: 2.1.2
2023-07-22 01:18:54,771:INFO:             pickle5: Not installed
2023-07-22 01:18:54,771:INFO:         cloudpickle: 2.2.1
2023-07-22 01:18:54,771:INFO:         deprecation: 2.1.0
2023-07-22 01:18:54,772:INFO:              xxhash: 3.2.0
2023-07-22 01:18:54,772:INFO:           wurlitzer: Not installed
2023-07-22 01:18:54,772:INFO:PyCaret optional dependencies:
2023-07-22 01:18:54,772:INFO:                shap: 0.41.0
2023-07-22 01:18:54,772:INFO:           interpret: 0.4.2
2023-07-22 01:18:54,772:INFO:                umap: 0.5.3
2023-07-22 01:18:54,772:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:18:54,772:INFO:  explainerdashboard: Not installed
2023-07-22 01:18:54,772:INFO:             autoviz: Not installed
2023-07-22 01:18:54,772:INFO:           fairlearn: Not installed
2023-07-22 01:18:54,772:INFO:          deepchecks: Not installed
2023-07-22 01:18:54,772:INFO:             xgboost: 1.7.6
2023-07-22 01:18:54,772:INFO:            catboost: Not installed
2023-07-22 01:18:54,772:INFO:              kmodes: Not installed
2023-07-22 01:18:54,772:INFO:             mlxtend: Not installed
2023-07-22 01:18:54,772:INFO:       statsforecast: Not installed
2023-07-22 01:18:54,772:INFO:        tune_sklearn: Not installed
2023-07-22 01:18:54,773:INFO:                 ray: Not installed
2023-07-22 01:18:54,773:INFO:            hyperopt: Not installed
2023-07-22 01:18:54,773:INFO:              optuna: 3.2.0
2023-07-22 01:18:54,773:INFO:               skopt: Not installed
2023-07-22 01:18:54,773:INFO:              mlflow: 2.4.2
2023-07-22 01:18:54,773:INFO:              gradio: Not installed
2023-07-22 01:18:54,773:INFO:             fastapi: 0.95.2
2023-07-22 01:18:54,773:INFO:             uvicorn: 0.22.0
2023-07-22 01:18:54,773:INFO:              m2cgen: Not installed
2023-07-22 01:18:54,773:INFO:           evidently: Not installed
2023-07-22 01:18:54,773:INFO:               fugue: Not installed
2023-07-22 01:18:54,773:INFO:           streamlit: Not installed
2023-07-22 01:18:54,773:INFO:             prophet: Not installed
2023-07-22 01:18:54,773:INFO:None
2023-07-22 01:18:54,773:INFO:Set up data.
2023-07-22 01:18:54,818:INFO:Set up train/test split.
2023-07-22 01:18:54,818:INFO:Set up data.
2023-07-22 01:18:54,852:INFO:Set up index.
2023-07-22 01:18:54,853:INFO:Set up folding strategy.
2023-07-22 01:18:54,853:INFO:Assigning column types.
2023-07-22 01:18:54,862:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:18:54,914:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:18:54,915:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:18:54,948:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:54,952:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:55,002:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:18:55,004:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:18:55,032:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:55,035:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:55,035:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:18:55,082:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:18:55,111:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:55,114:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:55,162:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:18:55,193:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:55,196:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:55,196:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:18:55,271:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:55,274:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:55,356:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:55,359:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:55,360:INFO:Preparing preprocessing pipeline...
2023-07-22 01:18:55,362:INFO:Set up iterative imputation.
2023-07-22 01:18:55,362:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:18:55,367:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:18:55,372:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:18:55,450:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:18:55,495:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:55,498:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:55,573:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:55,576:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:55,606:INFO:Set up encoding of categorical features.
2023-07-22 01:18:55,607:INFO:Set up imbalanced handling.
2023-07-22 01:18:55,607:INFO:Set up column transformation.
2023-07-22 01:18:55,607:INFO:Set up feature selection.
2023-07-22 01:18:55,684:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:55,686:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:56,375:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:18:56,405:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:18:56,405:INFO:Creating final display dataframe.
2023-07-22 01:18:58,540:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment  MlflowLogger
31                  Experiment Name    balanceado
32                              USI          f037
2023-07-22 01:18:58,649:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:58,653:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:58,733:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:18:58,736:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:18:58,737:INFO:Logging experiment in loggers
2023-07-22 01:18:58,839:INFO:SubProcess save_model() called ==================================
2023-07-22 01:18:58,881:INFO:Initializing save_model()
2023-07-22 01:18:58,881:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), model_name=C:\TEMP\tmpzx9jydx7\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2023-07-22 01:18:58,882:INFO:Adding model into prep_pipe
2023-07-22 01:18:58,882:WARNING:Only Model saved as it was a pipeline.
2023-07-22 01:18:59,064:INFO:C:\TEMP\tmpzx9jydx7\Transformation Pipeline.pkl saved in current working directory
2023-07-22 01:18:59,092:INFO:Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:18:59,092:INFO:save_model() successfully completed......................................
2023-07-22 01:18:59,192:INFO:SubProcess save_model() end ==================================
2023-07-22 01:18:59,256:INFO:setup() successfully completed in 3.99s...............
2023-07-22 01:19:00,942:INFO:Initializing compare_models()
2023-07-22 01:19:00,942:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:19:00,942:INFO:Checking exceptions
2023-07-22 01:19:00,950:INFO:Preparing display monitor
2023-07-22 01:19:00,993:INFO:Initializing Logistic Regression
2023-07-22 01:19:00,993:INFO:Total runtime is 0.0 minutes
2023-07-22 01:19:01,003:INFO:SubProcess create_model() called ==================================
2023-07-22 01:19:01,004:INFO:Initializing create_model()
2023-07-22 01:19:01,004:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:19:01,004:INFO:Checking exceptions
2023-07-22 01:19:01,004:INFO:Importing libraries
2023-07-22 01:19:01,004:INFO:Copying training dataset
2023-07-22 01:19:01,017:INFO:Defining folds
2023-07-22 01:19:01,017:INFO:Declaring metric variables
2023-07-22 01:19:01,021:INFO:Importing untrained model
2023-07-22 01:19:01,027:INFO:Logistic Regression Imported successfully
2023-07-22 01:19:01,038:INFO:Starting cross validation
2023-07-22 01:19:01,302:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:19:07,348:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,352:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,419:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,423:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,428:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,594:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,642:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,646:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,684:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,710:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,903:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,919:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:07,991:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,012:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,015:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,078:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,100:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,244:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,272:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,273:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,285:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,310:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,319:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,336:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,376:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,390:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,391:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,514:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,561:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,563:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,594:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,594:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,596:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,645:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,692:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,721:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,750:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,839:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,902:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,910:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,948:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:08,991:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,059:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,089:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,154:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,275:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,283:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,342:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,349:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,390:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,420:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,469:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,470:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,490:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,632:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,655:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,714:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,751:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,785:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,804:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,835:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:09,881:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,005:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,124:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,136:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,197:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,310:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,325:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,367:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,407:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,539:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,555:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,574:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,707:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,715:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,785:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,809:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,838:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,857:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:10,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,006:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,020:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,075:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,095:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,111:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,226:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,240:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,336:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,367:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,422:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,444:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,471:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,532:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,553:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,604:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,675:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,732:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,862:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,913:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:11,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,035:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,110:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,123:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,192:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,192:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,295:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,365:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,409:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,483:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,506:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,513:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,683:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,806:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,823:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,830:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,880:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,886:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:12,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,039:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,112:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,172:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,178:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,196:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,206:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,237:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,245:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,263:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,328:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,414:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,463:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,494:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,535:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,538:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,674:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,788:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,807:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,838:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,865:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,920:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:13,945:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,107:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,149:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,177:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,227:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,274:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,306:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,376:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,519:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,609:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,801:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,821:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,824:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,884:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:14,998:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,092:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,209:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,246:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,326:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,340:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,360:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,362:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,361:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,480:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:15,612:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,644:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,675:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,680:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,701:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,720:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,925:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,969:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:15,981:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,001:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,011:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,024:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,042:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,059:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,208:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,260:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,313:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,381:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,414:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,423:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,442:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,532:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,600:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,628:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,765:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,814:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,840:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,948:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,994:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:16,996:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,005:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,117:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,136:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,181:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,242:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,396:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,437:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,454:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,463:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,479:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,505:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,534:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,563:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,594:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,735:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,776:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,814:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,848:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,865:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,905:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:17,952:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,107:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,120:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,126:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,181:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,207:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,245:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,305:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,441:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,462:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,480:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,487:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,512:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,537:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,553:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,582:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,584:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,779:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,804:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,813:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,844:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,874:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:18,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,039:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,050:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,076:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,091:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,120:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,133:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,140:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,223:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,382:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,397:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,427:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,477:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,490:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,494:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,521:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,712:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,742:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,752:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,807:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:19,817:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,840:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:19,876:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:20,002:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:20,031:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:20,054:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:20,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:20,125:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:19:20,137:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:20,153:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:20,347:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:20,421:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:19:21,157:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:19:21,245:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:19:21,367:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:19:21,403:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:19:21,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:19:21,494:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:19:21,724:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:19:21,724:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:19:22,904:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:22,959:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:23,101:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:23,143:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:23,163:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:23,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:23,213:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:23,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:23,478:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:24,001:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:24,047:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:24,187:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:24,290:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:24,336:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:24,345:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:24,352:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:24,541:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:24,559:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:19:25,109:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:25,184:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:25,294:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:25,420:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:25,425:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:25,456:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:25,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:25,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:25,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:19:27,409:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:27,495:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:27,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:27,786:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:27,800:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:27,819:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:27,844:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:27,947:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:27,960:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-22 01:19:28,283:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:28,327:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:28,467:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:28,631:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:28,695:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:28,696:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:28,696:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:28,778:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:28,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:29,025:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,054:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,187:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,353:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,415:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,443:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,488:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:19:29,613:INFO:Calculating mean and std
2023-07-22 01:19:29,614:INFO:Creating metrics dataframe
2023-07-22 01:19:29,672:INFO:Uploading results into container
2023-07-22 01:19:29,673:INFO:Uploading model into container now
2023-07-22 01:19:29,674:INFO:_master_model_container: 1
2023-07-22 01:19:29,674:INFO:_display_container: 2
2023-07-22 01:19:29,674:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 01:19:29,674:INFO:create_model() successfully completed......................................
2023-07-22 01:19:29,761:INFO:SubProcess create_model() end ==================================
2023-07-22 01:19:29,761:INFO:Creating metrics dataframe
2023-07-22 01:19:29,769:INFO:Initializing K Neighbors Classifier
2023-07-22 01:19:29,770:INFO:Total runtime is 0.4796184301376343 minutes
2023-07-22 01:19:29,773:INFO:SubProcess create_model() called ==================================
2023-07-22 01:19:29,774:INFO:Initializing create_model()
2023-07-22 01:19:29,774:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:19:29,774:INFO:Checking exceptions
2023-07-22 01:19:29,774:INFO:Importing libraries
2023-07-22 01:19:29,774:INFO:Copying training dataset
2023-07-22 01:19:29,781:INFO:Defining folds
2023-07-22 01:19:29,782:INFO:Declaring metric variables
2023-07-22 01:19:29,786:INFO:Importing untrained model
2023-07-22 01:19:29,790:INFO:K Neighbors Classifier Imported successfully
2023-07-22 01:19:29,797:INFO:Starting cross validation
2023-07-22 01:19:30,050:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:19:33,784:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:33,977:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:34,004:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:34,014:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:34,046:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:34,110:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:34,164:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:36,915:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:36,952:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:37,646:INFO:Calculating mean and std
2023-07-22 01:19:37,647:INFO:Creating metrics dataframe
2023-07-22 01:19:37,713:INFO:Uploading results into container
2023-07-22 01:19:37,714:INFO:Uploading model into container now
2023-07-22 01:19:37,714:INFO:_master_model_container: 2
2023-07-22 01:19:37,715:INFO:_display_container: 2
2023-07-22 01:19:37,715:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 01:19:37,715:INFO:create_model() successfully completed......................................
2023-07-22 01:19:37,808:INFO:SubProcess create_model() end ==================================
2023-07-22 01:19:37,808:INFO:Creating metrics dataframe
2023-07-22 01:19:37,817:INFO:Initializing Naive Bayes
2023-07-22 01:19:37,817:INFO:Total runtime is 0.613721489906311 minutes
2023-07-22 01:19:37,821:INFO:SubProcess create_model() called ==================================
2023-07-22 01:19:37,821:INFO:Initializing create_model()
2023-07-22 01:19:37,821:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:19:37,821:INFO:Checking exceptions
2023-07-22 01:19:37,821:INFO:Importing libraries
2023-07-22 01:19:37,821:INFO:Copying training dataset
2023-07-22 01:19:37,830:INFO:Defining folds
2023-07-22 01:19:37,830:INFO:Declaring metric variables
2023-07-22 01:19:37,834:INFO:Importing untrained model
2023-07-22 01:19:37,837:INFO:Naive Bayes Imported successfully
2023-07-22 01:19:37,845:INFO:Starting cross validation
2023-07-22 01:19:38,047:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:19:41,702:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:41,884:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:41,920:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:41,934:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:41,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:41,982:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:41,982:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:42,026:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:42,164:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:43,292:INFO:Calculating mean and std
2023-07-22 01:19:43,295:INFO:Creating metrics dataframe
2023-07-22 01:19:43,405:INFO:Uploading results into container
2023-07-22 01:19:43,405:INFO:Uploading model into container now
2023-07-22 01:19:43,406:INFO:_master_model_container: 3
2023-07-22 01:19:43,406:INFO:_display_container: 2
2023-07-22 01:19:43,407:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 01:19:43,407:INFO:create_model() successfully completed......................................
2023-07-22 01:19:43,512:INFO:SubProcess create_model() end ==================================
2023-07-22 01:19:43,512:INFO:Creating metrics dataframe
2023-07-22 01:19:43,522:INFO:Initializing Decision Tree Classifier
2023-07-22 01:19:43,522:INFO:Total runtime is 0.7088185350100199 minutes
2023-07-22 01:19:43,526:INFO:SubProcess create_model() called ==================================
2023-07-22 01:19:43,527:INFO:Initializing create_model()
2023-07-22 01:19:43,527:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:19:43,527:INFO:Checking exceptions
2023-07-22 01:19:43,527:INFO:Importing libraries
2023-07-22 01:19:43,528:INFO:Copying training dataset
2023-07-22 01:19:43,536:INFO:Defining folds
2023-07-22 01:19:43,536:INFO:Declaring metric variables
2023-07-22 01:19:43,540:INFO:Importing untrained model
2023-07-22 01:19:43,545:INFO:Decision Tree Classifier Imported successfully
2023-07-22 01:19:43,553:INFO:Starting cross validation
2023-07-22 01:19:43,813:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:19:45,861:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:19:46,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 426, in predict
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:19:48,031:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:48,057:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:48,065:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:48,109:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:48,132:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:48,154:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:48,198:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:48,224:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:48,234:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:49,348:INFO:Calculating mean and std
2023-07-22 01:19:49,349:INFO:Creating metrics dataframe
2023-07-22 01:19:49,436:INFO:Uploading results into container
2023-07-22 01:19:49,436:INFO:Uploading model into container now
2023-07-22 01:19:49,437:INFO:_master_model_container: 4
2023-07-22 01:19:49,437:INFO:_display_container: 2
2023-07-22 01:19:49,437:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 01:19:49,437:INFO:create_model() successfully completed......................................
2023-07-22 01:19:49,555:INFO:SubProcess create_model() end ==================================
2023-07-22 01:19:49,555:INFO:Creating metrics dataframe
2023-07-22 01:19:49,566:INFO:Initializing SVM - Linear Kernel
2023-07-22 01:19:49,567:INFO:Total runtime is 0.8095610499382019 minutes
2023-07-22 01:19:49,571:INFO:SubProcess create_model() called ==================================
2023-07-22 01:19:49,572:INFO:Initializing create_model()
2023-07-22 01:19:49,572:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:19:49,572:INFO:Checking exceptions
2023-07-22 01:19:49,572:INFO:Importing libraries
2023-07-22 01:19:49,572:INFO:Copying training dataset
2023-07-22 01:19:49,583:INFO:Defining folds
2023-07-22 01:19:49,584:INFO:Declaring metric variables
2023-07-22 01:19:49,592:INFO:Importing untrained model
2023-07-22 01:19:49,598:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 01:19:49,609:INFO:Starting cross validation
2023-07-22 01:19:49,862:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:19:51,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:53,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:53,874:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:53,922:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:53,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:53,989:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:54,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:54,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:54,008:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:54,158:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:54,164:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:54,182:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:54,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:54,192:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:54,210:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:54,234:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:54,240:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:54,268:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:54,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:19:54,904:INFO:Calculating mean and std
2023-07-22 01:19:54,906:INFO:Creating metrics dataframe
2023-07-22 01:19:55,026:INFO:Uploading results into container
2023-07-22 01:19:55,028:INFO:Uploading model into container now
2023-07-22 01:19:55,030:INFO:_master_model_container: 5
2023-07-22 01:19:55,031:INFO:_display_container: 2
2023-07-22 01:19:55,031:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 01:19:55,032:INFO:create_model() successfully completed......................................
2023-07-22 01:19:55,215:INFO:SubProcess create_model() end ==================================
2023-07-22 01:19:55,215:INFO:Creating metrics dataframe
2023-07-22 01:19:55,239:INFO:Initializing Ridge Classifier
2023-07-22 01:19:55,240:INFO:Total runtime is 0.904106577237447 minutes
2023-07-22 01:19:55,248:INFO:SubProcess create_model() called ==================================
2023-07-22 01:19:55,249:INFO:Initializing create_model()
2023-07-22 01:19:55,250:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:19:55,250:INFO:Checking exceptions
2023-07-22 01:19:55,250:INFO:Importing libraries
2023-07-22 01:19:55,250:INFO:Copying training dataset
2023-07-22 01:19:55,272:INFO:Defining folds
2023-07-22 01:19:55,272:INFO:Declaring metric variables
2023-07-22 01:19:55,282:INFO:Importing untrained model
2023-07-22 01:19:55,289:INFO:Ridge Classifier Imported successfully
2023-07-22 01:19:55,306:INFO:Starting cross validation
2023-07-22 01:19:55,707:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:19:57,229:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.79155e-52): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:19:57,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.44723e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:19:57,409:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.49256e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:19:57,432:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.92242e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:19:57,462:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.07304e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:19:57,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.66771e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:19:58,311:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:19:58,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:19:58,761:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.39835e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:19:58,920:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.22379e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:19:59,597:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:19:59,640:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.85819e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:20:00,807:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:00,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:00,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.33s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:00,900:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:00,945:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:00,949:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:00,962:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:00,965:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:01,018:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:01,024:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:01,601:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:01,605:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:01,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:01,771:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:01,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:01,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:02,817:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:02,822:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:20:02,835:INFO:Calculating mean and std
2023-07-22 01:20:02,838:INFO:Creating metrics dataframe
2023-07-22 01:20:03,026:INFO:Uploading results into container
2023-07-22 01:20:03,027:INFO:Uploading model into container now
2023-07-22 01:20:03,029:INFO:_master_model_container: 6
2023-07-22 01:20:03,029:INFO:_display_container: 2
2023-07-22 01:20:03,030:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:20:03,030:INFO:create_model() successfully completed......................................
2023-07-22 01:20:03,217:INFO:SubProcess create_model() end ==================================
2023-07-22 01:20:03,217:INFO:Creating metrics dataframe
2023-07-22 01:20:03,238:INFO:Initializing Random Forest Classifier
2023-07-22 01:20:03,239:INFO:Total runtime is 1.0374220172564188 minutes
2023-07-22 01:20:03,246:INFO:SubProcess create_model() called ==================================
2023-07-22 01:20:03,247:INFO:Initializing create_model()
2023-07-22 01:20:03,247:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:20:03,247:INFO:Checking exceptions
2023-07-22 01:20:03,247:INFO:Importing libraries
2023-07-22 01:20:03,248:INFO:Copying training dataset
2023-07-22 01:20:03,266:INFO:Defining folds
2023-07-22 01:20:03,266:INFO:Declaring metric variables
2023-07-22 01:20:03,274:INFO:Importing untrained model
2023-07-22 01:20:03,282:INFO:Random Forest Classifier Imported successfully
2023-07-22 01:20:03,299:INFO:Starting cross validation
2023-07-22 01:20:03,630:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:20:08,362:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:20:14,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 01:20:14,647:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 01:20:14,815:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:14,932:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:15,021:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:15,179:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:15,327:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:15,364:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:20:16,410:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:16,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:16,960:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:20:17,890:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:17,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:18,102:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:19,110:INFO:Calculating mean and std
2023-07-22 01:20:19,112:INFO:Creating metrics dataframe
2023-07-22 01:20:19,261:INFO:Uploading results into container
2023-07-22 01:20:19,262:INFO:Uploading model into container now
2023-07-22 01:20:19,263:INFO:_master_model_container: 7
2023-07-22 01:20:19,264:INFO:_display_container: 2
2023-07-22 01:20:19,264:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 01:20:19,265:INFO:create_model() successfully completed......................................
2023-07-22 01:20:19,423:INFO:SubProcess create_model() end ==================================
2023-07-22 01:20:19,423:INFO:Creating metrics dataframe
2023-07-22 01:20:19,442:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 01:20:19,443:INFO:Total runtime is 1.3074950297673542 minutes
2023-07-22 01:20:19,450:INFO:SubProcess create_model() called ==================================
2023-07-22 01:20:19,450:INFO:Initializing create_model()
2023-07-22 01:20:19,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:20:19,451:INFO:Checking exceptions
2023-07-22 01:20:19,451:INFO:Importing libraries
2023-07-22 01:20:19,451:INFO:Copying training dataset
2023-07-22 01:20:19,464:INFO:Defining folds
2023-07-22 01:20:19,464:INFO:Declaring metric variables
2023-07-22 01:20:19,472:INFO:Importing untrained model
2023-07-22 01:20:19,479:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 01:20:19,493:INFO:Starting cross validation
2023-07-22 01:20:19,761:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:20:21,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:21,070:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:21,109:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:21,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:21,271:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:21,321:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:21,340:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:21,513:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:21,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:20:23,630:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:23,632:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,632:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,633:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:23,666:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:23,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:23,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:23,786:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,786:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:23,805:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:23,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,809:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:23,817:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:23,819:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,819:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:23,849:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:23,851:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,851:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,852:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:23,885:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:23,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:23,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,026:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:24,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,030:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,116:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:24,118:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,118:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,119:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,388:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,388:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,393:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,398:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:20:24,430:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,430:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,434:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,534:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,535:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,535:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,538:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,543:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:20:24,553:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,553:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,554:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:20:24,566:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,566:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,570:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,588:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,588:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,589:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,592:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:20:24,613:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,617:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,622:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:20:24,747:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,747:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,748:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,750:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,754:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:20:24,803:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,803:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:20:24,804:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:20:24,806:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:20:24,810:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:20:25,113:INFO:Calculating mean and std
2023-07-22 01:20:25,115:INFO:Creating metrics dataframe
2023-07-22 01:20:25,221:INFO:Uploading results into container
2023-07-22 01:20:25,222:INFO:Uploading model into container now
2023-07-22 01:20:25,222:INFO:_master_model_container: 8
2023-07-22 01:20:25,222:INFO:_display_container: 2
2023-07-22 01:20:25,222:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 01:20:25,222:INFO:create_model() successfully completed......................................
2023-07-22 01:20:25,321:INFO:SubProcess create_model() end ==================================
2023-07-22 01:20:25,321:INFO:Creating metrics dataframe
2023-07-22 01:20:25,334:INFO:Initializing Ada Boost Classifier
2023-07-22 01:20:25,334:INFO:Total runtime is 1.4056787848472594 minutes
2023-07-22 01:20:25,337:INFO:SubProcess create_model() called ==================================
2023-07-22 01:20:25,338:INFO:Initializing create_model()
2023-07-22 01:20:25,339:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:20:25,339:INFO:Checking exceptions
2023-07-22 01:20:25,339:INFO:Importing libraries
2023-07-22 01:20:25,339:INFO:Copying training dataset
2023-07-22 01:20:25,348:INFO:Defining folds
2023-07-22 01:20:25,348:INFO:Declaring metric variables
2023-07-22 01:20:25,352:INFO:Importing untrained model
2023-07-22 01:20:25,356:INFO:Ada Boost Classifier Imported successfully
2023-07-22 01:20:25,363:INFO:Starting cross validation
2023-07-22 01:20:25,564:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:20:28,961:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 879, in predict_proba
    decision = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:20:30,040:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 700, in predict
    pred = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:20:31,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:31,012:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:31,023:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:31,038:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:31,043:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:31,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:31,063:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:31,068:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:31,089:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:32,604:INFO:Calculating mean and std
2023-07-22 01:20:32,605:INFO:Creating metrics dataframe
2023-07-22 01:20:32,739:INFO:Uploading results into container
2023-07-22 01:20:32,740:INFO:Uploading model into container now
2023-07-22 01:20:32,741:INFO:_master_model_container: 9
2023-07-22 01:20:32,741:INFO:_display_container: 2
2023-07-22 01:20:32,742:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 01:20:32,742:INFO:create_model() successfully completed......................................
2023-07-22 01:20:32,857:INFO:SubProcess create_model() end ==================================
2023-07-22 01:20:32,857:INFO:Creating metrics dataframe
2023-07-22 01:20:32,871:INFO:Initializing Gradient Boosting Classifier
2023-07-22 01:20:32,871:INFO:Total runtime is 1.5312958955764768 minutes
2023-07-22 01:20:32,877:INFO:SubProcess create_model() called ==================================
2023-07-22 01:20:32,878:INFO:Initializing create_model()
2023-07-22 01:20:32,878:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:20:32,878:INFO:Checking exceptions
2023-07-22 01:20:32,878:INFO:Importing libraries
2023-07-22 01:20:32,878:INFO:Copying training dataset
2023-07-22 01:20:32,888:INFO:Defining folds
2023-07-22 01:20:32,889:INFO:Declaring metric variables
2023-07-22 01:20:32,894:INFO:Importing untrained model
2023-07-22 01:20:32,899:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 01:20:32,907:INFO:Starting cross validation
2023-07-22 01:20:33,147:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:20:44,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1355, in predict_proba
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:20:46,066:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1308, in predict
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:20:46,856:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:47,022:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:47,161:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:47,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:47,407:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:48,262:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:48,318:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:48,621:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:48,660:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:49,315:INFO:Calculating mean and std
2023-07-22 01:20:49,316:INFO:Creating metrics dataframe
2023-07-22 01:20:49,449:INFO:Uploading results into container
2023-07-22 01:20:49,450:INFO:Uploading model into container now
2023-07-22 01:20:49,450:INFO:_master_model_container: 10
2023-07-22 01:20:49,450:INFO:_display_container: 2
2023-07-22 01:20:49,451:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 01:20:49,451:INFO:create_model() successfully completed......................................
2023-07-22 01:20:49,543:INFO:SubProcess create_model() end ==================================
2023-07-22 01:20:49,543:INFO:Creating metrics dataframe
2023-07-22 01:20:49,556:INFO:Initializing Linear Discriminant Analysis
2023-07-22 01:20:49,556:INFO:Total runtime is 1.8093785007794696 minutes
2023-07-22 01:20:49,563:INFO:SubProcess create_model() called ==================================
2023-07-22 01:20:49,564:INFO:Initializing create_model()
2023-07-22 01:20:49,566:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:20:49,566:INFO:Checking exceptions
2023-07-22 01:20:49,566:INFO:Importing libraries
2023-07-22 01:20:49,566:INFO:Copying training dataset
2023-07-22 01:20:49,582:INFO:Defining folds
2023-07-22 01:20:49,582:INFO:Declaring metric variables
2023-07-22 01:20:49,591:INFO:Importing untrained model
2023-07-22 01:20:49,600:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:20:49,609:INFO:Starting cross validation
2023-07-22 01:20:49,848:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:20:53,287:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:53,330:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:53,352:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:53,362:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:53,391:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:53,404:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:53,416:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:53,427:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:53,511:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:54,861:INFO:Calculating mean and std
2023-07-22 01:20:54,862:INFO:Creating metrics dataframe
2023-07-22 01:20:55,006:INFO:Uploading results into container
2023-07-22 01:20:55,007:INFO:Uploading model into container now
2023-07-22 01:20:55,008:INFO:_master_model_container: 11
2023-07-22 01:20:55,008:INFO:_display_container: 2
2023-07-22 01:20:55,009:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:20:55,009:INFO:create_model() successfully completed......................................
2023-07-22 01:20:55,101:INFO:SubProcess create_model() end ==================================
2023-07-22 01:20:55,101:INFO:Creating metrics dataframe
2023-07-22 01:20:55,114:INFO:Initializing Extra Trees Classifier
2023-07-22 01:20:55,114:INFO:Total runtime is 1.9020061175028482 minutes
2023-07-22 01:20:55,118:INFO:SubProcess create_model() called ==================================
2023-07-22 01:20:55,119:INFO:Initializing create_model()
2023-07-22 01:20:55,119:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:20:55,119:INFO:Checking exceptions
2023-07-22 01:20:55,119:INFO:Importing libraries
2023-07-22 01:20:55,119:INFO:Copying training dataset
2023-07-22 01:20:55,129:INFO:Defining folds
2023-07-22 01:20:55,130:INFO:Declaring metric variables
2023-07-22 01:20:55,133:INFO:Importing untrained model
2023-07-22 01:20:55,138:INFO:Extra Trees Classifier Imported successfully
2023-07-22 01:20:55,147:INFO:Starting cross validation
2023-07-22 01:20:55,365:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:20:58,940:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:20:59,182:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:21:00,047:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:21:00,656:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:01,999:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:02,364:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:03,458:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:04,015:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:04,028:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:04,109:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:04,166:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:04,442:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:05,302:INFO:Calculating mean and std
2023-07-22 01:21:05,303:INFO:Creating metrics dataframe
2023-07-22 01:21:05,447:INFO:Uploading results into container
2023-07-22 01:21:05,448:INFO:Uploading model into container now
2023-07-22 01:21:05,449:INFO:_master_model_container: 12
2023-07-22 01:21:05,449:INFO:_display_container: 2
2023-07-22 01:21:05,449:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 01:21:05,450:INFO:create_model() successfully completed......................................
2023-07-22 01:21:05,537:INFO:SubProcess create_model() end ==================================
2023-07-22 01:21:05,537:INFO:Creating metrics dataframe
2023-07-22 01:21:05,549:INFO:Initializing Extreme Gradient Boosting
2023-07-22 01:21:05,549:INFO:Total runtime is 2.075922437508901 minutes
2023-07-22 01:21:05,553:INFO:SubProcess create_model() called ==================================
2023-07-22 01:21:05,554:INFO:Initializing create_model()
2023-07-22 01:21:05,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:21:05,554:INFO:Checking exceptions
2023-07-22 01:21:05,554:INFO:Importing libraries
2023-07-22 01:21:05,554:INFO:Copying training dataset
2023-07-22 01:21:05,560:INFO:Defining folds
2023-07-22 01:21:05,561:INFO:Declaring metric variables
2023-07-22 01:21:05,565:INFO:Importing untrained model
2023-07-22 01:21:05,570:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 01:21:05,579:INFO:Starting cross validation
2023-07-22 01:21:05,775:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:21:14,510:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:14,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:14,525:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:14,525:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:14,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:14,537:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:14,541:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:14,545:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:14,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:16,159:INFO:Calculating mean and std
2023-07-22 01:21:16,160:INFO:Creating metrics dataframe
2023-07-22 01:21:16,305:INFO:Uploading results into container
2023-07-22 01:21:16,306:INFO:Uploading model into container now
2023-07-22 01:21:16,306:INFO:_master_model_container: 13
2023-07-22 01:21:16,306:INFO:_display_container: 2
2023-07-22 01:21:16,307:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 01:21:16,307:INFO:create_model() successfully completed......................................
2023-07-22 01:21:16,403:INFO:SubProcess create_model() end ==================================
2023-07-22 01:21:16,403:INFO:Creating metrics dataframe
2023-07-22 01:21:16,418:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 01:21:16,418:INFO:Total runtime is 2.2570818940798443 minutes
2023-07-22 01:21:16,423:INFO:SubProcess create_model() called ==================================
2023-07-22 01:21:16,423:INFO:Initializing create_model()
2023-07-22 01:21:16,423:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:21:16,423:INFO:Checking exceptions
2023-07-22 01:21:16,423:INFO:Importing libraries
2023-07-22 01:21:16,423:INFO:Copying training dataset
2023-07-22 01:21:16,432:INFO:Defining folds
2023-07-22 01:21:16,433:INFO:Declaring metric variables
2023-07-22 01:21:16,438:INFO:Importing untrained model
2023-07-22 01:21:16,442:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:21:16,451:INFO:Starting cross validation
2023-07-22 01:21:16,660:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:21:20,609:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:20,613:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:20,617:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:20,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:20,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:20,647:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:20,679:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:20,695:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:20,708:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:22,335:INFO:Calculating mean and std
2023-07-22 01:21:22,336:INFO:Creating metrics dataframe
2023-07-22 01:21:22,490:INFO:Uploading results into container
2023-07-22 01:21:22,491:INFO:Uploading model into container now
2023-07-22 01:21:22,492:INFO:_master_model_container: 14
2023-07-22 01:21:22,492:INFO:_display_container: 2
2023-07-22 01:21:22,493:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:21:22,493:INFO:create_model() successfully completed......................................
2023-07-22 01:21:22,577:INFO:SubProcess create_model() end ==================================
2023-07-22 01:21:22,578:INFO:Creating metrics dataframe
2023-07-22 01:21:22,589:INFO:Initializing Dummy Classifier
2023-07-22 01:21:22,589:INFO:Total runtime is 2.359928874174754 minutes
2023-07-22 01:21:22,594:INFO:SubProcess create_model() called ==================================
2023-07-22 01:21:22,594:INFO:Initializing create_model()
2023-07-22 01:21:22,594:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233011E2470>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:21:22,594:INFO:Checking exceptions
2023-07-22 01:21:22,595:INFO:Importing libraries
2023-07-22 01:21:22,595:INFO:Copying training dataset
2023-07-22 01:21:22,601:INFO:Defining folds
2023-07-22 01:21:22,602:INFO:Declaring metric variables
2023-07-22 01:21:22,606:INFO:Importing untrained model
2023-07-22 01:21:22,610:INFO:Dummy Classifier Imported successfully
2023-07-22 01:21:22,619:INFO:Starting cross validation
2023-07-22 01:21:22,821:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:21:24,091:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:26,168:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,190:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,212:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,221:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,224:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,242:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,262:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,323:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:21:26,892:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:26,940:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:26,942:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:26,958:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:26,962:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:26,985:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:26,992:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:27,023:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:27,073:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:21:27,924:INFO:Calculating mean and std
2023-07-22 01:21:27,926:INFO:Creating metrics dataframe
2023-07-22 01:21:28,085:INFO:Uploading results into container
2023-07-22 01:21:28,086:INFO:Uploading model into container now
2023-07-22 01:21:28,087:INFO:_master_model_container: 15
2023-07-22 01:21:28,087:INFO:_display_container: 2
2023-07-22 01:21:28,088:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 01:21:28,088:INFO:create_model() successfully completed......................................
2023-07-22 01:21:28,184:INFO:SubProcess create_model() end ==================================
2023-07-22 01:21:28,184:INFO:Creating metrics dataframe
2023-07-22 01:21:28,207:INFO:Initializing create_model()
2023-07-22 01:21:28,207:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:21:28,207:INFO:Checking exceptions
2023-07-22 01:21:28,209:INFO:Importing libraries
2023-07-22 01:21:28,209:INFO:Copying training dataset
2023-07-22 01:21:28,216:INFO:Defining folds
2023-07-22 01:21:28,216:INFO:Declaring metric variables
2023-07-22 01:21:28,216:INFO:Importing untrained model
2023-07-22 01:21:28,216:INFO:Declaring custom model
2023-07-22 01:21:28,217:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:21:28,455:INFO:Cross validation set to False
2023-07-22 01:21:28,455:INFO:Fitting Model
2023-07-22 01:21:29,395:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:21:29,395:INFO:create_model() successfully completed......................................
2023-07-22 01:21:29,484:INFO:Creating Dashboard logs
2023-07-22 01:21:29,490:INFO:Model: Light Gradient Boosting Machine
2023-07-22 01:21:29,552:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2023-07-22 01:21:29,723:INFO:Initializing predict_model()
2023-07-22 01:21:29,724:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000023300039A20>)
2023-07-22 01:21:29,724:INFO:Checking exceptions
2023-07-22 01:21:29,724:INFO:Preloading libraries
2023-07-22 01:21:30,201:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:21:30,214:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\_distutils_hack\__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")

2023-07-22 01:21:30,861:INFO:Initializing create_model()
2023-07-22 01:21:30,862:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:21:30,862:INFO:Checking exceptions
2023-07-22 01:21:30,863:INFO:Importing libraries
2023-07-22 01:21:30,863:INFO:Copying training dataset
2023-07-22 01:21:30,871:INFO:Defining folds
2023-07-22 01:21:30,871:INFO:Declaring metric variables
2023-07-22 01:21:30,871:INFO:Importing untrained model
2023-07-22 01:21:30,871:INFO:Declaring custom model
2023-07-22 01:21:30,872:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 01:21:31,074:INFO:Cross validation set to False
2023-07-22 01:21:31,074:INFO:Fitting Model
2023-07-22 01:21:33,227:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 01:21:33,227:INFO:create_model() successfully completed......................................
2023-07-22 01:21:33,350:INFO:Creating Dashboard logs
2023-07-22 01:21:33,357:INFO:Model: Extreme Gradient Boosting
2023-07-22 01:21:33,422:INFO:Logged params: {'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': None, 'predictor': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2023-07-22 01:21:33,657:INFO:Initializing predict_model()
2023-07-22 01:21:33,657:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000233012CDCF0>)
2023-07-22 01:21:33,657:INFO:Checking exceptions
2023-07-22 01:21:33,657:INFO:Preloading libraries
2023-07-22 01:21:34,220:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:21:34,989:INFO:Initializing create_model()
2023-07-22 01:21:34,989:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:21:34,990:INFO:Checking exceptions
2023-07-22 01:21:34,992:INFO:Importing libraries
2023-07-22 01:21:34,992:INFO:Copying training dataset
2023-07-22 01:21:35,001:INFO:Defining folds
2023-07-22 01:21:35,001:INFO:Declaring metric variables
2023-07-22 01:21:35,001:INFO:Importing untrained model
2023-07-22 01:21:35,001:INFO:Declaring custom model
2023-07-22 01:21:35,002:INFO:Ridge Classifier Imported successfully
2023-07-22 01:21:35,246:INFO:Cross validation set to False
2023-07-22 01:21:35,246:INFO:Fitting Model
2023-07-22 01:21:35,880:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.42454e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:21:36,029:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:21:36,030:INFO:create_model() successfully completed......................................
2023-07-22 01:21:36,145:INFO:Creating Dashboard logs
2023-07-22 01:21:36,150:INFO:Model: Ridge Classifier
2023-07-22 01:21:36,223:INFO:Logged params: {'alpha': 1.0, 'class_weight': None, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 42, 'solver': 'auto', 'tol': 0.0001}
2023-07-22 01:21:36,453:INFO:Initializing predict_model()
2023-07-22 01:21:36,454:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000233012CFD90>)
2023-07-22 01:21:36,454:INFO:Checking exceptions
2023-07-22 01:21:36,454:INFO:Preloading libraries
2023-07-22 01:21:36,947:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:21:37,783:INFO:Creating Dashboard logs
2023-07-22 01:21:37,787:INFO:Model: Linear Discriminant Analysis
2023-07-22 01:21:37,864:INFO:Logged params: {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}
2023-07-22 01:21:38,840:INFO:Creating Dashboard logs
2023-07-22 01:21:38,848:INFO:Model: Logistic Regression
2023-07-22 01:21:38,942:INFO:Logged params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
2023-07-22 01:21:39,954:INFO:Creating Dashboard logs
2023-07-22 01:21:39,959:INFO:Model: Dummy Classifier
2023-07-22 01:21:40,025:INFO:Logged params: {'constant': None, 'random_state': 42, 'strategy': 'prior'}
2023-07-22 01:21:40,902:INFO:Creating Dashboard logs
2023-07-22 01:21:40,906:INFO:Model: Random Forest Classifier
2023-07-22 01:21:40,966:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
2023-07-22 01:21:41,824:INFO:Creating Dashboard logs
2023-07-22 01:21:41,829:INFO:Model: Gradient Boosting Classifier
2023-07-22 01:21:41,884:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-07-22 01:21:42,760:INFO:Creating Dashboard logs
2023-07-22 01:21:42,765:INFO:Model: Extra Trees Classifier
2023-07-22 01:21:42,831:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
2023-07-22 01:21:43,648:INFO:Creating Dashboard logs
2023-07-22 01:21:43,651:INFO:Model: Ada Boost Classifier
2023-07-22 01:21:43,707:INFO:Logged params: {'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 42}
2023-07-22 01:21:44,486:INFO:Creating Dashboard logs
2023-07-22 01:21:44,488:INFO:Model: SVM - Linear Kernel
2023-07-22 01:21:44,572:INFO:Logged params: {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.001, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-07-22 01:21:45,376:INFO:Creating Dashboard logs
2023-07-22 01:21:45,379:INFO:Model: K Neighbors Classifier
2023-07-22 01:21:45,462:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2023-07-22 01:21:46,229:INFO:Creating Dashboard logs
2023-07-22 01:21:46,233:INFO:Model: Quadratic Discriminant Analysis
2023-07-22 01:21:46,290:INFO:Logged params: {'priors': None, 'reg_param': 0.0, 'store_covariance': False, 'tol': 0.0001}
2023-07-22 01:21:47,091:INFO:Creating Dashboard logs
2023-07-22 01:21:47,095:INFO:Model: Decision Tree Classifier
2023-07-22 01:21:47,193:INFO:Logged params: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 42, 'splitter': 'best'}
2023-07-22 01:21:48,162:INFO:Creating Dashboard logs
2023-07-22 01:21:48,166:INFO:Model: Naive Bayes
2023-07-22 01:21:48,245:INFO:Logged params: {'priors': None, 'var_smoothing': 1e-09}
2023-07-22 01:21:49,128:INFO:_master_model_container: 15
2023-07-22 01:21:49,128:INFO:_display_container: 2
2023-07-22 01:21:49,130:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...), RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)]
2023-07-22 01:21:49,131:INFO:compare_models() successfully completed......................................
2023-07-22 01:23:39,573:INFO:Initializing compare_models()
2023-07-22 01:23:39,573:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:23:39,573:INFO:Checking exceptions
2023-07-22 01:23:39,577:INFO:Preparing display monitor
2023-07-22 01:23:39,601:INFO:Initializing Logistic Regression
2023-07-22 01:23:39,602:INFO:Total runtime is 1.679658889770508e-05 minutes
2023-07-22 01:23:39,607:INFO:SubProcess create_model() called ==================================
2023-07-22 01:23:39,607:INFO:Initializing create_model()
2023-07-22 01:23:39,607:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:23:39,608:INFO:Checking exceptions
2023-07-22 01:23:39,608:INFO:Importing libraries
2023-07-22 01:23:39,608:INFO:Copying training dataset
2023-07-22 01:23:39,616:INFO:Defining folds
2023-07-22 01:23:39,616:INFO:Declaring metric variables
2023-07-22 01:23:39,621:INFO:Importing untrained model
2023-07-22 01:23:39,625:INFO:Logistic Regression Imported successfully
2023-07-22 01:23:39,635:INFO:Starting cross validation
2023-07-22 01:23:39,884:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:23:41,140:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:41,146:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:41,180:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:41,211:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:41,211:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:41,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:41,257:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:41,260:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:41,287:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:23:43,384:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:43,434:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:43,456:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:43,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:43,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:43,495:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:43,516:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:43,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:43,525:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:44,110:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:44,160:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:44,161:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:44,177:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:44,186:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:44,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:44,250:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:44,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:44,259:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:23:45,180:INFO:Calculating mean and std
2023-07-22 01:23:45,181:INFO:Creating metrics dataframe
2023-07-22 01:23:45,352:INFO:Uploading results into container
2023-07-22 01:23:45,353:INFO:Uploading model into container now
2023-07-22 01:23:45,354:INFO:_master_model_container: 16
2023-07-22 01:23:45,355:INFO:_display_container: 3
2023-07-22 01:23:45,355:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 01:23:45,355:INFO:create_model() successfully completed......................................
2023-07-22 01:23:45,452:INFO:SubProcess create_model() end ==================================
2023-07-22 01:23:45,452:INFO:Creating metrics dataframe
2023-07-22 01:23:45,461:INFO:Initializing K Neighbors Classifier
2023-07-22 01:23:45,461:INFO:Total runtime is 0.09766701459884644 minutes
2023-07-22 01:23:45,467:INFO:SubProcess create_model() called ==================================
2023-07-22 01:23:45,467:INFO:Initializing create_model()
2023-07-22 01:23:45,467:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:23:45,468:INFO:Checking exceptions
2023-07-22 01:23:45,468:INFO:Importing libraries
2023-07-22 01:23:45,468:INFO:Copying training dataset
2023-07-22 01:23:45,475:INFO:Defining folds
2023-07-22 01:23:45,475:INFO:Declaring metric variables
2023-07-22 01:23:45,481:INFO:Importing untrained model
2023-07-22 01:23:45,486:INFO:K Neighbors Classifier Imported successfully
2023-07-22 01:23:45,493:INFO:Starting cross validation
2023-07-22 01:23:45,693:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:23:49,117:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:49,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:49,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:49,237:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:49,238:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:49,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:49,269:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:49,278:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:51,088:INFO:Calculating mean and std
2023-07-22 01:23:51,089:INFO:Creating metrics dataframe
2023-07-22 01:23:51,258:INFO:Uploading results into container
2023-07-22 01:23:51,259:INFO:Uploading model into container now
2023-07-22 01:23:51,259:INFO:_master_model_container: 17
2023-07-22 01:23:51,259:INFO:_display_container: 3
2023-07-22 01:23:51,259:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 01:23:51,259:INFO:create_model() successfully completed......................................
2023-07-22 01:23:51,355:INFO:SubProcess create_model() end ==================================
2023-07-22 01:23:51,356:INFO:Creating metrics dataframe
2023-07-22 01:23:51,365:INFO:Initializing Naive Bayes
2023-07-22 01:23:51,366:INFO:Total runtime is 0.19607816537221273 minutes
2023-07-22 01:23:51,370:INFO:SubProcess create_model() called ==================================
2023-07-22 01:23:51,371:INFO:Initializing create_model()
2023-07-22 01:23:51,371:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:23:51,371:INFO:Checking exceptions
2023-07-22 01:23:51,371:INFO:Importing libraries
2023-07-22 01:23:51,371:INFO:Copying training dataset
2023-07-22 01:23:51,377:INFO:Defining folds
2023-07-22 01:23:51,378:INFO:Declaring metric variables
2023-07-22 01:23:51,383:INFO:Importing untrained model
2023-07-22 01:23:51,388:INFO:Naive Bayes Imported successfully
2023-07-22 01:23:51,397:INFO:Starting cross validation
2023-07-22 01:23:51,593:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:23:54,840:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:54,929:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:54,942:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:54,951:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:54,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:55,980:INFO:Calculating mean and std
2023-07-22 01:23:55,981:INFO:Creating metrics dataframe
2023-07-22 01:23:56,152:INFO:Uploading results into container
2023-07-22 01:23:56,153:INFO:Uploading model into container now
2023-07-22 01:23:56,153:INFO:_master_model_container: 18
2023-07-22 01:23:56,153:INFO:_display_container: 3
2023-07-22 01:23:56,153:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 01:23:56,153:INFO:create_model() successfully completed......................................
2023-07-22 01:23:56,243:INFO:SubProcess create_model() end ==================================
2023-07-22 01:23:56,243:INFO:Creating metrics dataframe
2023-07-22 01:23:56,254:INFO:Initializing Decision Tree Classifier
2023-07-22 01:23:56,254:INFO:Total runtime is 0.27754875818888347 minutes
2023-07-22 01:23:56,258:INFO:SubProcess create_model() called ==================================
2023-07-22 01:23:56,258:INFO:Initializing create_model()
2023-07-22 01:23:56,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:23:56,258:INFO:Checking exceptions
2023-07-22 01:23:56,258:INFO:Importing libraries
2023-07-22 01:23:56,258:INFO:Copying training dataset
2023-07-22 01:23:56,267:INFO:Defining folds
2023-07-22 01:23:56,268:INFO:Declaring metric variables
2023-07-22 01:23:56,272:INFO:Importing untrained model
2023-07-22 01:23:56,276:INFO:Decision Tree Classifier Imported successfully
2023-07-22 01:23:56,284:INFO:Starting cross validation
2023-07-22 01:23:56,485:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:23:57,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:23:58,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 426, in predict
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:23:59,805:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:59,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:59,837:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:23:59,837:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:00,753:INFO:Calculating mean and std
2023-07-22 01:24:00,754:INFO:Creating metrics dataframe
2023-07-22 01:24:00,941:INFO:Uploading results into container
2023-07-22 01:24:00,942:INFO:Uploading model into container now
2023-07-22 01:24:00,942:INFO:_master_model_container: 19
2023-07-22 01:24:00,944:INFO:_display_container: 3
2023-07-22 01:24:00,944:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 01:24:00,944:INFO:create_model() successfully completed......................................
2023-07-22 01:24:01,041:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:01,041:INFO:Creating metrics dataframe
2023-07-22 01:24:01,052:INFO:Initializing SVM - Linear Kernel
2023-07-22 01:24:01,052:INFO:Total runtime is 0.35751248598098756 minutes
2023-07-22 01:24:01,056:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:01,056:INFO:Initializing create_model()
2023-07-22 01:24:01,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:01,056:INFO:Checking exceptions
2023-07-22 01:24:01,056:INFO:Importing libraries
2023-07-22 01:24:01,056:INFO:Copying training dataset
2023-07-22 01:24:01,063:INFO:Defining folds
2023-07-22 01:24:01,064:INFO:Declaring metric variables
2023-07-22 01:24:01,067:INFO:Importing untrained model
2023-07-22 01:24:01,072:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 01:24:01,079:INFO:Starting cross validation
2023-07-22 01:24:01,277:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:02,378:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:03,186:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:03,254:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:03,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:03,328:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:03,351:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:04,472:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:04,476:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:04,480:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:04,485:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:04,495:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:04,498:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:04,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:04,523:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:24:04,919:INFO:Calculating mean and std
2023-07-22 01:24:04,921:INFO:Creating metrics dataframe
2023-07-22 01:24:05,094:INFO:Uploading results into container
2023-07-22 01:24:05,095:INFO:Uploading model into container now
2023-07-22 01:24:05,095:INFO:_master_model_container: 20
2023-07-22 01:24:05,095:INFO:_display_container: 3
2023-07-22 01:24:05,095:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 01:24:05,095:INFO:create_model() successfully completed......................................
2023-07-22 01:24:05,183:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:05,183:INFO:Creating metrics dataframe
2023-07-22 01:24:05,193:INFO:Initializing Ridge Classifier
2023-07-22 01:24:05,193:INFO:Total runtime is 0.42652958234151206 minutes
2023-07-22 01:24:05,197:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:05,197:INFO:Initializing create_model()
2023-07-22 01:24:05,197:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:05,197:INFO:Checking exceptions
2023-07-22 01:24:05,197:INFO:Importing libraries
2023-07-22 01:24:05,197:INFO:Copying training dataset
2023-07-22 01:24:05,204:INFO:Defining folds
2023-07-22 01:24:05,205:INFO:Declaring metric variables
2023-07-22 01:24:05,209:INFO:Importing untrained model
2023-07-22 01:24:05,213:INFO:Ridge Classifier Imported successfully
2023-07-22 01:24:05,221:INFO:Starting cross validation
2023-07-22 01:24:05,421:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:06,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.66771e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:06,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:06,548:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.22379e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:06,580:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.39835e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:06,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.79155e-52): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:06,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.44723e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:06,618:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.92242e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:06,673:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.49256e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:06,695:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.85819e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:06,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.07304e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:07,286:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:07,320:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:07,349:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:07,357:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:07,360:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:07,404:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:07,419:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:08,420:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:08,422:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:08,581:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:08,583:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:24:08,589:INFO:Calculating mean and std
2023-07-22 01:24:08,592:INFO:Creating metrics dataframe
2023-07-22 01:24:08,773:INFO:Uploading results into container
2023-07-22 01:24:08,774:INFO:Uploading model into container now
2023-07-22 01:24:08,775:INFO:_master_model_container: 21
2023-07-22 01:24:08,775:INFO:_display_container: 3
2023-07-22 01:24:08,775:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:24:08,775:INFO:create_model() successfully completed......................................
2023-07-22 01:24:08,862:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:08,862:INFO:Creating metrics dataframe
2023-07-22 01:24:08,871:INFO:Initializing Random Forest Classifier
2023-07-22 01:24:08,872:INFO:Total runtime is 0.48785351912180586 minutes
2023-07-22 01:24:08,875:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:08,876:INFO:Initializing create_model()
2023-07-22 01:24:08,876:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:08,876:INFO:Checking exceptions
2023-07-22 01:24:08,876:INFO:Importing libraries
2023-07-22 01:24:08,876:INFO:Copying training dataset
2023-07-22 01:24:08,884:INFO:Defining folds
2023-07-22 01:24:08,884:INFO:Declaring metric variables
2023-07-22 01:24:08,888:INFO:Importing untrained model
2023-07-22 01:24:08,892:INFO:Random Forest Classifier Imported successfully
2023-07-22 01:24:08,900:INFO:Starting cross validation
2023-07-22 01:24:09,097:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:10,158:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:24:11,548:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:24:12,887:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:13,488:INFO:Calculating mean and std
2023-07-22 01:24:13,490:INFO:Creating metrics dataframe
2023-07-22 01:24:13,674:INFO:Uploading results into container
2023-07-22 01:24:13,675:INFO:Uploading model into container now
2023-07-22 01:24:13,675:INFO:_master_model_container: 22
2023-07-22 01:24:13,676:INFO:_display_container: 3
2023-07-22 01:24:13,676:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 01:24:13,676:INFO:create_model() successfully completed......................................
2023-07-22 01:24:13,762:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:13,764:INFO:Creating metrics dataframe
2023-07-22 01:24:13,773:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 01:24:13,773:INFO:Total runtime is 0.5695326725641887 minutes
2023-07-22 01:24:13,778:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:13,778:INFO:Initializing create_model()
2023-07-22 01:24:13,778:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:13,778:INFO:Checking exceptions
2023-07-22 01:24:13,778:INFO:Importing libraries
2023-07-22 01:24:13,778:INFO:Copying training dataset
2023-07-22 01:24:13,785:INFO:Defining folds
2023-07-22 01:24:13,786:INFO:Declaring metric variables
2023-07-22 01:24:13,788:INFO:Importing untrained model
2023-07-22 01:24:13,792:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 01:24:13,800:INFO:Starting cross validation
2023-07-22 01:24:13,995:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:15,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,080:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,081:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,091:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,142:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,160:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,164:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,199:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,202:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:24:15,708:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,708:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,709:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:15,749:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,749:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,750:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:15,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:15,775:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,776:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:15,828:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,866:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,866:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,866:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,866:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,866:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:15,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:15,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:15,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:15,869:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,358:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,358:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,358:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,360:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:16,364:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:16,427:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,427:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,428:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:16,434:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:16,438:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,438:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,439:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,442:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:16,447:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:16,449:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,449:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,449:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,451:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:16,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:16,529:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,529:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,530:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,532:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:16,539:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:16,555:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,555:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,556:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:16,559:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,559:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,560:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,561:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:16,561:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:16,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:16,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:24:16,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:24:16,566:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:16,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:24:17,541:INFO:Calculating mean and std
2023-07-22 01:24:17,542:INFO:Creating metrics dataframe
2023-07-22 01:24:17,713:INFO:Uploading results into container
2023-07-22 01:24:17,714:INFO:Uploading model into container now
2023-07-22 01:24:17,714:INFO:_master_model_container: 23
2023-07-22 01:24:17,714:INFO:_display_container: 3
2023-07-22 01:24:17,715:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 01:24:17,715:INFO:create_model() successfully completed......................................
2023-07-22 01:24:17,802:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:17,802:INFO:Creating metrics dataframe
2023-07-22 01:24:17,813:INFO:Initializing Ada Boost Classifier
2023-07-22 01:24:17,813:INFO:Total runtime is 0.6368629217147828 minutes
2023-07-22 01:24:17,817:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:17,817:INFO:Initializing create_model()
2023-07-22 01:24:17,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:17,817:INFO:Checking exceptions
2023-07-22 01:24:17,817:INFO:Importing libraries
2023-07-22 01:24:17,817:INFO:Copying training dataset
2023-07-22 01:24:17,824:INFO:Defining folds
2023-07-22 01:24:17,825:INFO:Declaring metric variables
2023-07-22 01:24:17,829:INFO:Importing untrained model
2023-07-22 01:24:17,834:INFO:Ada Boost Classifier Imported successfully
2023-07-22 01:24:17,840:INFO:Starting cross validation
2023-07-22 01:24:18,035:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:18,902:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 879, in predict_proba
    decision = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:24:19,962:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 700, in predict
    pred = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:24:21,863:INFO:Calculating mean and std
2023-07-22 01:24:21,865:INFO:Creating metrics dataframe
2023-07-22 01:24:22,040:INFO:Uploading results into container
2023-07-22 01:24:22,040:INFO:Uploading model into container now
2023-07-22 01:24:22,040:INFO:_master_model_container: 24
2023-07-22 01:24:22,040:INFO:_display_container: 3
2023-07-22 01:24:22,042:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 01:24:22,042:INFO:create_model() successfully completed......................................
2023-07-22 01:24:22,129:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:22,129:INFO:Creating metrics dataframe
2023-07-22 01:24:22,142:INFO:Initializing Gradient Boosting Classifier
2023-07-22 01:24:22,142:INFO:Total runtime is 0.7090118726094564 minutes
2023-07-22 01:24:22,145:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:22,146:INFO:Initializing create_model()
2023-07-22 01:24:22,146:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:22,146:INFO:Checking exceptions
2023-07-22 01:24:22,146:INFO:Importing libraries
2023-07-22 01:24:22,146:INFO:Copying training dataset
2023-07-22 01:24:22,152:INFO:Defining folds
2023-07-22 01:24:22,153:INFO:Declaring metric variables
2023-07-22 01:24:22,157:INFO:Importing untrained model
2023-07-22 01:24:22,161:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 01:24:22,170:INFO:Starting cross validation
2023-07-22 01:24:22,368:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:23,843:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1355, in predict_proba
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:24:25,087:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1308, in predict
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:24:25,840:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:25,853:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:25,862:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:25,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:25,906:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:26,025:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:26,028:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:26,048:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:26,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:24:27,741:INFO:Calculating mean and std
2023-07-22 01:24:27,742:INFO:Creating metrics dataframe
2023-07-22 01:24:27,918:INFO:Uploading results into container
2023-07-22 01:24:27,918:INFO:Uploading model into container now
2023-07-22 01:24:27,919:INFO:_master_model_container: 25
2023-07-22 01:24:27,919:INFO:_display_container: 3
2023-07-22 01:24:27,919:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 01:24:27,919:INFO:create_model() successfully completed......................................
2023-07-22 01:24:28,008:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:28,008:INFO:Creating metrics dataframe
2023-07-22 01:24:28,019:INFO:Initializing Linear Discriminant Analysis
2023-07-22 01:24:28,019:INFO:Total runtime is 0.8069692889849345 minutes
2023-07-22 01:24:28,022:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:28,023:INFO:Initializing create_model()
2023-07-22 01:24:28,023:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:28,023:INFO:Checking exceptions
2023-07-22 01:24:28,023:INFO:Importing libraries
2023-07-22 01:24:28,023:INFO:Copying training dataset
2023-07-22 01:24:28,031:INFO:Defining folds
2023-07-22 01:24:28,031:INFO:Declaring metric variables
2023-07-22 01:24:28,036:INFO:Importing untrained model
2023-07-22 01:24:28,039:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:24:28,046:INFO:Starting cross validation
2023-07-22 01:24:28,242:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:31,791:INFO:Calculating mean and std
2023-07-22 01:24:31,793:INFO:Creating metrics dataframe
2023-07-22 01:24:31,970:INFO:Uploading results into container
2023-07-22 01:24:31,971:INFO:Uploading model into container now
2023-07-22 01:24:31,973:INFO:_master_model_container: 26
2023-07-22 01:24:31,973:INFO:_display_container: 3
2023-07-22 01:24:31,973:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:24:31,974:INFO:create_model() successfully completed......................................
2023-07-22 01:24:32,062:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:32,062:INFO:Creating metrics dataframe
2023-07-22 01:24:32,073:INFO:Initializing Extra Trees Classifier
2023-07-22 01:24:32,073:INFO:Total runtime is 0.8745401581128438 minutes
2023-07-22 01:24:32,077:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:32,078:INFO:Initializing create_model()
2023-07-22 01:24:32,078:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:32,078:INFO:Checking exceptions
2023-07-22 01:24:32,078:INFO:Importing libraries
2023-07-22 01:24:32,078:INFO:Copying training dataset
2023-07-22 01:24:32,086:INFO:Defining folds
2023-07-22 01:24:32,087:INFO:Declaring metric variables
2023-07-22 01:24:32,092:INFO:Importing untrained model
2023-07-22 01:24:32,096:INFO:Extra Trees Classifier Imported successfully
2023-07-22 01:24:32,103:INFO:Starting cross validation
2023-07-22 01:24:32,336:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:33,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:24:34,863:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:24:36,680:INFO:Calculating mean and std
2023-07-22 01:24:36,682:INFO:Creating metrics dataframe
2023-07-22 01:24:36,859:INFO:Uploading results into container
2023-07-22 01:24:36,860:INFO:Uploading model into container now
2023-07-22 01:24:36,861:INFO:_master_model_container: 27
2023-07-22 01:24:36,861:INFO:_display_container: 3
2023-07-22 01:24:36,861:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 01:24:36,861:INFO:create_model() successfully completed......................................
2023-07-22 01:24:36,950:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:36,950:INFO:Creating metrics dataframe
2023-07-22 01:24:36,960:INFO:Initializing Extreme Gradient Boosting
2023-07-22 01:24:36,961:INFO:Total runtime is 0.9560045440991719 minutes
2023-07-22 01:24:36,965:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:36,966:INFO:Initializing create_model()
2023-07-22 01:24:36,966:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:36,966:INFO:Checking exceptions
2023-07-22 01:24:36,966:INFO:Importing libraries
2023-07-22 01:24:36,966:INFO:Copying training dataset
2023-07-22 01:24:36,974:INFO:Defining folds
2023-07-22 01:24:36,974:INFO:Declaring metric variables
2023-07-22 01:24:36,979:INFO:Importing untrained model
2023-07-22 01:24:36,983:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 01:24:36,991:INFO:Starting cross validation
2023-07-22 01:24:37,183:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:40,769:INFO:Calculating mean and std
2023-07-22 01:24:40,770:INFO:Creating metrics dataframe
2023-07-22 01:24:40,959:INFO:Uploading results into container
2023-07-22 01:24:40,960:INFO:Uploading model into container now
2023-07-22 01:24:40,961:INFO:_master_model_container: 28
2023-07-22 01:24:40,961:INFO:_display_container: 3
2023-07-22 01:24:40,961:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 01:24:40,962:INFO:create_model() successfully completed......................................
2023-07-22 01:24:41,055:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:41,055:INFO:Creating metrics dataframe
2023-07-22 01:24:41,067:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 01:24:41,068:INFO:Total runtime is 1.0244450648625691 minutes
2023-07-22 01:24:41,072:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:41,072:INFO:Initializing create_model()
2023-07-22 01:24:41,072:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:41,072:INFO:Checking exceptions
2023-07-22 01:24:41,072:INFO:Importing libraries
2023-07-22 01:24:41,072:INFO:Copying training dataset
2023-07-22 01:24:41,080:INFO:Defining folds
2023-07-22 01:24:41,080:INFO:Declaring metric variables
2023-07-22 01:24:41,085:INFO:Importing untrained model
2023-07-22 01:24:41,090:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:24:41,098:INFO:Starting cross validation
2023-07-22 01:24:41,294:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:44,918:INFO:Calculating mean and std
2023-07-22 01:24:44,919:INFO:Creating metrics dataframe
2023-07-22 01:24:45,094:INFO:Uploading results into container
2023-07-22 01:24:45,094:INFO:Uploading model into container now
2023-07-22 01:24:45,095:INFO:_master_model_container: 29
2023-07-22 01:24:45,095:INFO:_display_container: 3
2023-07-22 01:24:45,095:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:24:45,095:INFO:create_model() successfully completed......................................
2023-07-22 01:24:45,186:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:45,186:INFO:Creating metrics dataframe
2023-07-22 01:24:45,198:INFO:Initializing Dummy Classifier
2023-07-22 01:24:45,198:INFO:Total runtime is 1.0932899236679077 minutes
2023-07-22 01:24:45,202:INFO:SubProcess create_model() called ==================================
2023-07-22 01:24:45,203:INFO:Initializing create_model()
2023-07-22 01:24:45,203:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000233619D2E30>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:45,203:INFO:Checking exceptions
2023-07-22 01:24:45,203:INFO:Importing libraries
2023-07-22 01:24:45,203:INFO:Copying training dataset
2023-07-22 01:24:45,211:INFO:Defining folds
2023-07-22 01:24:45,211:INFO:Declaring metric variables
2023-07-22 01:24:45,216:INFO:Importing untrained model
2023-07-22 01:24:45,221:INFO:Dummy Classifier Imported successfully
2023-07-22 01:24:45,229:INFO:Starting cross validation
2023-07-22 01:24:45,432:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:24:46,229:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:47,992:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:48,013:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:48,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:48,060:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:48,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:48,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:48,120:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:48,133:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:48,135:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:24:49,151:INFO:Calculating mean and std
2023-07-22 01:24:49,152:INFO:Creating metrics dataframe
2023-07-22 01:24:49,338:INFO:Uploading results into container
2023-07-22 01:24:49,340:INFO:Uploading model into container now
2023-07-22 01:24:49,340:INFO:_master_model_container: 30
2023-07-22 01:24:49,341:INFO:_display_container: 3
2023-07-22 01:24:49,341:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 01:24:49,341:INFO:create_model() successfully completed......................................
2023-07-22 01:24:49,430:INFO:SubProcess create_model() end ==================================
2023-07-22 01:24:49,430:INFO:Creating metrics dataframe
2023-07-22 01:24:49,453:INFO:Initializing create_model()
2023-07-22 01:24:49,453:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:49,453:INFO:Checking exceptions
2023-07-22 01:24:49,455:INFO:Importing libraries
2023-07-22 01:24:49,455:INFO:Copying training dataset
2023-07-22 01:24:49,462:INFO:Defining folds
2023-07-22 01:24:49,462:INFO:Declaring metric variables
2023-07-22 01:24:49,463:INFO:Importing untrained model
2023-07-22 01:24:49,463:INFO:Declaring custom model
2023-07-22 01:24:49,463:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:24:49,674:INFO:Cross validation set to False
2023-07-22 01:24:49,674:INFO:Fitting Model
2023-07-22 01:24:50,336:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:24:50,337:INFO:create_model() successfully completed......................................
2023-07-22 01:24:50,427:INFO:Creating Dashboard logs
2023-07-22 01:24:50,431:INFO:Model: Light Gradient Boosting Machine
2023-07-22 01:24:50,485:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2023-07-22 01:24:50,641:INFO:Initializing predict_model()
2023-07-22 01:24:50,641:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002330122BAC0>)
2023-07-22 01:24:50,641:INFO:Checking exceptions
2023-07-22 01:24:50,641:INFO:Preloading libraries
2023-07-22 01:24:51,072:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:24:51,758:INFO:Initializing create_model()
2023-07-22 01:24:51,758:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:51,758:INFO:Checking exceptions
2023-07-22 01:24:51,761:INFO:Importing libraries
2023-07-22 01:24:51,761:INFO:Copying training dataset
2023-07-22 01:24:51,768:INFO:Defining folds
2023-07-22 01:24:51,768:INFO:Declaring metric variables
2023-07-22 01:24:51,768:INFO:Importing untrained model
2023-07-22 01:24:51,769:INFO:Declaring custom model
2023-07-22 01:24:51,769:INFO:Ridge Classifier Imported successfully
2023-07-22 01:24:51,977:INFO:Cross validation set to False
2023-07-22 01:24:51,977:INFO:Fitting Model
2023-07-22 01:24:52,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.42454e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:24:52,635:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:24:52,636:INFO:create_model() successfully completed......................................
2023-07-22 01:24:52,748:INFO:Creating Dashboard logs
2023-07-22 01:24:52,754:INFO:Model: Ridge Classifier
2023-07-22 01:24:52,855:INFO:Logged params: {'alpha': 1.0, 'class_weight': None, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 42, 'solver': 'auto', 'tol': 0.0001}
2023-07-22 01:24:53,060:INFO:Initializing predict_model()
2023-07-22 01:24:53,060:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000233012295A0>)
2023-07-22 01:24:53,060:INFO:Checking exceptions
2023-07-22 01:24:53,060:INFO:Preloading libraries
2023-07-22 01:24:53,574:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:24:54,232:INFO:Initializing create_model()
2023-07-22 01:24:54,232:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:24:54,232:INFO:Checking exceptions
2023-07-22 01:24:54,235:INFO:Importing libraries
2023-07-22 01:24:54,235:INFO:Copying training dataset
2023-07-22 01:24:54,244:INFO:Defining folds
2023-07-22 01:24:54,244:INFO:Declaring metric variables
2023-07-22 01:24:54,244:INFO:Importing untrained model
2023-07-22 01:24:54,244:INFO:Declaring custom model
2023-07-22 01:24:54,246:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:24:54,469:INFO:Cross validation set to False
2023-07-22 01:24:54,469:INFO:Fitting Model
2023-07-22 01:24:55,194:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:24:55,194:INFO:create_model() successfully completed......................................
2023-07-22 01:24:55,296:INFO:Creating Dashboard logs
2023-07-22 01:24:55,300:INFO:Model: Linear Discriminant Analysis
2023-07-22 01:24:55,358:INFO:Logged params: {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}
2023-07-22 01:24:55,507:INFO:Initializing predict_model()
2023-07-22 01:24:55,507:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023364D94310>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000023301228790>)
2023-07-22 01:24:55,507:INFO:Checking exceptions
2023-07-22 01:24:55,508:INFO:Preloading libraries
2023-07-22 01:24:55,931:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:24:56,585:INFO:Creating Dashboard logs
2023-07-22 01:24:56,589:INFO:Model: Extreme Gradient Boosting
2023-07-22 01:24:56,649:INFO:Logged params: {'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': None, 'predictor': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2023-07-22 01:24:57,494:INFO:Creating Dashboard logs
2023-07-22 01:24:57,499:INFO:Model: Gradient Boosting Classifier
2023-07-22 01:24:57,553:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-07-22 01:24:58,528:INFO:Creating Dashboard logs
2023-07-22 01:24:58,533:INFO:Model: Random Forest Classifier
2023-07-22 01:24:58,592:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
2023-07-22 01:24:59,406:INFO:Creating Dashboard logs
2023-07-22 01:24:59,411:INFO:Model: Extra Trees Classifier
2023-07-22 01:24:59,467:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
2023-07-22 01:25:00,311:INFO:Creating Dashboard logs
2023-07-22 01:25:00,315:INFO:Model: Ada Boost Classifier
2023-07-22 01:25:00,373:INFO:Logged params: {'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 42}
2023-07-22 01:25:01,245:INFO:Creating Dashboard logs
2023-07-22 01:25:01,249:INFO:Model: K Neighbors Classifier
2023-07-22 01:25:01,314:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2023-07-22 01:25:02,158:INFO:Creating Dashboard logs
2023-07-22 01:25:02,162:INFO:Model: Decision Tree Classifier
2023-07-22 01:25:02,262:INFO:Logged params: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 42, 'splitter': 'best'}
2023-07-22 01:25:03,164:INFO:Creating Dashboard logs
2023-07-22 01:25:03,169:INFO:Model: Naive Bayes
2023-07-22 01:25:03,233:INFO:Logged params: {'priors': None, 'var_smoothing': 1e-09}
2023-07-22 01:25:04,105:INFO:Creating Dashboard logs
2023-07-22 01:25:04,110:INFO:Model: Quadratic Discriminant Analysis
2023-07-22 01:25:04,201:INFO:Logged params: {'priors': None, 'reg_param': 0.0, 'store_covariance': False, 'tol': 0.0001}
2023-07-22 01:25:05,137:INFO:Creating Dashboard logs
2023-07-22 01:25:05,142:INFO:Model: SVM - Linear Kernel
2023-07-22 01:25:05,210:INFO:Logged params: {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.001, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-07-22 01:25:06,116:INFO:Creating Dashboard logs
2023-07-22 01:25:06,121:INFO:Model: Logistic Regression
2023-07-22 01:25:06,205:INFO:Logged params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
2023-07-22 01:25:07,024:INFO:Creating Dashboard logs
2023-07-22 01:25:07,027:INFO:Model: Dummy Classifier
2023-07-22 01:25:07,081:INFO:Logged params: {'constant': None, 'random_state': 42, 'strategy': 'prior'}
2023-07-22 01:25:07,840:INFO:_master_model_container: 30
2023-07-22 01:25:07,841:INFO:_display_container: 3
2023-07-22 01:25:07,842:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)]
2023-07-22 01:25:07,842:INFO:compare_models() successfully completed......................................
2023-07-22 01:26:36,038:INFO:PyCaret ClassificationExperiment
2023-07-22 01:26:36,038:INFO:Logging name: balanceado
2023-07-22 01:26:36,038:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:26:36,038:INFO:version 3.0.4
2023-07-22 01:26:36,039:INFO:Initializing setup()
2023-07-22 01:26:36,039:INFO:self.USI: 8bab
2023-07-22 01:26:36,039:INFO:self._variable_keys: {'logging_param', 'X_train', 'log_plots_param', '_available_plots', 'y_test', 'seed', 'exp_name_log', 'html_param', 'USI', 'X_test', 'fix_imbalance', 'target_param', 'y_train', 'gpu_n_jobs_param', 'fold_shuffle_param', 'data', 'memory', 'X', 'fold_groups_param', 'fold_generator', 'y', 'is_multiclass', 'n_jobs_param', 'exp_id', 'idx', 'pipeline', 'gpu_param', '_ml_usecase'}
2023-07-22 01:26:36,039:INFO:Checking environment
2023-07-22 01:26:36,039:INFO:python_version: 3.10.8
2023-07-22 01:26:36,039:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:26:36,039:INFO:machine: AMD64
2023-07-22 01:26:36,039:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:26:36,045:INFO:Memory: svmem(total=16505966592, available=2701881344, percent=83.6, used=13804085248, free=2701881344)
2023-07-22 01:26:36,045:INFO:Physical Core: 6
2023-07-22 01:26:36,045:INFO:Logical Core: 12
2023-07-22 01:26:36,045:INFO:Checking libraries
2023-07-22 01:26:36,045:INFO:System:
2023-07-22 01:26:36,045:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:26:36,045:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:26:36,045:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:26:36,045:INFO:PyCaret required dependencies:
2023-07-22 01:26:36,045:INFO:                 pip: 22.2.2
2023-07-22 01:26:36,045:INFO:          setuptools: 63.2.0
2023-07-22 01:26:36,045:INFO:             pycaret: 3.0.4
2023-07-22 01:26:36,046:INFO:             IPython: 8.11.0
2023-07-22 01:26:36,046:INFO:          ipywidgets: 8.0.7
2023-07-22 01:26:36,046:INFO:                tqdm: 4.64.1
2023-07-22 01:26:36,046:INFO:               numpy: 1.23.5
2023-07-22 01:26:36,046:INFO:              pandas: 1.5.3
2023-07-22 01:26:36,046:INFO:              jinja2: 3.1.2
2023-07-22 01:26:36,046:INFO:               scipy: 1.9.3
2023-07-22 01:26:36,046:INFO:              joblib: 1.2.0
2023-07-22 01:26:36,046:INFO:             sklearn: 1.2.2
2023-07-22 01:26:36,046:INFO:                pyod: 1.1.0
2023-07-22 01:26:36,046:INFO:            imblearn: 0.10.1
2023-07-22 01:26:36,046:INFO:   category_encoders: 2.6.1
2023-07-22 01:26:36,046:INFO:            lightgbm: 3.3.5
2023-07-22 01:26:36,046:INFO:               numba: 0.57.0
2023-07-22 01:26:36,046:INFO:            requests: 2.28.2
2023-07-22 01:26:36,046:INFO:          matplotlib: 3.7.1
2023-07-22 01:26:36,046:INFO:          scikitplot: 0.3.7
2023-07-22 01:26:36,046:INFO:         yellowbrick: 1.5
2023-07-22 01:26:36,046:INFO:              plotly: 5.15.0
2023-07-22 01:26:36,046:INFO:    plotly-resampler: Not installed
2023-07-22 01:26:36,046:INFO:             kaleido: 0.2.1
2023-07-22 01:26:36,046:INFO:           schemdraw: 0.15
2023-07-22 01:26:36,046:INFO:         statsmodels: 0.13.5
2023-07-22 01:26:36,046:INFO:              sktime: 0.21.0
2023-07-22 01:26:36,046:INFO:               tbats: 1.1.3
2023-07-22 01:26:36,046:INFO:            pmdarima: 2.0.3
2023-07-22 01:26:36,046:INFO:              psutil: 5.9.4
2023-07-22 01:26:36,046:INFO:          markupsafe: 2.1.2
2023-07-22 01:26:36,046:INFO:             pickle5: Not installed
2023-07-22 01:26:36,046:INFO:         cloudpickle: 2.2.1
2023-07-22 01:26:36,047:INFO:         deprecation: 2.1.0
2023-07-22 01:26:36,047:INFO:              xxhash: 3.2.0
2023-07-22 01:26:36,047:INFO:           wurlitzer: Not installed
2023-07-22 01:26:36,047:INFO:PyCaret optional dependencies:
2023-07-22 01:26:36,047:INFO:                shap: 0.41.0
2023-07-22 01:26:36,047:INFO:           interpret: 0.4.2
2023-07-22 01:26:36,047:INFO:                umap: 0.5.3
2023-07-22 01:26:36,047:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:26:36,047:INFO:  explainerdashboard: Not installed
2023-07-22 01:26:36,047:INFO:             autoviz: Not installed
2023-07-22 01:26:36,047:INFO:           fairlearn: Not installed
2023-07-22 01:26:36,047:INFO:          deepchecks: Not installed
2023-07-22 01:26:36,047:INFO:             xgboost: 1.7.6
2023-07-22 01:26:36,047:INFO:            catboost: Not installed
2023-07-22 01:26:36,047:INFO:              kmodes: Not installed
2023-07-22 01:26:36,047:INFO:             mlxtend: Not installed
2023-07-22 01:26:36,047:INFO:       statsforecast: Not installed
2023-07-22 01:26:36,047:INFO:        tune_sklearn: Not installed
2023-07-22 01:26:36,047:INFO:                 ray: Not installed
2023-07-22 01:26:36,047:INFO:            hyperopt: Not installed
2023-07-22 01:26:36,047:INFO:              optuna: 3.2.0
2023-07-22 01:26:36,047:INFO:               skopt: Not installed
2023-07-22 01:26:36,047:INFO:              mlflow: 2.4.2
2023-07-22 01:26:36,047:INFO:              gradio: Not installed
2023-07-22 01:26:36,047:INFO:             fastapi: 0.95.2
2023-07-22 01:26:36,047:INFO:             uvicorn: 0.22.0
2023-07-22 01:26:36,047:INFO:              m2cgen: Not installed
2023-07-22 01:26:36,047:INFO:           evidently: Not installed
2023-07-22 01:26:36,047:INFO:               fugue: Not installed
2023-07-22 01:26:36,048:INFO:           streamlit: Not installed
2023-07-22 01:26:36,048:INFO:             prophet: Not installed
2023-07-22 01:26:36,048:INFO:None
2023-07-22 01:26:36,048:INFO:Set up data.
2023-07-22 01:26:36,073:INFO:Set up train/test split.
2023-07-22 01:26:36,073:INFO:Set up data.
2023-07-22 01:26:36,092:INFO:Set up index.
2023-07-22 01:26:36,092:INFO:Set up folding strategy.
2023-07-22 01:26:36,092:INFO:Assigning column types.
2023-07-22 01:26:36,099:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:26:36,149:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,150:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,180:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,182:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:36,237:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,237:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,266:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,270:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:36,270:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:26:36,316:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,344:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,347:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:36,394:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,422:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,425:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:36,425:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:26:36,501:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,504:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:36,580:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,583:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:36,585:INFO:Preparing preprocessing pipeline...
2023-07-22 01:26:36,587:INFO:Set up iterative imputation.
2023-07-22 01:26:36,588:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,592:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,598:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,662:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:26:36,709:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,711:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:36,787:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,790:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:36,819:INFO:Set up encoding of categorical features.
2023-07-22 01:26:36,819:INFO:Set up imbalanced handling.
2023-07-22 01:26:36,819:INFO:Set up column transformation.
2023-07-22 01:26:36,820:INFO:Set up feature selection.
2023-07-22 01:26:36,892:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:36,896:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:37,391:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:26:37,422:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:26:37,422:INFO:Creating final display dataframe.
2023-07-22 01:26:38,456:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment  MlflowLogger
31                  Experiment Name    balanceado
32                              USI          8bab
2023-07-22 01:26:38,559:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:38,562:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:38,642:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:26:38,645:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:26:38,646:INFO:Logging experiment in loggers
2023-07-22 01:26:38,759:INFO:SubProcess save_model() called ==================================
2023-07-22 01:26:38,803:INFO:Initializing save_model()
2023-07-22 01:26:38,803:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), model_name=C:\TEMP\tmpwty5uiyv\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2023-07-22 01:26:38,803:INFO:Adding model into prep_pipe
2023-07-22 01:26:38,803:WARNING:Only Model saved as it was a pipeline.
2023-07-22 01:26:38,989:INFO:C:\TEMP\tmpwty5uiyv\Transformation Pipeline.pkl saved in current working directory
2023-07-22 01:26:39,018:INFO:Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:26:39,018:INFO:save_model() successfully completed......................................
2023-07-22 01:26:39,123:INFO:SubProcess save_model() end ==================================
2023-07-22 01:26:39,211:INFO:setup() successfully completed in 2.73s...............
2023-07-22 01:27:23,539:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:27:23,539:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:27:23,539:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:27:23,539:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:27:34,407:INFO:PyCaret ClassificationExperiment
2023-07-22 01:27:34,407:INFO:Logging name: balanceado
2023-07-22 01:27:34,407:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:27:34,407:INFO:version 3.0.4
2023-07-22 01:27:34,407:INFO:Initializing setup()
2023-07-22 01:27:34,407:INFO:self.USI: 6052
2023-07-22 01:27:34,407:INFO:self._variable_keys: {'data', 'fold_generator', 'target_param', 'exp_id', 'y_test', '_available_plots', 'n_jobs_param', '_ml_usecase', 'html_param', 'logging_param', 'is_multiclass', 'gpu_n_jobs_param', 'idx', 'X_train', 'seed', 'y', 'memory', 'fold_shuffle_param', 'log_plots_param', 'fold_groups_param', 'X', 'USI', 'fix_imbalance', 'pipeline', 'exp_name_log', 'X_test', 'gpu_param', 'y_train'}
2023-07-22 01:27:34,407:INFO:Checking environment
2023-07-22 01:27:34,407:INFO:python_version: 3.10.8
2023-07-22 01:27:34,407:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:27:34,407:INFO:machine: AMD64
2023-07-22 01:27:34,407:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:27:34,413:INFO:Memory: svmem(total=16505966592, available=4383969280, percent=73.4, used=12121997312, free=4383969280)
2023-07-22 01:27:34,414:INFO:Physical Core: 6
2023-07-22 01:27:34,414:INFO:Logical Core: 12
2023-07-22 01:27:34,414:INFO:Checking libraries
2023-07-22 01:27:34,414:INFO:System:
2023-07-22 01:27:34,414:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:27:34,414:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:27:34,414:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:27:34,414:INFO:PyCaret required dependencies:
2023-07-22 01:27:34,417:INFO:                 pip: 22.2.2
2023-07-22 01:27:34,417:INFO:          setuptools: 63.2.0
2023-07-22 01:27:34,417:INFO:             pycaret: 3.0.4
2023-07-22 01:27:34,417:INFO:             IPython: 8.11.0
2023-07-22 01:27:34,417:INFO:          ipywidgets: 8.0.7
2023-07-22 01:27:34,417:INFO:                tqdm: 4.64.1
2023-07-22 01:27:34,417:INFO:               numpy: 1.23.5
2023-07-22 01:27:34,417:INFO:              pandas: 1.5.3
2023-07-22 01:27:34,417:INFO:              jinja2: 3.1.2
2023-07-22 01:27:34,417:INFO:               scipy: 1.9.3
2023-07-22 01:27:34,417:INFO:              joblib: 1.2.0
2023-07-22 01:27:34,417:INFO:             sklearn: 1.2.2
2023-07-22 01:27:34,417:INFO:                pyod: 1.1.0
2023-07-22 01:27:34,417:INFO:            imblearn: 0.10.1
2023-07-22 01:27:34,417:INFO:   category_encoders: 2.6.1
2023-07-22 01:27:34,417:INFO:            lightgbm: 3.3.5
2023-07-22 01:27:34,417:INFO:               numba: 0.57.0
2023-07-22 01:27:34,417:INFO:            requests: 2.28.2
2023-07-22 01:27:34,417:INFO:          matplotlib: 3.7.1
2023-07-22 01:27:34,417:INFO:          scikitplot: 0.3.7
2023-07-22 01:27:34,417:INFO:         yellowbrick: 1.5
2023-07-22 01:27:34,417:INFO:              plotly: 5.15.0
2023-07-22 01:27:34,417:INFO:    plotly-resampler: Not installed
2023-07-22 01:27:34,417:INFO:             kaleido: 0.2.1
2023-07-22 01:27:34,417:INFO:           schemdraw: 0.15
2023-07-22 01:27:34,417:INFO:         statsmodels: 0.13.5
2023-07-22 01:27:34,418:INFO:              sktime: 0.21.0
2023-07-22 01:27:34,418:INFO:               tbats: 1.1.3
2023-07-22 01:27:34,418:INFO:            pmdarima: 2.0.3
2023-07-22 01:27:34,418:INFO:              psutil: 5.9.4
2023-07-22 01:27:34,418:INFO:          markupsafe: 2.1.2
2023-07-22 01:27:34,418:INFO:             pickle5: Not installed
2023-07-22 01:27:34,418:INFO:         cloudpickle: 2.2.1
2023-07-22 01:27:34,418:INFO:         deprecation: 2.1.0
2023-07-22 01:27:34,418:INFO:              xxhash: 3.2.0
2023-07-22 01:27:34,418:INFO:           wurlitzer: Not installed
2023-07-22 01:27:34,418:INFO:PyCaret optional dependencies:
2023-07-22 01:27:34,835:INFO:                shap: 0.41.0
2023-07-22 01:27:34,835:INFO:           interpret: 0.4.2
2023-07-22 01:27:34,835:INFO:                umap: 0.5.3
2023-07-22 01:27:34,835:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:27:34,835:INFO:  explainerdashboard: Not installed
2023-07-22 01:27:34,835:INFO:             autoviz: Not installed
2023-07-22 01:27:34,835:INFO:           fairlearn: Not installed
2023-07-22 01:27:34,835:INFO:          deepchecks: Not installed
2023-07-22 01:27:34,835:INFO:             xgboost: 1.7.6
2023-07-22 01:27:34,835:INFO:            catboost: Not installed
2023-07-22 01:27:34,835:INFO:              kmodes: Not installed
2023-07-22 01:27:34,836:INFO:             mlxtend: Not installed
2023-07-22 01:27:34,836:INFO:       statsforecast: Not installed
2023-07-22 01:27:34,836:INFO:        tune_sklearn: Not installed
2023-07-22 01:27:34,836:INFO:                 ray: Not installed
2023-07-22 01:27:34,836:INFO:            hyperopt: Not installed
2023-07-22 01:27:34,836:INFO:              optuna: 3.2.0
2023-07-22 01:27:34,836:INFO:               skopt: Not installed
2023-07-22 01:27:34,836:INFO:              mlflow: 2.4.2
2023-07-22 01:27:34,836:INFO:              gradio: Not installed
2023-07-22 01:27:34,836:INFO:             fastapi: 0.95.2
2023-07-22 01:27:34,836:INFO:             uvicorn: 0.22.0
2023-07-22 01:27:34,836:INFO:              m2cgen: Not installed
2023-07-22 01:27:34,836:INFO:           evidently: Not installed
2023-07-22 01:27:34,836:INFO:               fugue: Not installed
2023-07-22 01:27:34,836:INFO:           streamlit: Not installed
2023-07-22 01:27:34,836:INFO:             prophet: Not installed
2023-07-22 01:27:34,836:INFO:None
2023-07-22 01:27:34,836:INFO:Set up data.
2023-07-22 01:27:34,863:INFO:Set up train/test split.
2023-07-22 01:27:34,863:INFO:Set up data.
2023-07-22 01:27:34,883:INFO:Set up index.
2023-07-22 01:27:34,883:INFO:Set up folding strategy.
2023-07-22 01:27:34,884:INFO:Assigning column types.
2023-07-22 01:27:34,889:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:27:34,940:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:27:34,942:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:27:34,980:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:34,983:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:35,033:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:27:35,034:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:27:35,063:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:35,065:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:35,066:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:27:35,111:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:27:35,138:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:35,141:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:35,189:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:27:35,219:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:35,222:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:35,223:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:27:35,297:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:35,301:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:35,375:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:35,378:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:35,380:INFO:Preparing preprocessing pipeline...
2023-07-22 01:27:35,381:INFO:Set up iterative imputation.
2023-07-22 01:27:35,382:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:27:35,386:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:27:35,391:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:27:35,452:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:27:35,497:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:35,499:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:35,574:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:35,577:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:35,609:INFO:Set up encoding of categorical features.
2023-07-22 01:27:35,609:INFO:Set up imbalanced handling.
2023-07-22 01:27:35,609:INFO:Set up column transformation.
2023-07-22 01:27:35,609:INFO:Set up feature selection.
2023-07-22 01:27:35,682:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:35,685:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:36,319:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:27:36,348:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:27:36,348:INFO:Creating final display dataframe.
2023-07-22 01:27:37,193:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment  MlflowLogger
31                  Experiment Name    balanceado
32                              USI          6052
2023-07-22 01:27:37,296:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:37,299:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:37,375:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:27:37,378:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:27:37,378:INFO:Logging experiment in loggers
2023-07-22 01:27:37,749:INFO:SubProcess save_model() called ==================================
2023-07-22 01:27:37,792:INFO:Initializing save_model()
2023-07-22 01:27:37,792:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), model_name=C:\TEMP\tmpcc14csxf\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2023-07-22 01:27:37,792:INFO:Adding model into prep_pipe
2023-07-22 01:27:37,792:WARNING:Only Model saved as it was a pipeline.
2023-07-22 01:27:37,977:INFO:C:\TEMP\tmpcc14csxf\Transformation Pipeline.pkl saved in current working directory
2023-07-22 01:27:38,003:INFO:Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:27:38,004:INFO:save_model() successfully completed......................................
2023-07-22 01:27:38,099:INFO:SubProcess save_model() end ==================================
2023-07-22 01:27:38,157:INFO:setup() successfully completed in 3.1s...............
2023-07-22 01:29:48,388:WARNING:C:\TEMP\ipykernel_24284\3943853602.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).
  ipython.magic("matplotlib inline")

2023-07-22 01:29:59,356:INFO:PyCaret ClassificationExperiment
2023-07-22 01:29:59,356:INFO:Logging name: balanceado
2023-07-22 01:29:59,356:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:29:59,356:INFO:version 3.0.4
2023-07-22 01:29:59,356:INFO:Initializing setup()
2023-07-22 01:29:59,357:INFO:self.USI: a11f
2023-07-22 01:29:59,357:INFO:self._variable_keys: {'data', 'fold_generator', 'target_param', 'exp_id', 'y_test', '_available_plots', 'n_jobs_param', '_ml_usecase', 'html_param', 'logging_param', 'is_multiclass', 'gpu_n_jobs_param', 'idx', 'X_train', 'seed', 'y', 'memory', 'fold_shuffle_param', 'log_plots_param', 'fold_groups_param', 'X', 'USI', 'fix_imbalance', 'pipeline', 'exp_name_log', 'X_test', 'gpu_param', 'y_train'}
2023-07-22 01:29:59,357:INFO:Checking environment
2023-07-22 01:29:59,357:INFO:python_version: 3.10.8
2023-07-22 01:29:59,357:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:29:59,357:INFO:machine: AMD64
2023-07-22 01:29:59,357:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:29:59,363:INFO:Memory: svmem(total=16505966592, available=4169166848, percent=74.7, used=12336799744, free=4169166848)
2023-07-22 01:29:59,363:INFO:Physical Core: 6
2023-07-22 01:29:59,363:INFO:Logical Core: 12
2023-07-22 01:29:59,363:INFO:Checking libraries
2023-07-22 01:29:59,363:INFO:System:
2023-07-22 01:29:59,363:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:29:59,363:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:29:59,363:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:29:59,363:INFO:PyCaret required dependencies:
2023-07-22 01:29:59,363:INFO:                 pip: 22.2.2
2023-07-22 01:29:59,363:INFO:          setuptools: 63.2.0
2023-07-22 01:29:59,363:INFO:             pycaret: 3.0.4
2023-07-22 01:29:59,363:INFO:             IPython: 8.11.0
2023-07-22 01:29:59,363:INFO:          ipywidgets: 8.0.7
2023-07-22 01:29:59,363:INFO:                tqdm: 4.64.1
2023-07-22 01:29:59,363:INFO:               numpy: 1.23.5
2023-07-22 01:29:59,363:INFO:              pandas: 1.5.3
2023-07-22 01:29:59,363:INFO:              jinja2: 3.1.2
2023-07-22 01:29:59,363:INFO:               scipy: 1.9.3
2023-07-22 01:29:59,363:INFO:              joblib: 1.2.0
2023-07-22 01:29:59,363:INFO:             sklearn: 1.2.2
2023-07-22 01:29:59,363:INFO:                pyod: 1.1.0
2023-07-22 01:29:59,363:INFO:            imblearn: 0.10.1
2023-07-22 01:29:59,363:INFO:   category_encoders: 2.6.1
2023-07-22 01:29:59,363:INFO:            lightgbm: 3.3.5
2023-07-22 01:29:59,363:INFO:               numba: 0.57.0
2023-07-22 01:29:59,363:INFO:            requests: 2.28.2
2023-07-22 01:29:59,363:INFO:          matplotlib: 3.7.1
2023-07-22 01:29:59,363:INFO:          scikitplot: 0.3.7
2023-07-22 01:29:59,363:INFO:         yellowbrick: 1.5
2023-07-22 01:29:59,363:INFO:              plotly: 5.15.0
2023-07-22 01:29:59,363:INFO:    plotly-resampler: Not installed
2023-07-22 01:29:59,363:INFO:             kaleido: 0.2.1
2023-07-22 01:29:59,363:INFO:           schemdraw: 0.15
2023-07-22 01:29:59,365:INFO:         statsmodels: 0.13.5
2023-07-22 01:29:59,365:INFO:              sktime: 0.21.0
2023-07-22 01:29:59,365:INFO:               tbats: 1.1.3
2023-07-22 01:29:59,365:INFO:            pmdarima: 2.0.3
2023-07-22 01:29:59,366:INFO:              psutil: 5.9.4
2023-07-22 01:29:59,366:INFO:          markupsafe: 2.1.2
2023-07-22 01:29:59,366:INFO:             pickle5: Not installed
2023-07-22 01:29:59,366:INFO:         cloudpickle: 2.2.1
2023-07-22 01:29:59,366:INFO:         deprecation: 2.1.0
2023-07-22 01:29:59,366:INFO:              xxhash: 3.2.0
2023-07-22 01:29:59,366:INFO:           wurlitzer: Not installed
2023-07-22 01:29:59,366:INFO:PyCaret optional dependencies:
2023-07-22 01:29:59,366:INFO:                shap: 0.41.0
2023-07-22 01:29:59,366:INFO:           interpret: 0.4.2
2023-07-22 01:29:59,366:INFO:                umap: 0.5.3
2023-07-22 01:29:59,366:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:29:59,366:INFO:  explainerdashboard: Not installed
2023-07-22 01:29:59,366:INFO:             autoviz: Not installed
2023-07-22 01:29:59,366:INFO:           fairlearn: Not installed
2023-07-22 01:29:59,367:INFO:          deepchecks: Not installed
2023-07-22 01:29:59,367:INFO:             xgboost: 1.7.6
2023-07-22 01:29:59,367:INFO:            catboost: Not installed
2023-07-22 01:29:59,367:INFO:              kmodes: Not installed
2023-07-22 01:29:59,367:INFO:             mlxtend: Not installed
2023-07-22 01:29:59,367:INFO:       statsforecast: Not installed
2023-07-22 01:29:59,367:INFO:        tune_sklearn: Not installed
2023-07-22 01:29:59,367:INFO:                 ray: Not installed
2023-07-22 01:29:59,367:INFO:            hyperopt: Not installed
2023-07-22 01:29:59,367:INFO:              optuna: 3.2.0
2023-07-22 01:29:59,367:INFO:               skopt: Not installed
2023-07-22 01:29:59,367:INFO:              mlflow: 2.4.2
2023-07-22 01:29:59,367:INFO:              gradio: Not installed
2023-07-22 01:29:59,367:INFO:             fastapi: 0.95.2
2023-07-22 01:29:59,367:INFO:             uvicorn: 0.22.0
2023-07-22 01:29:59,367:INFO:              m2cgen: Not installed
2023-07-22 01:29:59,367:INFO:           evidently: Not installed
2023-07-22 01:29:59,367:INFO:               fugue: Not installed
2023-07-22 01:29:59,367:INFO:           streamlit: Not installed
2023-07-22 01:29:59,367:INFO:             prophet: Not installed
2023-07-22 01:29:59,367:INFO:None
2023-07-22 01:29:59,367:INFO:Set up data.
2023-07-22 01:29:59,395:INFO:Set up train/test split.
2023-07-22 01:29:59,395:INFO:Set up data.
2023-07-22 01:29:59,419:INFO:Set up index.
2023-07-22 01:29:59,419:INFO:Set up folding strategy.
2023-07-22 01:29:59,419:INFO:Assigning column types.
2023-07-22 01:29:59,426:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:29:59,477:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:29:59,478:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:29:59,511:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:29:59,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:29:59,559:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:29:59,560:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:29:59,590:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:29:59,593:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:29:59,593:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:29:59,640:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:29:59,674:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:29:59,678:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:29:59,731:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:29:59,761:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:29:59,764:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:29:59,765:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:29:59,852:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:29:59,855:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:29:59,938:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:29:59,941:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:29:59,941:INFO:Preparing preprocessing pipeline...
2023-07-22 01:29:59,942:INFO:Set up iterative imputation.
2023-07-22 01:29:59,943:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:29:59,948:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:29:59,953:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:30:00,023:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:30:00,079:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:30:00,081:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:30:00,169:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:30:00,172:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:30:00,208:INFO:Set up encoding of categorical features.
2023-07-22 01:30:00,208:INFO:Set up imbalanced handling.
2023-07-22 01:30:00,208:INFO:Set up column transformation.
2023-07-22 01:30:00,208:INFO:Set up feature selection.
2023-07-22 01:30:00,284:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:30:00,287:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:30:00,768:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:30:00,799:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:30:00,799:INFO:Creating final display dataframe.
2023-07-22 01:30:02,870:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment  MlflowLogger
31                  Experiment Name    balanceado
32                              USI          a11f
2023-07-22 01:30:02,978:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:30:02,982:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:30:03,062:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:30:03,065:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:30:03,065:INFO:Logging experiment in loggers
2023-07-22 01:30:03,171:INFO:SubProcess save_model() called ==================================
2023-07-22 01:30:03,214:INFO:Initializing save_model()
2023-07-22 01:30:03,214:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), model_name=C:\TEMP\tmpmueuz_vo\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2023-07-22 01:30:03,214:INFO:Adding model into prep_pipe
2023-07-22 01:30:03,214:WARNING:Only Model saved as it was a pipeline.
2023-07-22 01:30:03,395:INFO:C:\TEMP\tmpmueuz_vo\Transformation Pipeline.pkl saved in current working directory
2023-07-22 01:30:03,424:INFO:Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:30:03,424:INFO:save_model() successfully completed......................................
2023-07-22 01:30:03,527:INFO:SubProcess save_model() end ==================================
2023-07-22 01:30:03,591:INFO:setup() successfully completed in 3.85s...............
2023-07-22 01:30:13,608:INFO:Initializing compare_models()
2023-07-22 01:30:13,608:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:30:13,608:INFO:Checking exceptions
2023-07-22 01:30:13,614:INFO:Preparing display monitor
2023-07-22 01:30:13,646:INFO:Initializing Logistic Regression
2023-07-22 01:30:13,647:INFO:Total runtime is 1.6657511393229165e-05 minutes
2023-07-22 01:30:13,652:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:13,652:INFO:Initializing create_model()
2023-07-22 01:30:13,652:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:13,652:INFO:Checking exceptions
2023-07-22 01:30:13,653:INFO:Importing libraries
2023-07-22 01:30:13,653:INFO:Copying training dataset
2023-07-22 01:30:13,663:INFO:Defining folds
2023-07-22 01:30:13,663:INFO:Declaring metric variables
2023-07-22 01:30:13,667:INFO:Importing untrained model
2023-07-22 01:30:13,670:INFO:Logistic Regression Imported successfully
2023-07-22 01:30:13,679:INFO:Starting cross validation
2023-07-22 01:30:14,003:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:21,574:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:21,645:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:21,722:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:21,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:21,835:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:21,851:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:21,883:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:21,908:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:21,919:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:30:23,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:23,040:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:23,150:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:23,182:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:23,268:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:23,301:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:23,313:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:23,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:23,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:24,324:INFO:Calculating mean and std
2023-07-22 01:30:24,326:INFO:Creating metrics dataframe
2023-07-22 01:30:24,507:INFO:Uploading results into container
2023-07-22 01:30:24,507:INFO:Uploading model into container now
2023-07-22 01:30:24,508:INFO:_master_model_container: 1
2023-07-22 01:30:24,508:INFO:_display_container: 2
2023-07-22 01:30:24,508:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 01:30:24,509:INFO:create_model() successfully completed......................................
2023-07-22 01:30:24,591:INFO:SubProcess create_model() end ==================================
2023-07-22 01:30:24,592:INFO:Creating metrics dataframe
2023-07-22 01:30:24,605:INFO:Initializing K Neighbors Classifier
2023-07-22 01:30:24,605:INFO:Total runtime is 0.1826499342918396 minutes
2023-07-22 01:30:24,609:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:24,610:INFO:Initializing create_model()
2023-07-22 01:30:24,610:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:24,610:INFO:Checking exceptions
2023-07-22 01:30:24,610:INFO:Importing libraries
2023-07-22 01:30:24,610:INFO:Copying training dataset
2023-07-22 01:30:24,617:INFO:Defining folds
2023-07-22 01:30:24,618:INFO:Declaring metric variables
2023-07-22 01:30:24,622:INFO:Importing untrained model
2023-07-22 01:30:24,626:INFO:K Neighbors Classifier Imported successfully
2023-07-22 01:30:24,633:INFO:Starting cross validation
2023-07-22 01:30:24,840:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:30,908:INFO:Calculating mean and std
2023-07-22 01:30:30,910:INFO:Creating metrics dataframe
2023-07-22 01:30:31,115:INFO:Uploading results into container
2023-07-22 01:30:31,116:INFO:Uploading model into container now
2023-07-22 01:30:31,116:INFO:_master_model_container: 2
2023-07-22 01:30:31,116:INFO:_display_container: 2
2023-07-22 01:30:31,116:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 01:30:31,117:INFO:create_model() successfully completed......................................
2023-07-22 01:30:31,203:INFO:SubProcess create_model() end ==================================
2023-07-22 01:30:31,203:INFO:Creating metrics dataframe
2023-07-22 01:30:31,214:INFO:Initializing Naive Bayes
2023-07-22 01:30:31,214:INFO:Total runtime is 0.29280045032501223 minutes
2023-07-22 01:30:31,218:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:31,218:INFO:Initializing create_model()
2023-07-22 01:30:31,219:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:31,219:INFO:Checking exceptions
2023-07-22 01:30:31,219:INFO:Importing libraries
2023-07-22 01:30:31,219:INFO:Copying training dataset
2023-07-22 01:30:31,227:INFO:Defining folds
2023-07-22 01:30:31,228:INFO:Declaring metric variables
2023-07-22 01:30:31,232:INFO:Importing untrained model
2023-07-22 01:30:31,236:INFO:Naive Bayes Imported successfully
2023-07-22 01:30:31,251:INFO:Starting cross validation
2023-07-22 01:30:31,458:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:35,454:INFO:Calculating mean and std
2023-07-22 01:30:35,455:INFO:Creating metrics dataframe
2023-07-22 01:30:35,648:INFO:Uploading results into container
2023-07-22 01:30:35,649:INFO:Uploading model into container now
2023-07-22 01:30:35,650:INFO:_master_model_container: 3
2023-07-22 01:30:35,650:INFO:_display_container: 2
2023-07-22 01:30:35,650:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 01:30:35,651:INFO:create_model() successfully completed......................................
2023-07-22 01:30:35,738:INFO:SubProcess create_model() end ==================================
2023-07-22 01:30:35,738:INFO:Creating metrics dataframe
2023-07-22 01:30:35,749:INFO:Initializing Decision Tree Classifier
2023-07-22 01:30:35,749:INFO:Total runtime is 0.3683831612269084 minutes
2023-07-22 01:30:35,753:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:35,753:INFO:Initializing create_model()
2023-07-22 01:30:35,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:35,753:INFO:Checking exceptions
2023-07-22 01:30:35,754:INFO:Importing libraries
2023-07-22 01:30:35,754:INFO:Copying training dataset
2023-07-22 01:30:35,764:INFO:Defining folds
2023-07-22 01:30:35,764:INFO:Declaring metric variables
2023-07-22 01:30:35,770:INFO:Importing untrained model
2023-07-22 01:30:35,775:INFO:Decision Tree Classifier Imported successfully
2023-07-22 01:30:35,784:INFO:Starting cross validation
2023-07-22 01:30:36,001:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:36,824:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:30:37,826:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 426, in predict
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:30:39,846:INFO:Calculating mean and std
2023-07-22 01:30:39,848:INFO:Creating metrics dataframe
2023-07-22 01:30:40,048:INFO:Uploading results into container
2023-07-22 01:30:40,048:INFO:Uploading model into container now
2023-07-22 01:30:40,049:INFO:_master_model_container: 4
2023-07-22 01:30:40,049:INFO:_display_container: 2
2023-07-22 01:30:40,050:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 01:30:40,050:INFO:create_model() successfully completed......................................
2023-07-22 01:30:40,145:INFO:SubProcess create_model() end ==================================
2023-07-22 01:30:40,145:INFO:Creating metrics dataframe
2023-07-22 01:30:40,158:INFO:Initializing SVM - Linear Kernel
2023-07-22 01:30:40,158:INFO:Total runtime is 0.44187417825063074 minutes
2023-07-22 01:30:40,163:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:40,163:INFO:Initializing create_model()
2023-07-22 01:30:40,163:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:40,163:INFO:Checking exceptions
2023-07-22 01:30:40,163:INFO:Importing libraries
2023-07-22 01:30:40,164:INFO:Copying training dataset
2023-07-22 01:30:40,173:INFO:Defining folds
2023-07-22 01:30:40,173:INFO:Declaring metric variables
2023-07-22 01:30:40,179:INFO:Importing untrained model
2023-07-22 01:30:40,185:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 01:30:40,195:INFO:Starting cross validation
2023-07-22 01:30:40,414:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:41,078:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,373:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,417:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,472:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,514:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,519:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,530:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:42,535:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:30:43,505:INFO:Calculating mean and std
2023-07-22 01:30:43,507:INFO:Creating metrics dataframe
2023-07-22 01:30:43,692:INFO:Uploading results into container
2023-07-22 01:30:43,693:INFO:Uploading model into container now
2023-07-22 01:30:43,693:INFO:_master_model_container: 5
2023-07-22 01:30:43,693:INFO:_display_container: 2
2023-07-22 01:30:43,694:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 01:30:43,694:INFO:create_model() successfully completed......................................
2023-07-22 01:30:43,776:INFO:SubProcess create_model() end ==================================
2023-07-22 01:30:43,776:INFO:Creating metrics dataframe
2023-07-22 01:30:43,787:INFO:Initializing Ridge Classifier
2023-07-22 01:30:43,788:INFO:Total runtime is 0.5023717641830445 minutes
2023-07-22 01:30:43,793:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:43,793:INFO:Initializing create_model()
2023-07-22 01:30:43,793:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:43,793:INFO:Checking exceptions
2023-07-22 01:30:43,793:INFO:Importing libraries
2023-07-22 01:30:43,793:INFO:Copying training dataset
2023-07-22 01:30:43,801:INFO:Defining folds
2023-07-22 01:30:43,801:INFO:Declaring metric variables
2023-07-22 01:30:43,806:INFO:Importing untrained model
2023-07-22 01:30:43,811:INFO:Ridge Classifier Imported successfully
2023-07-22 01:30:43,818:INFO:Starting cross validation
2023-07-22 01:30:44,019:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:44,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:45,107:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.22379e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,135:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.39835e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,142:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.79155e-52): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,236:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.44723e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.07304e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,253:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.92242e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.66771e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.85819e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.49256e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:30:45,829:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:45,831:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:45,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:45,956:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:45,989:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:45,994:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:45,994:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:46,013:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:46,016:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:30:47,097:INFO:Calculating mean and std
2023-07-22 01:30:47,099:INFO:Creating metrics dataframe
2023-07-22 01:30:47,289:INFO:Uploading results into container
2023-07-22 01:30:47,290:INFO:Uploading model into container now
2023-07-22 01:30:47,290:INFO:_master_model_container: 6
2023-07-22 01:30:47,290:INFO:_display_container: 2
2023-07-22 01:30:47,290:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:30:47,290:INFO:create_model() successfully completed......................................
2023-07-22 01:30:47,374:INFO:SubProcess create_model() end ==================================
2023-07-22 01:30:47,374:INFO:Creating metrics dataframe
2023-07-22 01:30:47,384:INFO:Initializing Random Forest Classifier
2023-07-22 01:30:47,385:INFO:Total runtime is 0.5623279889424643 minutes
2023-07-22 01:30:47,390:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:47,390:INFO:Initializing create_model()
2023-07-22 01:30:47,391:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:47,391:INFO:Checking exceptions
2023-07-22 01:30:47,391:INFO:Importing libraries
2023-07-22 01:30:47,391:INFO:Copying training dataset
2023-07-22 01:30:47,398:INFO:Defining folds
2023-07-22 01:30:47,398:INFO:Declaring metric variables
2023-07-22 01:30:47,403:INFO:Importing untrained model
2023-07-22 01:30:47,408:INFO:Random Forest Classifier Imported successfully
2023-07-22 01:30:47,418:INFO:Starting cross validation
2023-07-22 01:30:47,621:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:48,737:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:30:50,226:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:30:52,185:INFO:Calculating mean and std
2023-07-22 01:30:52,186:INFO:Creating metrics dataframe
2023-07-22 01:30:52,376:INFO:Uploading results into container
2023-07-22 01:30:52,377:INFO:Uploading model into container now
2023-07-22 01:30:52,378:INFO:_master_model_container: 7
2023-07-22 01:30:52,378:INFO:_display_container: 2
2023-07-22 01:30:52,380:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 01:30:52,381:INFO:create_model() successfully completed......................................
2023-07-22 01:30:52,526:INFO:SubProcess create_model() end ==================================
2023-07-22 01:30:52,526:INFO:Creating metrics dataframe
2023-07-22 01:30:52,535:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 01:30:52,535:INFO:Total runtime is 0.6481543978055319 minutes
2023-07-22 01:30:52,543:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:52,543:INFO:Initializing create_model()
2023-07-22 01:30:52,543:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:52,543:INFO:Checking exceptions
2023-07-22 01:30:52,543:INFO:Importing libraries
2023-07-22 01:30:52,543:INFO:Copying training dataset
2023-07-22 01:30:52,551:INFO:Defining folds
2023-07-22 01:30:52,551:INFO:Declaring metric variables
2023-07-22 01:30:52,554:INFO:Importing untrained model
2023-07-22 01:30:52,558:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 01:30:52,567:INFO:Starting cross validation
2023-07-22 01:30:52,769:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:53,947:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:53,978:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:54,001:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:54,024:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:54,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:54,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:54,050:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:54,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:54,101:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:30:54,706:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,706:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,707:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:54,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,744:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:54,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:54,758:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:54,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,774:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:54,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,784:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:54,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,788:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,788:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:54,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:54,928:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,928:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:54,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,440:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,441:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,441:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,445:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,450:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:55,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,460:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,462:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,462:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,462:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,463:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,474:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:55,493:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,493:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,494:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,496:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,496:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,497:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,498:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,500:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:55,506:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:55,506:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,507:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,507:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,508:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,510:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,511:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:55,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:55,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,523:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,723:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,723:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:30:55,724:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:30:55,728:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:30:55,736:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:30:56,784:INFO:Calculating mean and std
2023-07-22 01:30:56,786:INFO:Creating metrics dataframe
2023-07-22 01:30:57,031:INFO:Uploading results into container
2023-07-22 01:30:57,032:INFO:Uploading model into container now
2023-07-22 01:30:57,033:INFO:_master_model_container: 8
2023-07-22 01:30:57,033:INFO:_display_container: 2
2023-07-22 01:30:57,034:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 01:30:57,034:INFO:create_model() successfully completed......................................
2023-07-22 01:30:57,122:INFO:SubProcess create_model() end ==================================
2023-07-22 01:30:57,122:INFO:Creating metrics dataframe
2023-07-22 01:30:57,135:INFO:Initializing Ada Boost Classifier
2023-07-22 01:30:57,135:INFO:Total runtime is 0.7248299360275269 minutes
2023-07-22 01:30:57,141:INFO:SubProcess create_model() called ==================================
2023-07-22 01:30:57,141:INFO:Initializing create_model()
2023-07-22 01:30:57,141:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:30:57,141:INFO:Checking exceptions
2023-07-22 01:30:57,141:INFO:Importing libraries
2023-07-22 01:30:57,141:INFO:Copying training dataset
2023-07-22 01:30:57,150:INFO:Defining folds
2023-07-22 01:30:57,150:INFO:Declaring metric variables
2023-07-22 01:30:57,155:INFO:Importing untrained model
2023-07-22 01:30:57,161:INFO:Ada Boost Classifier Imported successfully
2023-07-22 01:30:57,172:INFO:Starting cross validation
2023-07-22 01:30:57,373:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:30:58,370:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 879, in predict_proba
    decision = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:30:59,443:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 700, in predict
    pred = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:31:01,300:INFO:Calculating mean and std
2023-07-22 01:31:01,301:INFO:Creating metrics dataframe
2023-07-22 01:31:01,562:INFO:Uploading results into container
2023-07-22 01:31:01,564:INFO:Uploading model into container now
2023-07-22 01:31:01,564:INFO:_master_model_container: 9
2023-07-22 01:31:01,564:INFO:_display_container: 2
2023-07-22 01:31:01,564:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 01:31:01,564:INFO:create_model() successfully completed......................................
2023-07-22 01:31:01,648:INFO:SubProcess create_model() end ==================================
2023-07-22 01:31:01,648:INFO:Creating metrics dataframe
2023-07-22 01:31:01,663:INFO:Initializing Gradient Boosting Classifier
2023-07-22 01:31:01,663:INFO:Total runtime is 0.8002846280733744 minutes
2023-07-22 01:31:01,667:INFO:SubProcess create_model() called ==================================
2023-07-22 01:31:01,667:INFO:Initializing create_model()
2023-07-22 01:31:01,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:01,667:INFO:Checking exceptions
2023-07-22 01:31:01,667:INFO:Importing libraries
2023-07-22 01:31:01,667:INFO:Copying training dataset
2023-07-22 01:31:01,675:INFO:Defining folds
2023-07-22 01:31:01,675:INFO:Declaring metric variables
2023-07-22 01:31:01,682:INFO:Importing untrained model
2023-07-22 01:31:01,686:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 01:31:01,697:INFO:Starting cross validation
2023-07-22 01:31:01,898:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:31:02,814:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1355, in predict_proba
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:31:03,996:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1308, in predict
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:31:05,757:INFO:Calculating mean and std
2023-07-22 01:31:05,759:INFO:Creating metrics dataframe
2023-07-22 01:31:06,005:INFO:Uploading results into container
2023-07-22 01:31:06,006:INFO:Uploading model into container now
2023-07-22 01:31:06,007:INFO:_master_model_container: 10
2023-07-22 01:31:06,007:INFO:_display_container: 2
2023-07-22 01:31:06,008:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 01:31:06,008:INFO:create_model() successfully completed......................................
2023-07-22 01:31:06,093:INFO:SubProcess create_model() end ==================================
2023-07-22 01:31:06,093:INFO:Creating metrics dataframe
2023-07-22 01:31:06,106:INFO:Initializing Linear Discriminant Analysis
2023-07-22 01:31:06,106:INFO:Total runtime is 0.874334450562795 minutes
2023-07-22 01:31:06,110:INFO:SubProcess create_model() called ==================================
2023-07-22 01:31:06,111:INFO:Initializing create_model()
2023-07-22 01:31:06,111:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:06,111:INFO:Checking exceptions
2023-07-22 01:31:06,111:INFO:Importing libraries
2023-07-22 01:31:06,111:INFO:Copying training dataset
2023-07-22 01:31:06,121:INFO:Defining folds
2023-07-22 01:31:06,121:INFO:Declaring metric variables
2023-07-22 01:31:06,126:INFO:Importing untrained model
2023-07-22 01:31:06,131:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:31:06,143:INFO:Starting cross validation
2023-07-22 01:31:06,358:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:31:10,147:INFO:Calculating mean and std
2023-07-22 01:31:10,148:INFO:Creating metrics dataframe
2023-07-22 01:31:10,403:INFO:Uploading results into container
2023-07-22 01:31:10,404:INFO:Uploading model into container now
2023-07-22 01:31:10,405:INFO:_master_model_container: 11
2023-07-22 01:31:10,405:INFO:_display_container: 2
2023-07-22 01:31:10,406:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:31:10,406:INFO:create_model() successfully completed......................................
2023-07-22 01:31:10,490:INFO:SubProcess create_model() end ==================================
2023-07-22 01:31:10,490:INFO:Creating metrics dataframe
2023-07-22 01:31:10,504:INFO:Initializing Extra Trees Classifier
2023-07-22 01:31:10,504:INFO:Total runtime is 0.9476341366767883 minutes
2023-07-22 01:31:10,509:INFO:SubProcess create_model() called ==================================
2023-07-22 01:31:10,509:INFO:Initializing create_model()
2023-07-22 01:31:10,510:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:10,510:INFO:Checking exceptions
2023-07-22 01:31:10,510:INFO:Importing libraries
2023-07-22 01:31:10,510:INFO:Copying training dataset
2023-07-22 01:31:10,519:INFO:Defining folds
2023-07-22 01:31:10,519:INFO:Declaring metric variables
2023-07-22 01:31:10,525:INFO:Importing untrained model
2023-07-22 01:31:10,532:INFO:Extra Trees Classifier Imported successfully
2023-07-22 01:31:10,542:INFO:Starting cross validation
2023-07-22 01:31:10,769:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:31:12,055:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:31:13,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:31:15,537:INFO:Calculating mean and std
2023-07-22 01:31:15,538:INFO:Creating metrics dataframe
2023-07-22 01:31:15,791:INFO:Uploading results into container
2023-07-22 01:31:15,793:INFO:Uploading model into container now
2023-07-22 01:31:15,794:INFO:_master_model_container: 12
2023-07-22 01:31:15,795:INFO:_display_container: 2
2023-07-22 01:31:15,795:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 01:31:15,795:INFO:create_model() successfully completed......................................
2023-07-22 01:31:15,878:INFO:SubProcess create_model() end ==================================
2023-07-22 01:31:15,878:INFO:Creating metrics dataframe
2023-07-22 01:31:15,889:INFO:Initializing Extreme Gradient Boosting
2023-07-22 01:31:15,890:INFO:Total runtime is 1.0373874068260194 minutes
2023-07-22 01:31:15,893:INFO:SubProcess create_model() called ==================================
2023-07-22 01:31:15,893:INFO:Initializing create_model()
2023-07-22 01:31:15,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:15,893:INFO:Checking exceptions
2023-07-22 01:31:15,893:INFO:Importing libraries
2023-07-22 01:31:15,893:INFO:Copying training dataset
2023-07-22 01:31:15,902:INFO:Defining folds
2023-07-22 01:31:15,902:INFO:Declaring metric variables
2023-07-22 01:31:15,905:INFO:Importing untrained model
2023-07-22 01:31:15,913:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 01:31:15,924:INFO:Starting cross validation
2023-07-22 01:31:16,127:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:31:20,710:INFO:Calculating mean and std
2023-07-22 01:31:20,711:INFO:Creating metrics dataframe
2023-07-22 01:31:20,962:INFO:Uploading results into container
2023-07-22 01:31:20,963:INFO:Uploading model into container now
2023-07-22 01:31:20,963:INFO:_master_model_container: 13
2023-07-22 01:31:20,963:INFO:_display_container: 2
2023-07-22 01:31:20,964:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 01:31:20,964:INFO:create_model() successfully completed......................................
2023-07-22 01:31:21,055:INFO:SubProcess create_model() end ==================================
2023-07-22 01:31:21,055:INFO:Creating metrics dataframe
2023-07-22 01:31:21,070:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 01:31:21,070:INFO:Total runtime is 1.1237333099047344 minutes
2023-07-22 01:31:21,074:INFO:SubProcess create_model() called ==================================
2023-07-22 01:31:21,074:INFO:Initializing create_model()
2023-07-22 01:31:21,075:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:21,075:INFO:Checking exceptions
2023-07-22 01:31:21,075:INFO:Importing libraries
2023-07-22 01:31:21,075:INFO:Copying training dataset
2023-07-22 01:31:21,083:INFO:Defining folds
2023-07-22 01:31:21,084:INFO:Declaring metric variables
2023-07-22 01:31:21,088:INFO:Importing untrained model
2023-07-22 01:31:21,093:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:31:21,104:INFO:Starting cross validation
2023-07-22 01:31:21,306:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:31:25,137:INFO:Calculating mean and std
2023-07-22 01:31:25,138:INFO:Creating metrics dataframe
2023-07-22 01:31:25,393:INFO:Uploading results into container
2023-07-22 01:31:25,393:INFO:Uploading model into container now
2023-07-22 01:31:25,394:INFO:_master_model_container: 14
2023-07-22 01:31:25,395:INFO:_display_container: 2
2023-07-22 01:31:25,395:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:31:25,396:INFO:create_model() successfully completed......................................
2023-07-22 01:31:25,480:INFO:SubProcess create_model() end ==================================
2023-07-22 01:31:25,480:INFO:Creating metrics dataframe
2023-07-22 01:31:25,492:INFO:Initializing Dummy Classifier
2023-07-22 01:31:25,492:INFO:Total runtime is 1.1974453965822858 minutes
2023-07-22 01:31:25,498:INFO:SubProcess create_model() called ==================================
2023-07-22 01:31:25,498:INFO:Initializing create_model()
2023-07-22 01:31:25,498:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C214580>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:25,498:INFO:Checking exceptions
2023-07-22 01:31:25,499:INFO:Importing libraries
2023-07-22 01:31:25,499:INFO:Copying training dataset
2023-07-22 01:31:25,507:INFO:Defining folds
2023-07-22 01:31:25,507:INFO:Declaring metric variables
2023-07-22 01:31:25,514:INFO:Importing untrained model
2023-07-22 01:31:25,518:INFO:Dummy Classifier Imported successfully
2023-07-22 01:31:25,532:INFO:Starting cross validation
2023-07-22 01:31:25,740:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:31:26,631:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,372:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,372:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,385:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,436:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,467:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,491:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:28,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:31:29,646:INFO:Calculating mean and std
2023-07-22 01:31:29,649:INFO:Creating metrics dataframe
2023-07-22 01:31:29,908:INFO:Uploading results into container
2023-07-22 01:31:29,909:INFO:Uploading model into container now
2023-07-22 01:31:29,910:INFO:_master_model_container: 15
2023-07-22 01:31:29,910:INFO:_display_container: 2
2023-07-22 01:31:29,910:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 01:31:29,911:INFO:create_model() successfully completed......................................
2023-07-22 01:31:30,007:INFO:SubProcess create_model() end ==================================
2023-07-22 01:31:30,007:INFO:Creating metrics dataframe
2023-07-22 01:31:30,037:INFO:Initializing create_model()
2023-07-22 01:31:30,038:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:30,038:INFO:Checking exceptions
2023-07-22 01:31:30,040:INFO:Importing libraries
2023-07-22 01:31:30,040:INFO:Copying training dataset
2023-07-22 01:31:30,050:INFO:Defining folds
2023-07-22 01:31:30,050:INFO:Declaring metric variables
2023-07-22 01:31:30,051:INFO:Importing untrained model
2023-07-22 01:31:30,051:INFO:Declaring custom model
2023-07-22 01:31:30,052:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:31:30,274:INFO:Cross validation set to False
2023-07-22 01:31:30,274:INFO:Fitting Model
2023-07-22 01:31:30,903:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:31:30,903:INFO:create_model() successfully completed......................................
2023-07-22 01:31:30,998:INFO:Creating Dashboard logs
2023-07-22 01:31:31,004:INFO:Model: Light Gradient Boosting Machine
2023-07-22 01:31:31,107:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2023-07-22 01:31:31,320:INFO:Initializing predict_model()
2023-07-22 01:31:31,320:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000265498372E0>)
2023-07-22 01:31:31,320:INFO:Checking exceptions
2023-07-22 01:31:31,320:INFO:Preloading libraries
2023-07-22 01:31:31,843:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:31:31,847:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\_distutils_hack\__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")

2023-07-22 01:31:32,503:INFO:Initializing create_model()
2023-07-22 01:31:32,503:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:32,503:INFO:Checking exceptions
2023-07-22 01:31:32,506:INFO:Importing libraries
2023-07-22 01:31:32,506:INFO:Copying training dataset
2023-07-22 01:31:32,513:INFO:Defining folds
2023-07-22 01:31:32,513:INFO:Declaring metric variables
2023-07-22 01:31:32,513:INFO:Importing untrained model
2023-07-22 01:31:32,513:INFO:Declaring custom model
2023-07-22 01:31:32,514:INFO:Ridge Classifier Imported successfully
2023-07-22 01:31:32,714:INFO:Cross validation set to False
2023-07-22 01:31:32,714:INFO:Fitting Model
2023-07-22 01:31:33,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.42454e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:31:33,338:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:31:33,338:INFO:create_model() successfully completed......................................
2023-07-22 01:31:33,427:INFO:Creating Dashboard logs
2023-07-22 01:31:33,432:INFO:Model: Ridge Classifier
2023-07-22 01:31:33,512:INFO:Logged params: {'alpha': 1.0, 'class_weight': None, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 42, 'solver': 'auto', 'tol': 0.0001}
2023-07-22 01:31:33,661:INFO:Initializing predict_model()
2023-07-22 01:31:33,661:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000265499464D0>)
2023-07-22 01:31:33,661:INFO:Checking exceptions
2023-07-22 01:31:33,661:INFO:Preloading libraries
2023-07-22 01:31:34,093:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:31:34,777:INFO:Initializing create_model()
2023-07-22 01:31:34,777:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:31:34,777:INFO:Checking exceptions
2023-07-22 01:31:34,779:INFO:Importing libraries
2023-07-22 01:31:34,780:INFO:Copying training dataset
2023-07-22 01:31:34,787:INFO:Defining folds
2023-07-22 01:31:34,787:INFO:Declaring metric variables
2023-07-22 01:31:34,787:INFO:Importing untrained model
2023-07-22 01:31:34,787:INFO:Declaring custom model
2023-07-22 01:31:34,787:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:31:34,985:INFO:Cross validation set to False
2023-07-22 01:31:34,985:INFO:Fitting Model
2023-07-22 01:31:35,608:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:31:35,608:INFO:create_model() successfully completed......................................
2023-07-22 01:31:35,697:INFO:Creating Dashboard logs
2023-07-22 01:31:35,704:INFO:Model: Linear Discriminant Analysis
2023-07-22 01:31:35,804:INFO:Logged params: {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}
2023-07-22 01:31:35,959:INFO:Initializing predict_model()
2023-07-22 01:31:35,959:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000026549946710>)
2023-07-22 01:31:35,959:INFO:Checking exceptions
2023-07-22 01:31:35,959:INFO:Preloading libraries
2023-07-22 01:31:36,378:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:31:37,029:INFO:Creating Dashboard logs
2023-07-22 01:31:37,034:INFO:Model: Extreme Gradient Boosting
2023-07-22 01:31:37,090:INFO:Logged params: {'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': None, 'predictor': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2023-07-22 01:31:37,914:INFO:Creating Dashboard logs
2023-07-22 01:31:37,917:INFO:Model: Gradient Boosting Classifier
2023-07-22 01:31:38,009:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-07-22 01:31:38,834:INFO:Creating Dashboard logs
2023-07-22 01:31:38,839:INFO:Model: Random Forest Classifier
2023-07-22 01:31:38,898:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
2023-07-22 01:31:39,720:INFO:Creating Dashboard logs
2023-07-22 01:31:39,724:INFO:Model: Extra Trees Classifier
2023-07-22 01:31:39,784:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
2023-07-22 01:31:40,569:INFO:Creating Dashboard logs
2023-07-22 01:31:40,572:INFO:Model: Ada Boost Classifier
2023-07-22 01:31:40,656:INFO:Logged params: {'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 42}
2023-07-22 01:31:41,430:INFO:Creating Dashboard logs
2023-07-22 01:31:41,434:INFO:Model: K Neighbors Classifier
2023-07-22 01:31:41,496:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2023-07-22 01:31:42,301:INFO:Creating Dashboard logs
2023-07-22 01:31:42,306:INFO:Model: Decision Tree Classifier
2023-07-22 01:31:42,381:INFO:Logged params: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 42, 'splitter': 'best'}
2023-07-22 01:31:43,136:INFO:Creating Dashboard logs
2023-07-22 01:31:43,141:INFO:Model: Naive Bayes
2023-07-22 01:31:43,202:INFO:Logged params: {'priors': None, 'var_smoothing': 1e-09}
2023-07-22 01:31:43,990:INFO:Creating Dashboard logs
2023-07-22 01:31:43,994:INFO:Model: Quadratic Discriminant Analysis
2023-07-22 01:31:44,051:INFO:Logged params: {'priors': None, 'reg_param': 0.0, 'store_covariance': False, 'tol': 0.0001}
2023-07-22 01:31:44,903:INFO:Creating Dashboard logs
2023-07-22 01:31:44,908:INFO:Model: SVM - Linear Kernel
2023-07-22 01:31:44,972:INFO:Logged params: {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.001, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-07-22 01:31:45,854:INFO:Creating Dashboard logs
2023-07-22 01:31:45,859:INFO:Model: Logistic Regression
2023-07-22 01:31:45,924:INFO:Logged params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
2023-07-22 01:31:46,771:INFO:Creating Dashboard logs
2023-07-22 01:31:46,772:INFO:Model: Dummy Classifier
2023-07-22 01:31:46,836:INFO:Logged params: {'constant': None, 'random_state': 42, 'strategy': 'prior'}
2023-07-22 01:31:47,615:INFO:_master_model_container: 15
2023-07-22 01:31:47,616:INFO:_display_container: 2
2023-07-22 01:31:47,617:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)]
2023-07-22 01:31:47,617:INFO:compare_models() successfully completed......................................
2023-07-22 01:35:34,288:INFO:Initializing plot_model()
2023-07-22 01:35:34,288:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)], feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, system=True)
2023-07-22 01:35:34,288:INFO:Checking exceptions
2023-07-22 01:36:13,523:INFO:Initializing compare_models()
2023-07-22 01:36:13,523:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:36:13,523:INFO:Checking exceptions
2023-07-22 01:36:13,528:INFO:Preparing display monitor
2023-07-22 01:36:13,553:INFO:Initializing Logistic Regression
2023-07-22 01:36:13,553:INFO:Total runtime is 0.0 minutes
2023-07-22 01:36:13,558:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:13,559:INFO:Initializing create_model()
2023-07-22 01:36:13,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:13,559:INFO:Checking exceptions
2023-07-22 01:36:13,559:INFO:Importing libraries
2023-07-22 01:36:13,559:INFO:Copying training dataset
2023-07-22 01:36:13,569:INFO:Defining folds
2023-07-22 01:36:13,569:INFO:Declaring metric variables
2023-07-22 01:36:13,573:INFO:Importing untrained model
2023-07-22 01:36:13,579:INFO:Logistic Regression Imported successfully
2023-07-22 01:36:13,588:INFO:Starting cross validation
2023-07-22 01:36:13,882:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:15,157:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:15,174:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:15,199:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:15,214:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:15,254:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:15,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:15,294:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:15,305:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:15,316:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:36:16,539:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:16,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:16,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:16,645:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:16,648:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:16,663:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:16,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:16,720:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:16,764:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:17,731:INFO:Calculating mean and std
2023-07-22 01:36:17,732:INFO:Creating metrics dataframe
2023-07-22 01:36:17,918:INFO:Uploading results into container
2023-07-22 01:36:17,919:INFO:Uploading model into container now
2023-07-22 01:36:17,919:INFO:_master_model_container: 16
2023-07-22 01:36:17,920:INFO:_display_container: 3
2023-07-22 01:36:17,920:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 01:36:17,920:INFO:create_model() successfully completed......................................
2023-07-22 01:36:18,019:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:18,019:INFO:Creating metrics dataframe
2023-07-22 01:36:18,028:INFO:Initializing K Neighbors Classifier
2023-07-22 01:36:18,029:INFO:Total runtime is 0.07461327314376831 minutes
2023-07-22 01:36:18,033:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:18,034:INFO:Initializing create_model()
2023-07-22 01:36:18,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:18,035:INFO:Checking exceptions
2023-07-22 01:36:18,035:INFO:Importing libraries
2023-07-22 01:36:18,035:INFO:Copying training dataset
2023-07-22 01:36:18,044:INFO:Defining folds
2023-07-22 01:36:18,044:INFO:Declaring metric variables
2023-07-22 01:36:18,048:INFO:Importing untrained model
2023-07-22 01:36:18,052:INFO:K Neighbors Classifier Imported successfully
2023-07-22 01:36:18,060:INFO:Starting cross validation
2023-07-22 01:36:18,262:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:22,187:INFO:Calculating mean and std
2023-07-22 01:36:22,189:INFO:Creating metrics dataframe
2023-07-22 01:36:22,384:INFO:Uploading results into container
2023-07-22 01:36:22,385:INFO:Uploading model into container now
2023-07-22 01:36:22,385:INFO:_master_model_container: 17
2023-07-22 01:36:22,385:INFO:_display_container: 3
2023-07-22 01:36:22,385:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 01:36:22,386:INFO:create_model() successfully completed......................................
2023-07-22 01:36:22,491:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:22,491:INFO:Creating metrics dataframe
2023-07-22 01:36:22,500:INFO:Initializing Naive Bayes
2023-07-22 01:36:22,500:INFO:Total runtime is 0.14912310043970745 minutes
2023-07-22 01:36:22,504:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:22,505:INFO:Initializing create_model()
2023-07-22 01:36:22,505:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:22,505:INFO:Checking exceptions
2023-07-22 01:36:22,505:INFO:Importing libraries
2023-07-22 01:36:22,505:INFO:Copying training dataset
2023-07-22 01:36:22,512:INFO:Defining folds
2023-07-22 01:36:22,512:INFO:Declaring metric variables
2023-07-22 01:36:22,518:INFO:Importing untrained model
2023-07-22 01:36:22,523:INFO:Naive Bayes Imported successfully
2023-07-22 01:36:22,530:INFO:Starting cross validation
2023-07-22 01:36:22,741:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:26,388:INFO:Calculating mean and std
2023-07-22 01:36:26,389:INFO:Creating metrics dataframe
2023-07-22 01:36:26,576:INFO:Uploading results into container
2023-07-22 01:36:26,577:INFO:Uploading model into container now
2023-07-22 01:36:26,577:INFO:_master_model_container: 18
2023-07-22 01:36:26,579:INFO:_display_container: 3
2023-07-22 01:36:26,579:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 01:36:26,579:INFO:create_model() successfully completed......................................
2023-07-22 01:36:26,681:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:26,681:INFO:Creating metrics dataframe
2023-07-22 01:36:26,689:INFO:Initializing Decision Tree Classifier
2023-07-22 01:36:26,689:INFO:Total runtime is 0.21894689003626505 minutes
2023-07-22 01:36:26,693:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:26,693:INFO:Initializing create_model()
2023-07-22 01:36:26,693:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:26,693:INFO:Checking exceptions
2023-07-22 01:36:26,693:INFO:Importing libraries
2023-07-22 01:36:26,695:INFO:Copying training dataset
2023-07-22 01:36:26,702:INFO:Defining folds
2023-07-22 01:36:26,703:INFO:Declaring metric variables
2023-07-22 01:36:26,706:INFO:Importing untrained model
2023-07-22 01:36:26,710:INFO:Decision Tree Classifier Imported successfully
2023-07-22 01:36:26,719:INFO:Starting cross validation
2023-07-22 01:36:26,917:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:27,864:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:36:28,907:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 426, in predict
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:36:30,776:INFO:Calculating mean and std
2023-07-22 01:36:30,777:INFO:Creating metrics dataframe
2023-07-22 01:36:30,981:INFO:Uploading results into container
2023-07-22 01:36:30,981:INFO:Uploading model into container now
2023-07-22 01:36:30,983:INFO:_master_model_container: 19
2023-07-22 01:36:30,983:INFO:_display_container: 3
2023-07-22 01:36:30,983:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 01:36:30,984:INFO:create_model() successfully completed......................................
2023-07-22 01:36:31,089:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:31,090:INFO:Creating metrics dataframe
2023-07-22 01:36:31,099:INFO:Initializing SVM - Linear Kernel
2023-07-22 01:36:31,099:INFO:Total runtime is 0.2924437443415324 minutes
2023-07-22 01:36:31,103:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:31,104:INFO:Initializing create_model()
2023-07-22 01:36:31,104:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:31,104:INFO:Checking exceptions
2023-07-22 01:36:31,104:INFO:Importing libraries
2023-07-22 01:36:31,104:INFO:Copying training dataset
2023-07-22 01:36:31,111:INFO:Defining folds
2023-07-22 01:36:31,111:INFO:Declaring metric variables
2023-07-22 01:36:31,116:INFO:Importing untrained model
2023-07-22 01:36:31,122:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 01:36:31,129:INFO:Starting cross validation
2023-07-22 01:36:31,332:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:31,952:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,174:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,190:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,215:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,243:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,294:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,335:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,400:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:33,406:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:36:34,366:INFO:Calculating mean and std
2023-07-22 01:36:34,367:INFO:Creating metrics dataframe
2023-07-22 01:36:34,543:INFO:Uploading results into container
2023-07-22 01:36:34,544:INFO:Uploading model into container now
2023-07-22 01:36:34,546:INFO:_master_model_container: 20
2023-07-22 01:36:34,546:INFO:_display_container: 3
2023-07-22 01:36:34,547:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 01:36:34,547:INFO:create_model() successfully completed......................................
2023-07-22 01:36:34,646:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:34,646:INFO:Creating metrics dataframe
2023-07-22 01:36:34,656:INFO:Initializing Ridge Classifier
2023-07-22 01:36:34,656:INFO:Total runtime is 0.3517191529273987 minutes
2023-07-22 01:36:34,660:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:34,660:INFO:Initializing create_model()
2023-07-22 01:36:34,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:34,661:INFO:Checking exceptions
2023-07-22 01:36:34,661:INFO:Importing libraries
2023-07-22 01:36:34,661:INFO:Copying training dataset
2023-07-22 01:36:34,671:INFO:Defining folds
2023-07-22 01:36:34,671:INFO:Declaring metric variables
2023-07-22 01:36:34,677:INFO:Importing untrained model
2023-07-22 01:36:34,681:INFO:Ridge Classifier Imported successfully
2023-07-22 01:36:34,691:INFO:Starting cross validation
2023-07-22 01:36:34,891:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:35,511:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:35,983:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.66771e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,011:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.22379e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.79155e-52): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,046:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.39835e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,078:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.07304e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,086:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.44723e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.92242e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,099:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.85819e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,137:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.49256e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:36:36,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:36,711:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:36,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:36,763:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:36,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:36,788:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:36,789:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:36,798:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:36,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:36:37,898:INFO:Calculating mean and std
2023-07-22 01:36:37,899:INFO:Creating metrics dataframe
2023-07-22 01:36:38,102:INFO:Uploading results into container
2023-07-22 01:36:38,103:INFO:Uploading model into container now
2023-07-22 01:36:38,103:INFO:_master_model_container: 21
2023-07-22 01:36:38,103:INFO:_display_container: 3
2023-07-22 01:36:38,104:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:36:38,104:INFO:create_model() successfully completed......................................
2023-07-22 01:36:38,209:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:38,209:INFO:Creating metrics dataframe
2023-07-22 01:36:38,219:INFO:Initializing Random Forest Classifier
2023-07-22 01:36:38,220:INFO:Total runtime is 0.41112775802612306 minutes
2023-07-22 01:36:38,224:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:38,224:INFO:Initializing create_model()
2023-07-22 01:36:38,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:38,224:INFO:Checking exceptions
2023-07-22 01:36:38,224:INFO:Importing libraries
2023-07-22 01:36:38,224:INFO:Copying training dataset
2023-07-22 01:36:38,232:INFO:Defining folds
2023-07-22 01:36:38,232:INFO:Declaring metric variables
2023-07-22 01:36:38,238:INFO:Importing untrained model
2023-07-22 01:36:38,242:INFO:Random Forest Classifier Imported successfully
2023-07-22 01:36:38,250:INFO:Starting cross validation
2023-07-22 01:36:38,449:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:39,461:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:36:40,877:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:36:42,993:INFO:Calculating mean and std
2023-07-22 01:36:42,993:INFO:Creating metrics dataframe
2023-07-22 01:36:43,182:INFO:Uploading results into container
2023-07-22 01:36:43,183:INFO:Uploading model into container now
2023-07-22 01:36:43,183:INFO:_master_model_container: 22
2023-07-22 01:36:43,184:INFO:_display_container: 3
2023-07-22 01:36:43,184:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 01:36:43,185:INFO:create_model() successfully completed......................................
2023-07-22 01:36:43,302:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:43,302:INFO:Creating metrics dataframe
2023-07-22 01:36:43,314:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 01:36:43,315:INFO:Total runtime is 0.4960411826769511 minutes
2023-07-22 01:36:43,320:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:43,320:INFO:Initializing create_model()
2023-07-22 01:36:43,320:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:43,320:INFO:Checking exceptions
2023-07-22 01:36:43,320:INFO:Importing libraries
2023-07-22 01:36:43,321:INFO:Copying training dataset
2023-07-22 01:36:43,330:INFO:Defining folds
2023-07-22 01:36:43,330:INFO:Declaring metric variables
2023-07-22 01:36:43,337:INFO:Importing untrained model
2023-07-22 01:36:43,342:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 01:36:43,351:INFO:Starting cross validation
2023-07-22 01:36:43,576:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:44,735:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:44,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:44,817:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:44,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:44,862:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:44,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:44,890:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:44,913:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:44,918:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:36:45,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,425:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:45,483:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,484:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,484:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:45,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:45,539:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,539:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,540:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:45,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,558:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:45,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:45,610:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,616:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,616:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,616:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:45,617:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:45,617:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,114:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,114:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,115:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,116:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,121:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:46,168:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,168:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,168:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,170:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,177:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:46,197:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,197:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,198:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,205:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:46,225:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,225:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,225:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,226:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,226:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,227:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,229:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,230:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,234:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:46,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,259:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,260:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,262:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,312:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,313:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,313:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,322:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:46,331:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,331:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,331:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,331:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,331:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:36:46,333:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:36:46,333:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,336:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:36:46,338:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:46,342:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:36:47,357:INFO:Calculating mean and std
2023-07-22 01:36:47,358:INFO:Creating metrics dataframe
2023-07-22 01:36:47,538:INFO:Uploading results into container
2023-07-22 01:36:47,539:INFO:Uploading model into container now
2023-07-22 01:36:47,540:INFO:_master_model_container: 23
2023-07-22 01:36:47,540:INFO:_display_container: 3
2023-07-22 01:36:47,540:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 01:36:47,541:INFO:create_model() successfully completed......................................
2023-07-22 01:36:47,638:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:47,638:INFO:Creating metrics dataframe
2023-07-22 01:36:47,649:INFO:Initializing Ada Boost Classifier
2023-07-22 01:36:47,650:INFO:Total runtime is 0.5682915647824605 minutes
2023-07-22 01:36:47,654:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:47,654:INFO:Initializing create_model()
2023-07-22 01:36:47,654:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:47,654:INFO:Checking exceptions
2023-07-22 01:36:47,654:INFO:Importing libraries
2023-07-22 01:36:47,654:INFO:Copying training dataset
2023-07-22 01:36:47,662:INFO:Defining folds
2023-07-22 01:36:47,663:INFO:Declaring metric variables
2023-07-22 01:36:47,666:INFO:Importing untrained model
2023-07-22 01:36:47,672:INFO:Ada Boost Classifier Imported successfully
2023-07-22 01:36:47,679:INFO:Starting cross validation
2023-07-22 01:36:47,892:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:48,908:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 879, in predict_proba
    decision = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:36:50,022:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 700, in predict
    pred = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:36:51,933:INFO:Calculating mean and std
2023-07-22 01:36:51,934:INFO:Creating metrics dataframe
2023-07-22 01:36:52,132:INFO:Uploading results into container
2023-07-22 01:36:52,133:INFO:Uploading model into container now
2023-07-22 01:36:52,134:INFO:_master_model_container: 24
2023-07-22 01:36:52,134:INFO:_display_container: 3
2023-07-22 01:36:52,134:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 01:36:52,134:INFO:create_model() successfully completed......................................
2023-07-22 01:36:52,249:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:52,249:INFO:Creating metrics dataframe
2023-07-22 01:36:52,261:INFO:Initializing Gradient Boosting Classifier
2023-07-22 01:36:52,261:INFO:Total runtime is 0.6451369841893513 minutes
2023-07-22 01:36:52,265:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:52,266:INFO:Initializing create_model()
2023-07-22 01:36:52,266:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:52,266:INFO:Checking exceptions
2023-07-22 01:36:52,266:INFO:Importing libraries
2023-07-22 01:36:52,266:INFO:Copying training dataset
2023-07-22 01:36:52,275:INFO:Defining folds
2023-07-22 01:36:52,275:INFO:Declaring metric variables
2023-07-22 01:36:52,280:INFO:Importing untrained model
2023-07-22 01:36:52,284:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 01:36:52,293:INFO:Starting cross validation
2023-07-22 01:36:52,511:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:36:53,513:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1355, in predict_proba
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:36:54,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1308, in predict
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:36:56,857:INFO:Calculating mean and std
2023-07-22 01:36:56,859:INFO:Creating metrics dataframe
2023-07-22 01:36:57,055:INFO:Uploading results into container
2023-07-22 01:36:57,056:INFO:Uploading model into container now
2023-07-22 01:36:57,057:INFO:_master_model_container: 25
2023-07-22 01:36:57,057:INFO:_display_container: 3
2023-07-22 01:36:57,058:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 01:36:57,059:INFO:create_model() successfully completed......................................
2023-07-22 01:36:57,171:INFO:SubProcess create_model() end ==================================
2023-07-22 01:36:57,171:INFO:Creating metrics dataframe
2023-07-22 01:36:57,183:INFO:Initializing Linear Discriminant Analysis
2023-07-22 01:36:57,183:INFO:Total runtime is 0.7271711190541584 minutes
2023-07-22 01:36:57,186:INFO:SubProcess create_model() called ==================================
2023-07-22 01:36:57,186:INFO:Initializing create_model()
2023-07-22 01:36:57,186:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:36:57,186:INFO:Checking exceptions
2023-07-22 01:36:57,187:INFO:Importing libraries
2023-07-22 01:36:57,187:INFO:Copying training dataset
2023-07-22 01:36:57,195:INFO:Defining folds
2023-07-22 01:36:57,195:INFO:Declaring metric variables
2023-07-22 01:36:57,199:INFO:Importing untrained model
2023-07-22 01:36:57,205:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:36:57,212:INFO:Starting cross validation
2023-07-22 01:36:57,422:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:37:01,664:INFO:Calculating mean and std
2023-07-22 01:37:01,665:INFO:Creating metrics dataframe
2023-07-22 01:37:01,896:INFO:Uploading results into container
2023-07-22 01:37:01,897:INFO:Uploading model into container now
2023-07-22 01:37:01,898:INFO:_master_model_container: 26
2023-07-22 01:37:01,899:INFO:_display_container: 3
2023-07-22 01:37:01,899:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:37:01,900:INFO:create_model() successfully completed......................................
2023-07-22 01:37:02,017:INFO:SubProcess create_model() end ==================================
2023-07-22 01:37:02,026:INFO:Creating metrics dataframe
2023-07-22 01:37:02,037:INFO:Initializing Extra Trees Classifier
2023-07-22 01:37:02,038:INFO:Total runtime is 0.8080850442250569 minutes
2023-07-22 01:37:02,042:INFO:SubProcess create_model() called ==================================
2023-07-22 01:37:02,042:INFO:Initializing create_model()
2023-07-22 01:37:02,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:37:02,042:INFO:Checking exceptions
2023-07-22 01:37:02,042:INFO:Importing libraries
2023-07-22 01:37:02,042:INFO:Copying training dataset
2023-07-22 01:37:02,050:INFO:Defining folds
2023-07-22 01:37:02,050:INFO:Declaring metric variables
2023-07-22 01:37:02,054:INFO:Importing untrained model
2023-07-22 01:37:02,060:INFO:Extra Trees Classifier Imported successfully
2023-07-22 01:37:02,069:INFO:Starting cross validation
2023-07-22 01:37:02,276:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:37:03,547:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:37:05,006:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:37:07,294:INFO:Calculating mean and std
2023-07-22 01:37:07,295:INFO:Creating metrics dataframe
2023-07-22 01:37:07,537:INFO:Uploading results into container
2023-07-22 01:37:07,539:INFO:Uploading model into container now
2023-07-22 01:37:07,540:INFO:_master_model_container: 27
2023-07-22 01:37:07,540:INFO:_display_container: 3
2023-07-22 01:37:07,541:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 01:37:07,541:INFO:create_model() successfully completed......................................
2023-07-22 01:37:07,642:INFO:SubProcess create_model() end ==================================
2023-07-22 01:37:07,643:INFO:Creating metrics dataframe
2023-07-22 01:37:07,654:INFO:Initializing Extreme Gradient Boosting
2023-07-22 01:37:07,654:INFO:Total runtime is 0.9016960064570109 minutes
2023-07-22 01:37:07,659:INFO:SubProcess create_model() called ==================================
2023-07-22 01:37:07,659:INFO:Initializing create_model()
2023-07-22 01:37:07,659:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:37:07,659:INFO:Checking exceptions
2023-07-22 01:37:07,659:INFO:Importing libraries
2023-07-22 01:37:07,659:INFO:Copying training dataset
2023-07-22 01:37:07,667:INFO:Defining folds
2023-07-22 01:37:07,668:INFO:Declaring metric variables
2023-07-22 01:37:07,671:INFO:Importing untrained model
2023-07-22 01:37:07,676:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 01:37:07,685:INFO:Starting cross validation
2023-07-22 01:37:07,937:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:37:12,123:INFO:Calculating mean and std
2023-07-22 01:37:12,124:INFO:Creating metrics dataframe
2023-07-22 01:37:12,334:INFO:Uploading results into container
2023-07-22 01:37:12,335:INFO:Uploading model into container now
2023-07-22 01:37:12,336:INFO:_master_model_container: 28
2023-07-22 01:37:12,336:INFO:_display_container: 3
2023-07-22 01:37:12,337:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 01:37:12,337:INFO:create_model() successfully completed......................................
2023-07-22 01:37:12,435:INFO:SubProcess create_model() end ==================================
2023-07-22 01:37:12,435:INFO:Creating metrics dataframe
2023-07-22 01:37:12,448:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 01:37:12,448:INFO:Total runtime is 0.9815852244695027 minutes
2023-07-22 01:37:12,451:INFO:SubProcess create_model() called ==================================
2023-07-22 01:37:12,451:INFO:Initializing create_model()
2023-07-22 01:37:12,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:37:12,451:INFO:Checking exceptions
2023-07-22 01:37:12,451:INFO:Importing libraries
2023-07-22 01:37:12,451:INFO:Copying training dataset
2023-07-22 01:37:12,461:INFO:Defining folds
2023-07-22 01:37:12,461:INFO:Declaring metric variables
2023-07-22 01:37:12,466:INFO:Importing untrained model
2023-07-22 01:37:12,472:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:37:12,480:INFO:Starting cross validation
2023-07-22 01:37:12,713:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:37:17,347:INFO:Calculating mean and std
2023-07-22 01:37:17,348:INFO:Creating metrics dataframe
2023-07-22 01:37:17,600:INFO:Uploading results into container
2023-07-22 01:37:17,601:INFO:Uploading model into container now
2023-07-22 01:37:17,602:INFO:_master_model_container: 29
2023-07-22 01:37:17,602:INFO:_display_container: 3
2023-07-22 01:37:17,603:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:37:17,604:INFO:create_model() successfully completed......................................
2023-07-22 01:37:17,711:INFO:SubProcess create_model() end ==================================
2023-07-22 01:37:17,711:INFO:Creating metrics dataframe
2023-07-22 01:37:17,724:INFO:Initializing Dummy Classifier
2023-07-22 01:37:17,724:INFO:Total runtime is 1.0695303519566852 minutes
2023-07-22 01:37:17,729:INFO:SubProcess create_model() called ==================================
2023-07-22 01:37:17,730:INFO:Initializing create_model()
2023-07-22 01:37:17,730:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265496B0F10>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:37:17,731:INFO:Checking exceptions
2023-07-22 01:37:17,731:INFO:Importing libraries
2023-07-22 01:37:17,731:INFO:Copying training dataset
2023-07-22 01:37:17,742:INFO:Defining folds
2023-07-22 01:37:17,742:INFO:Declaring metric variables
2023-07-22 01:37:17,749:INFO:Importing untrained model
2023-07-22 01:37:17,755:INFO:Dummy Classifier Imported successfully
2023-07-22 01:37:17,767:INFO:Starting cross validation
2023-07-22 01:37:17,996:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:37:18,859:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,484:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,602:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,609:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,625:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,629:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,630:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:20,685:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:37:22,100:INFO:Calculating mean and std
2023-07-22 01:37:22,102:INFO:Creating metrics dataframe
2023-07-22 01:37:22,377:INFO:Uploading results into container
2023-07-22 01:37:22,379:INFO:Uploading model into container now
2023-07-22 01:37:22,380:INFO:_master_model_container: 30
2023-07-22 01:37:22,380:INFO:_display_container: 3
2023-07-22 01:37:22,380:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 01:37:22,381:INFO:create_model() successfully completed......................................
2023-07-22 01:37:22,485:INFO:SubProcess create_model() end ==================================
2023-07-22 01:37:22,485:INFO:Creating metrics dataframe
2023-07-22 01:37:22,510:INFO:Initializing create_model()
2023-07-22 01:37:22,510:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:37:22,510:INFO:Checking exceptions
2023-07-22 01:37:22,512:INFO:Importing libraries
2023-07-22 01:37:22,513:INFO:Copying training dataset
2023-07-22 01:37:22,520:INFO:Defining folds
2023-07-22 01:37:22,520:INFO:Declaring metric variables
2023-07-22 01:37:22,520:INFO:Importing untrained model
2023-07-22 01:37:22,520:INFO:Declaring custom model
2023-07-22 01:37:22,521:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:37:22,727:INFO:Cross validation set to False
2023-07-22 01:37:22,729:INFO:Fitting Model
2023-07-22 01:37:23,387:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:37:23,387:INFO:create_model() successfully completed......................................
2023-07-22 01:37:23,503:INFO:Creating Dashboard logs
2023-07-22 01:37:23,509:INFO:Model: Light Gradient Boosting Machine
2023-07-22 01:37:23,602:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2023-07-22 01:37:23,790:INFO:Initializing predict_model()
2023-07-22 01:37:23,790:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000026568D76710>)
2023-07-22 01:37:23,790:INFO:Checking exceptions
2023-07-22 01:37:23,790:INFO:Preloading libraries
2023-07-22 01:37:24,271:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:37:25,014:INFO:Initializing create_model()
2023-07-22 01:37:25,014:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:37:25,014:INFO:Checking exceptions
2023-07-22 01:37:25,017:INFO:Importing libraries
2023-07-22 01:37:25,017:INFO:Copying training dataset
2023-07-22 01:37:25,025:INFO:Defining folds
2023-07-22 01:37:25,025:INFO:Declaring metric variables
2023-07-22 01:37:25,025:INFO:Importing untrained model
2023-07-22 01:37:25,025:INFO:Declaring custom model
2023-07-22 01:37:25,026:INFO:Ridge Classifier Imported successfully
2023-07-22 01:37:25,239:INFO:Cross validation set to False
2023-07-22 01:37:25,239:INFO:Fitting Model
2023-07-22 01:37:25,696:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.42454e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:37:25,868:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:37:25,868:INFO:create_model() successfully completed......................................
2023-07-22 01:37:25,972:INFO:Creating Dashboard logs
2023-07-22 01:37:25,977:INFO:Model: Ridge Classifier
2023-07-22 01:37:26,057:INFO:Logged params: {'alpha': 1.0, 'class_weight': None, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 42, 'solver': 'auto', 'tol': 0.0001}
2023-07-22 01:37:26,212:INFO:Initializing predict_model()
2023-07-22 01:37:26,212:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000026568D76EF0>)
2023-07-22 01:37:26,212:INFO:Checking exceptions
2023-07-22 01:37:26,212:INFO:Preloading libraries
2023-07-22 01:37:26,637:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:37:27,334:INFO:Initializing create_model()
2023-07-22 01:37:27,334:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:37:27,334:INFO:Checking exceptions
2023-07-22 01:37:27,337:INFO:Importing libraries
2023-07-22 01:37:27,337:INFO:Copying training dataset
2023-07-22 01:37:27,343:INFO:Defining folds
2023-07-22 01:37:27,343:INFO:Declaring metric variables
2023-07-22 01:37:27,344:INFO:Importing untrained model
2023-07-22 01:37:27,344:INFO:Declaring custom model
2023-07-22 01:37:27,344:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:37:27,552:INFO:Cross validation set to False
2023-07-22 01:37:27,552:INFO:Fitting Model
2023-07-22 01:37:28,210:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:37:28,210:INFO:create_model() successfully completed......................................
2023-07-22 01:37:28,311:INFO:Creating Dashboard logs
2023-07-22 01:37:28,317:INFO:Model: Linear Discriminant Analysis
2023-07-22 01:37:28,416:INFO:Logged params: {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}
2023-07-22 01:37:28,570:INFO:Initializing predict_model()
2023-07-22 01:37:28,570:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000026568D74C10>)
2023-07-22 01:37:28,570:INFO:Checking exceptions
2023-07-22 01:37:28,570:INFO:Preloading libraries
2023-07-22 01:37:29,017:WARNING:Couldn't create holdout prediction for model, exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\loggers\dashboard_logger.py", line 128, in log_model
    experiment.predict_model(model, verbose=False)  # type: ignore
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\classification\oop.py", line 2812, in predict_model
    return super().predict_model(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 5016, in predict_model
    X_test_.index = old_index
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 5915, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas\_libs\properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\generic.py", line 823, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\managers.py", line 230, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pandas\core\internals\base.py", line 70, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4494 elements, new values have 2500 elements

2023-07-22 01:37:29,866:INFO:Creating Dashboard logs
2023-07-22 01:37:29,873:INFO:Model: Extreme Gradient Boosting
2023-07-22 01:37:29,989:INFO:Logged params: {'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': -1, 'num_parallel_tree': None, 'predictor': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0}
2023-07-22 01:37:31,009:INFO:Creating Dashboard logs
2023-07-22 01:37:31,013:INFO:Model: Gradient Boosting Classifier
2023-07-22 01:37:31,075:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-07-22 01:37:31,969:INFO:Creating Dashboard logs
2023-07-22 01:37:31,973:INFO:Model: Random Forest Classifier
2023-07-22 01:37:32,041:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
2023-07-22 01:37:32,895:INFO:Creating Dashboard logs
2023-07-22 01:37:32,899:INFO:Model: Extra Trees Classifier
2023-07-22 01:37:32,965:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}
2023-07-22 01:37:33,873:INFO:Creating Dashboard logs
2023-07-22 01:37:33,877:INFO:Model: Ada Boost Classifier
2023-07-22 01:37:33,942:INFO:Logged params: {'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 42}
2023-07-22 01:37:34,819:INFO:Creating Dashboard logs
2023-07-22 01:37:34,823:INFO:Model: K Neighbors Classifier
2023-07-22 01:37:34,882:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2023-07-22 01:37:35,838:INFO:Creating Dashboard logs
2023-07-22 01:37:35,841:INFO:Model: Decision Tree Classifier
2023-07-22 01:37:35,900:INFO:Logged params: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 42, 'splitter': 'best'}
2023-07-22 01:37:36,861:INFO:Creating Dashboard logs
2023-07-22 01:37:36,866:INFO:Model: Naive Bayes
2023-07-22 01:37:36,924:INFO:Logged params: {'priors': None, 'var_smoothing': 1e-09}
2023-07-22 01:37:37,753:INFO:Creating Dashboard logs
2023-07-22 01:37:37,756:INFO:Model: Quadratic Discriminant Analysis
2023-07-22 01:37:37,817:INFO:Logged params: {'priors': None, 'reg_param': 0.0, 'store_covariance': False, 'tol': 0.0001}
2023-07-22 01:37:38,710:INFO:Creating Dashboard logs
2023-07-22 01:37:38,713:INFO:Model: SVM - Linear Kernel
2023-07-22 01:37:38,775:INFO:Logged params: {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.001, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2023-07-22 01:37:39,703:INFO:Creating Dashboard logs
2023-07-22 01:37:39,706:INFO:Model: Logistic Regression
2023-07-22 01:37:39,771:INFO:Logged params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
2023-07-22 01:37:40,630:INFO:Creating Dashboard logs
2023-07-22 01:37:40,633:INFO:Model: Dummy Classifier
2023-07-22 01:37:40,699:INFO:Logged params: {'constant': None, 'random_state': 42, 'strategy': 'prior'}
2023-07-22 01:37:41,546:INFO:_master_model_container: 30
2023-07-22 01:37:41,546:INFO:_display_container: 3
2023-07-22 01:37:41,547:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)]
2023-07-22 01:37:41,548:INFO:compare_models() successfully completed......................................
2023-07-22 01:39:38,971:INFO:Initializing plot_model()
2023-07-22 01:39:38,971:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)], feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000265491A78E0>, system=True)
2023-07-22 01:39:38,971:INFO:Checking exceptions
2023-07-22 01:40:07,498:INFO:PyCaret ClassificationExperiment
2023-07-22 01:40:07,498:INFO:Logging name: balanceado
2023-07-22 01:40:07,498:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:40:07,498:INFO:version 3.0.4
2023-07-22 01:40:07,498:INFO:Initializing setup()
2023-07-22 01:40:07,498:INFO:self.USI: c242
2023-07-22 01:40:07,498:INFO:self._variable_keys: {'data', 'fold_generator', 'target_param', 'exp_id', 'y_test', '_available_plots', 'n_jobs_param', '_ml_usecase', 'html_param', 'logging_param', 'is_multiclass', 'gpu_n_jobs_param', 'idx', 'X_train', 'seed', 'y', 'memory', 'fold_shuffle_param', 'log_plots_param', 'fold_groups_param', 'X', 'USI', 'fix_imbalance', 'pipeline', 'exp_name_log', 'X_test', 'gpu_param', 'y_train'}
2023-07-22 01:40:07,498:INFO:Checking environment
2023-07-22 01:40:07,498:INFO:python_version: 3.10.8
2023-07-22 01:40:07,498:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:40:07,498:INFO:machine: AMD64
2023-07-22 01:40:07,498:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:40:07,505:INFO:Memory: svmem(total=16505966592, available=2226962432, percent=86.5, used=14279004160, free=2226962432)
2023-07-22 01:40:07,506:INFO:Physical Core: 6
2023-07-22 01:40:07,506:INFO:Logical Core: 12
2023-07-22 01:40:07,506:INFO:Checking libraries
2023-07-22 01:40:07,506:INFO:System:
2023-07-22 01:40:07,506:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:40:07,506:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:40:07,506:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:40:07,506:INFO:PyCaret required dependencies:
2023-07-22 01:40:07,506:INFO:                 pip: 22.2.2
2023-07-22 01:40:07,506:INFO:          setuptools: 63.2.0
2023-07-22 01:40:07,506:INFO:             pycaret: 3.0.4
2023-07-22 01:40:07,506:INFO:             IPython: 8.11.0
2023-07-22 01:40:07,506:INFO:          ipywidgets: 8.0.7
2023-07-22 01:40:07,506:INFO:                tqdm: 4.64.1
2023-07-22 01:40:07,506:INFO:               numpy: 1.23.5
2023-07-22 01:40:07,506:INFO:              pandas: 1.5.3
2023-07-22 01:40:07,506:INFO:              jinja2: 3.1.2
2023-07-22 01:40:07,506:INFO:               scipy: 1.9.3
2023-07-22 01:40:07,506:INFO:              joblib: 1.2.0
2023-07-22 01:40:07,506:INFO:             sklearn: 1.2.2
2023-07-22 01:40:07,506:INFO:                pyod: 1.1.0
2023-07-22 01:40:07,506:INFO:            imblearn: 0.10.1
2023-07-22 01:40:07,506:INFO:   category_encoders: 2.6.1
2023-07-22 01:40:07,506:INFO:            lightgbm: 3.3.5
2023-07-22 01:40:07,506:INFO:               numba: 0.57.0
2023-07-22 01:40:07,506:INFO:            requests: 2.28.2
2023-07-22 01:40:07,507:INFO:          matplotlib: 3.7.1
2023-07-22 01:40:07,507:INFO:          scikitplot: 0.3.7
2023-07-22 01:40:07,507:INFO:         yellowbrick: 1.5
2023-07-22 01:40:07,507:INFO:              plotly: 5.15.0
2023-07-22 01:40:07,507:INFO:    plotly-resampler: Not installed
2023-07-22 01:40:07,507:INFO:             kaleido: 0.2.1
2023-07-22 01:40:07,507:INFO:           schemdraw: 0.15
2023-07-22 01:40:07,507:INFO:         statsmodels: 0.13.5
2023-07-22 01:40:07,507:INFO:              sktime: 0.21.0
2023-07-22 01:40:07,507:INFO:               tbats: 1.1.3
2023-07-22 01:40:07,507:INFO:            pmdarima: 2.0.3
2023-07-22 01:40:07,507:INFO:              psutil: 5.9.4
2023-07-22 01:40:07,507:INFO:          markupsafe: 2.1.2
2023-07-22 01:40:07,507:INFO:             pickle5: Not installed
2023-07-22 01:40:07,507:INFO:         cloudpickle: 2.2.1
2023-07-22 01:40:07,507:INFO:         deprecation: 2.1.0
2023-07-22 01:40:07,507:INFO:              xxhash: 3.2.0
2023-07-22 01:40:07,507:INFO:           wurlitzer: Not installed
2023-07-22 01:40:07,507:INFO:PyCaret optional dependencies:
2023-07-22 01:40:07,507:INFO:                shap: 0.41.0
2023-07-22 01:40:07,507:INFO:           interpret: 0.4.2
2023-07-22 01:40:07,507:INFO:                umap: 0.5.3
2023-07-22 01:40:07,507:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:40:07,507:INFO:  explainerdashboard: Not installed
2023-07-22 01:40:07,507:INFO:             autoviz: Not installed
2023-07-22 01:40:07,507:INFO:           fairlearn: Not installed
2023-07-22 01:40:07,507:INFO:          deepchecks: Not installed
2023-07-22 01:40:07,507:INFO:             xgboost: 1.7.6
2023-07-22 01:40:07,507:INFO:            catboost: Not installed
2023-07-22 01:40:07,507:INFO:              kmodes: Not installed
2023-07-22 01:40:07,508:INFO:             mlxtend: Not installed
2023-07-22 01:40:07,508:INFO:       statsforecast: Not installed
2023-07-22 01:40:07,508:INFO:        tune_sklearn: Not installed
2023-07-22 01:40:07,508:INFO:                 ray: Not installed
2023-07-22 01:40:07,508:INFO:            hyperopt: Not installed
2023-07-22 01:40:07,508:INFO:              optuna: 3.2.0
2023-07-22 01:40:07,508:INFO:               skopt: Not installed
2023-07-22 01:40:07,508:INFO:              mlflow: 2.4.2
2023-07-22 01:40:07,508:INFO:              gradio: Not installed
2023-07-22 01:40:07,508:INFO:             fastapi: 0.95.2
2023-07-22 01:40:07,508:INFO:             uvicorn: 0.22.0
2023-07-22 01:40:07,508:INFO:              m2cgen: Not installed
2023-07-22 01:40:07,508:INFO:           evidently: Not installed
2023-07-22 01:40:07,508:INFO:               fugue: Not installed
2023-07-22 01:40:07,508:INFO:           streamlit: Not installed
2023-07-22 01:40:07,508:INFO:             prophet: Not installed
2023-07-22 01:40:07,508:INFO:None
2023-07-22 01:40:07,508:INFO:Set up data.
2023-07-22 01:40:07,536:INFO:Set up train/test split.
2023-07-22 01:40:07,536:INFO:Set up data.
2023-07-22 01:40:07,557:INFO:Set up index.
2023-07-22 01:40:07,558:INFO:Set up folding strategy.
2023-07-22 01:40:07,558:INFO:Assigning column types.
2023-07-22 01:40:07,563:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:40:07,614:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:40:07,615:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:40:07,645:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:07,648:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:07,694:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:40:07,695:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:40:07,724:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:07,727:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:07,728:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:40:07,776:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:40:07,813:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:07,815:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:07,862:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:40:07,890:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:07,892:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:07,893:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:40:07,970:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:07,972:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:08,050:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:08,053:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:08,054:INFO:Preparing preprocessing pipeline...
2023-07-22 01:40:08,055:INFO:Set up iterative imputation.
2023-07-22 01:40:08,056:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:40:08,060:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:40:08,065:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:40:08,126:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:40:08,172:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:08,175:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:08,250:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:08,253:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:08,283:INFO:Set up encoding of categorical features.
2023-07-22 01:40:08,283:INFO:Set up imbalanced handling.
2023-07-22 01:40:08,283:INFO:Set up column transformation.
2023-07-22 01:40:08,283:INFO:Set up feature selection.
2023-07-22 01:40:08,364:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:08,366:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:08,893:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:40:08,920:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:40:08,920:INFO:Creating final display dataframe.
2023-07-22 01:40:09,737:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment         False
31                  Experiment Name    balanceado
32                              USI          c242
2023-07-22 01:40:09,846:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:09,849:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:09,929:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:40:09,932:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:40:09,933:INFO:setup() successfully completed in 2.59s...............
2023-07-22 01:40:17,042:INFO:Initializing compare_models()
2023-07-22 01:40:17,043:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:40:17,043:INFO:Checking exceptions
2023-07-22 01:40:17,050:INFO:Preparing display monitor
2023-07-22 01:40:17,077:INFO:Initializing Logistic Regression
2023-07-22 01:40:17,077:INFO:Total runtime is 0.0 minutes
2023-07-22 01:40:17,081:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:17,081:INFO:Initializing create_model()
2023-07-22 01:40:17,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:17,082:INFO:Checking exceptions
2023-07-22 01:40:17,082:INFO:Importing libraries
2023-07-22 01:40:17,082:INFO:Copying training dataset
2023-07-22 01:40:17,090:INFO:Defining folds
2023-07-22 01:40:17,090:INFO:Declaring metric variables
2023-07-22 01:40:17,095:INFO:Importing untrained model
2023-07-22 01:40:17,100:INFO:Logistic Regression Imported successfully
2023-07-22 01:40:17,109:INFO:Starting cross validation
2023-07-22 01:40:17,331:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:18,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:18,590:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:18,613:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:18,626:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:18,628:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:18,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:18,653:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:18,663:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:18,677:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:40:19,989:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:19,999:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:20,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:20,072:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:20,092:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:20,097:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:20,114:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:20,119:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:20,180:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:21,405:INFO:Calculating mean and std
2023-07-22 01:40:21,407:INFO:Creating metrics dataframe
2023-07-22 01:40:21,607:INFO:Uploading results into container
2023-07-22 01:40:21,608:INFO:Uploading model into container now
2023-07-22 01:40:21,609:INFO:_master_model_container: 1
2023-07-22 01:40:21,609:INFO:_display_container: 2
2023-07-22 01:40:21,610:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 01:40:21,610:INFO:create_model() successfully completed......................................
2023-07-22 01:40:21,714:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:21,714:INFO:Creating metrics dataframe
2023-07-22 01:40:21,722:INFO:Initializing K Neighbors Classifier
2023-07-22 01:40:21,722:INFO:Total runtime is 0.07742488781611125 minutes
2023-07-22 01:40:21,726:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:21,726:INFO:Initializing create_model()
2023-07-22 01:40:21,726:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:21,726:INFO:Checking exceptions
2023-07-22 01:40:21,726:INFO:Importing libraries
2023-07-22 01:40:21,726:INFO:Copying training dataset
2023-07-22 01:40:21,734:INFO:Defining folds
2023-07-22 01:40:21,734:INFO:Declaring metric variables
2023-07-22 01:40:21,739:INFO:Importing untrained model
2023-07-22 01:40:21,743:INFO:K Neighbors Classifier Imported successfully
2023-07-22 01:40:21,751:INFO:Starting cross validation
2023-07-22 01:40:21,954:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:25,981:INFO:Calculating mean and std
2023-07-22 01:40:25,982:INFO:Creating metrics dataframe
2023-07-22 01:40:26,184:INFO:Uploading results into container
2023-07-22 01:40:26,185:INFO:Uploading model into container now
2023-07-22 01:40:26,185:INFO:_master_model_container: 2
2023-07-22 01:40:26,185:INFO:_display_container: 2
2023-07-22 01:40:26,185:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 01:40:26,185:INFO:create_model() successfully completed......................................
2023-07-22 01:40:26,287:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:26,287:INFO:Creating metrics dataframe
2023-07-22 01:40:26,298:INFO:Initializing Naive Bayes
2023-07-22 01:40:26,298:INFO:Total runtime is 0.15369029045104982 minutes
2023-07-22 01:40:26,302:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:26,302:INFO:Initializing create_model()
2023-07-22 01:40:26,302:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:26,302:INFO:Checking exceptions
2023-07-22 01:40:26,302:INFO:Importing libraries
2023-07-22 01:40:26,303:INFO:Copying training dataset
2023-07-22 01:40:26,311:INFO:Defining folds
2023-07-22 01:40:26,311:INFO:Declaring metric variables
2023-07-22 01:40:26,315:INFO:Importing untrained model
2023-07-22 01:40:26,320:INFO:Naive Bayes Imported successfully
2023-07-22 01:40:26,327:INFO:Starting cross validation
2023-07-22 01:40:26,530:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:30,421:INFO:Calculating mean and std
2023-07-22 01:40:30,423:INFO:Creating metrics dataframe
2023-07-22 01:40:30,627:INFO:Uploading results into container
2023-07-22 01:40:30,628:INFO:Uploading model into container now
2023-07-22 01:40:30,628:INFO:_master_model_container: 3
2023-07-22 01:40:30,629:INFO:_display_container: 2
2023-07-22 01:40:30,629:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 01:40:30,629:INFO:create_model() successfully completed......................................
2023-07-22 01:40:30,730:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:30,730:INFO:Creating metrics dataframe
2023-07-22 01:40:30,741:INFO:Initializing Decision Tree Classifier
2023-07-22 01:40:30,741:INFO:Total runtime is 0.22772953907648724 minutes
2023-07-22 01:40:30,745:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:30,746:INFO:Initializing create_model()
2023-07-22 01:40:30,746:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:30,746:INFO:Checking exceptions
2023-07-22 01:40:30,746:INFO:Importing libraries
2023-07-22 01:40:30,746:INFO:Copying training dataset
2023-07-22 01:40:30,753:INFO:Defining folds
2023-07-22 01:40:30,753:INFO:Declaring metric variables
2023-07-22 01:40:30,758:INFO:Importing untrained model
2023-07-22 01:40:30,763:INFO:Decision Tree Classifier Imported successfully
2023-07-22 01:40:30,771:INFO:Starting cross validation
2023-07-22 01:40:30,992:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:31,905:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:40:32,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 426, in predict
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:40:35,139:INFO:Calculating mean and std
2023-07-22 01:40:35,141:INFO:Creating metrics dataframe
2023-07-22 01:40:35,354:INFO:Uploading results into container
2023-07-22 01:40:35,355:INFO:Uploading model into container now
2023-07-22 01:40:35,355:INFO:_master_model_container: 4
2023-07-22 01:40:35,356:INFO:_display_container: 2
2023-07-22 01:40:35,357:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 01:40:35,357:INFO:create_model() successfully completed......................................
2023-07-22 01:40:35,461:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:35,461:INFO:Creating metrics dataframe
2023-07-22 01:40:35,469:INFO:Initializing SVM - Linear Kernel
2023-07-22 01:40:35,470:INFO:Total runtime is 0.3065317908922831 minutes
2023-07-22 01:40:35,472:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:35,473:INFO:Initializing create_model()
2023-07-22 01:40:35,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:35,473:INFO:Checking exceptions
2023-07-22 01:40:35,474:INFO:Importing libraries
2023-07-22 01:40:35,474:INFO:Copying training dataset
2023-07-22 01:40:35,482:INFO:Defining folds
2023-07-22 01:40:35,482:INFO:Declaring metric variables
2023-07-22 01:40:35,486:INFO:Importing untrained model
2023-07-22 01:40:35,491:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 01:40:35,499:INFO:Starting cross validation
2023-07-22 01:40:35,706:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:36,440:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,647:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,647:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,656:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,703:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,706:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,720:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:37,760:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:40:39,050:INFO:Calculating mean and std
2023-07-22 01:40:39,051:INFO:Creating metrics dataframe
2023-07-22 01:40:39,299:INFO:Uploading results into container
2023-07-22 01:40:39,300:INFO:Uploading model into container now
2023-07-22 01:40:39,301:INFO:_master_model_container: 5
2023-07-22 01:40:39,301:INFO:_display_container: 2
2023-07-22 01:40:39,301:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 01:40:39,302:INFO:create_model() successfully completed......................................
2023-07-22 01:40:39,420:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:39,420:INFO:Creating metrics dataframe
2023-07-22 01:40:39,429:INFO:Initializing Ridge Classifier
2023-07-22 01:40:39,429:INFO:Total runtime is 0.37253652016321814 minutes
2023-07-22 01:40:39,433:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:39,433:INFO:Initializing create_model()
2023-07-22 01:40:39,434:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:39,434:INFO:Checking exceptions
2023-07-22 01:40:39,434:INFO:Importing libraries
2023-07-22 01:40:39,434:INFO:Copying training dataset
2023-07-22 01:40:39,441:INFO:Defining folds
2023-07-22 01:40:39,441:INFO:Declaring metric variables
2023-07-22 01:40:39,446:INFO:Importing untrained model
2023-07-22 01:40:39,451:INFO:Ridge Classifier Imported successfully
2023-07-22 01:40:39,459:INFO:Starting cross validation
2023-07-22 01:40:39,672:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:40,347:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:40,896:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.22379e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:40,898:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.66771e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:40,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.79155e-52): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:40,918:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.44723e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:40,920:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.39835e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:40,944:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.92242e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:41,001:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.07304e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:41,003:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.85819e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:41,028:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.49256e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:40:41,616:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:41,638:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:41,640:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:41,658:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:41,658:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:41,681:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:41,721:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:41,755:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:41,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:40:43,062:INFO:Calculating mean and std
2023-07-22 01:40:43,063:INFO:Creating metrics dataframe
2023-07-22 01:40:43,266:INFO:Uploading results into container
2023-07-22 01:40:43,267:INFO:Uploading model into container now
2023-07-22 01:40:43,267:INFO:_master_model_container: 6
2023-07-22 01:40:43,267:INFO:_display_container: 2
2023-07-22 01:40:43,268:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:40:43,268:INFO:create_model() successfully completed......................................
2023-07-22 01:40:43,364:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:43,365:INFO:Creating metrics dataframe
2023-07-22 01:40:43,377:INFO:Initializing Random Forest Classifier
2023-07-22 01:40:43,377:INFO:Total runtime is 0.4383285284042358 minutes
2023-07-22 01:40:43,381:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:43,381:INFO:Initializing create_model()
2023-07-22 01:40:43,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:43,382:INFO:Checking exceptions
2023-07-22 01:40:43,382:INFO:Importing libraries
2023-07-22 01:40:43,382:INFO:Copying training dataset
2023-07-22 01:40:43,389:INFO:Defining folds
2023-07-22 01:40:43,389:INFO:Declaring metric variables
2023-07-22 01:40:43,393:INFO:Importing untrained model
2023-07-22 01:40:43,398:INFO:Random Forest Classifier Imported successfully
2023-07-22 01:40:43,405:INFO:Starting cross validation
2023-07-22 01:40:43,615:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:44,789:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:40:46,160:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:40:48,150:INFO:Calculating mean and std
2023-07-22 01:40:48,151:INFO:Creating metrics dataframe
2023-07-22 01:40:48,363:INFO:Uploading results into container
2023-07-22 01:40:48,364:INFO:Uploading model into container now
2023-07-22 01:40:48,365:INFO:_master_model_container: 7
2023-07-22 01:40:48,365:INFO:_display_container: 2
2023-07-22 01:40:48,365:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 01:40:48,365:INFO:create_model() successfully completed......................................
2023-07-22 01:40:48,476:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:48,476:INFO:Creating metrics dataframe
2023-07-22 01:40:48,487:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 01:40:48,488:INFO:Total runtime is 0.5235159595807393 minutes
2023-07-22 01:40:48,492:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:48,492:INFO:Initializing create_model()
2023-07-22 01:40:48,493:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:48,493:INFO:Checking exceptions
2023-07-22 01:40:48,493:INFO:Importing libraries
2023-07-22 01:40:48,493:INFO:Copying training dataset
2023-07-22 01:40:48,501:INFO:Defining folds
2023-07-22 01:40:48,501:INFO:Declaring metric variables
2023-07-22 01:40:48,505:INFO:Importing untrained model
2023-07-22 01:40:48,511:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 01:40:48,520:INFO:Starting cross validation
2023-07-22 01:40:48,742:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:49,845:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:49,910:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:49,967:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:50,083:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:50,120:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:50,196:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:50,198:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:50,233:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:50,239:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:40:50,672:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,672:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,673:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:50,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:50,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:50,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,826:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,826:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:50,870:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,870:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,870:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:50,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:50,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:50,983:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,984:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:50,984:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,009:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,009:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,009:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,357:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,357:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,358:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,359:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:51,363:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:51,382:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,382:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,383:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,385:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:51,391:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:51,423:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,423:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:51,434:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:51,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,521:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,523:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:51,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:51,565:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,565:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,565:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:51,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,640:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:51,646:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:51,682:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,682:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,683:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:51,687:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,689:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:51,691:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:51,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:40:51,717:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,717:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:40:51,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:40:51,719:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:40:52,797:INFO:Calculating mean and std
2023-07-22 01:40:52,798:INFO:Creating metrics dataframe
2023-07-22 01:40:53,009:INFO:Uploading results into container
2023-07-22 01:40:53,010:INFO:Uploading model into container now
2023-07-22 01:40:53,010:INFO:_master_model_container: 8
2023-07-22 01:40:53,011:INFO:_display_container: 2
2023-07-22 01:40:53,011:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 01:40:53,011:INFO:create_model() successfully completed......................................
2023-07-22 01:40:53,122:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:53,122:INFO:Creating metrics dataframe
2023-07-22 01:40:53,132:INFO:Initializing Ada Boost Classifier
2023-07-22 01:40:53,132:INFO:Total runtime is 0.60091548760732 minutes
2023-07-22 01:40:53,135:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:53,136:INFO:Initializing create_model()
2023-07-22 01:40:53,136:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:53,136:INFO:Checking exceptions
2023-07-22 01:40:53,136:INFO:Importing libraries
2023-07-22 01:40:53,136:INFO:Copying training dataset
2023-07-22 01:40:53,144:INFO:Defining folds
2023-07-22 01:40:53,145:INFO:Declaring metric variables
2023-07-22 01:40:53,150:INFO:Importing untrained model
2023-07-22 01:40:53,155:INFO:Ada Boost Classifier Imported successfully
2023-07-22 01:40:53,162:INFO:Starting cross validation
2023-07-22 01:40:53,365:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:54,301:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 879, in predict_proba
    decision = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:40:55,419:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 700, in predict
    pred = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:40:57,516:INFO:Calculating mean and std
2023-07-22 01:40:57,517:INFO:Creating metrics dataframe
2023-07-22 01:40:57,760:INFO:Uploading results into container
2023-07-22 01:40:57,761:INFO:Uploading model into container now
2023-07-22 01:40:57,761:INFO:_master_model_container: 9
2023-07-22 01:40:57,762:INFO:_display_container: 2
2023-07-22 01:40:57,762:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 01:40:57,762:INFO:create_model() successfully completed......................................
2023-07-22 01:40:57,872:INFO:SubProcess create_model() end ==================================
2023-07-22 01:40:57,872:INFO:Creating metrics dataframe
2023-07-22 01:40:57,883:INFO:Initializing Gradient Boosting Classifier
2023-07-22 01:40:57,883:INFO:Total runtime is 0.6801095962524413 minutes
2023-07-22 01:40:57,887:INFO:SubProcess create_model() called ==================================
2023-07-22 01:40:57,887:INFO:Initializing create_model()
2023-07-22 01:40:57,887:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:40:57,888:INFO:Checking exceptions
2023-07-22 01:40:57,888:INFO:Importing libraries
2023-07-22 01:40:57,888:INFO:Copying training dataset
2023-07-22 01:40:57,900:INFO:Defining folds
2023-07-22 01:40:57,900:INFO:Declaring metric variables
2023-07-22 01:40:57,907:INFO:Importing untrained model
2023-07-22 01:40:57,913:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 01:40:57,928:INFO:Starting cross validation
2023-07-22 01:40:58,147:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:40:59,065:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1355, in predict_proba
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:41:00,242:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1308, in predict
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:41:02,158:INFO:Calculating mean and std
2023-07-22 01:41:02,160:INFO:Creating metrics dataframe
2023-07-22 01:41:02,367:INFO:Uploading results into container
2023-07-22 01:41:02,368:INFO:Uploading model into container now
2023-07-22 01:41:02,368:INFO:_master_model_container: 10
2023-07-22 01:41:02,368:INFO:_display_container: 2
2023-07-22 01:41:02,369:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 01:41:02,369:INFO:create_model() successfully completed......................................
2023-07-22 01:41:02,466:INFO:SubProcess create_model() end ==================================
2023-07-22 01:41:02,466:INFO:Creating metrics dataframe
2023-07-22 01:41:02,478:INFO:Initializing Linear Discriminant Analysis
2023-07-22 01:41:02,479:INFO:Total runtime is 0.7566998322804768 minutes
2023-07-22 01:41:02,483:INFO:SubProcess create_model() called ==================================
2023-07-22 01:41:02,483:INFO:Initializing create_model()
2023-07-22 01:41:02,483:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:02,483:INFO:Checking exceptions
2023-07-22 01:41:02,483:INFO:Importing libraries
2023-07-22 01:41:02,484:INFO:Copying training dataset
2023-07-22 01:41:02,492:INFO:Defining folds
2023-07-22 01:41:02,493:INFO:Declaring metric variables
2023-07-22 01:41:02,497:INFO:Importing untrained model
2023-07-22 01:41:02,501:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:41:02,508:INFO:Starting cross validation
2023-07-22 01:41:02,724:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:41:06,652:INFO:Calculating mean and std
2023-07-22 01:41:06,653:INFO:Creating metrics dataframe
2023-07-22 01:41:06,862:INFO:Uploading results into container
2023-07-22 01:41:06,863:INFO:Uploading model into container now
2023-07-22 01:41:06,863:INFO:_master_model_container: 11
2023-07-22 01:41:06,863:INFO:_display_container: 2
2023-07-22 01:41:06,863:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:41:06,863:INFO:create_model() successfully completed......................................
2023-07-22 01:41:06,960:INFO:SubProcess create_model() end ==================================
2023-07-22 01:41:06,960:INFO:Creating metrics dataframe
2023-07-22 01:41:06,973:INFO:Initializing Extra Trees Classifier
2023-07-22 01:41:06,973:INFO:Total runtime is 0.8315946181615193 minutes
2023-07-22 01:41:06,977:INFO:SubProcess create_model() called ==================================
2023-07-22 01:41:06,978:INFO:Initializing create_model()
2023-07-22 01:41:06,978:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:06,978:INFO:Checking exceptions
2023-07-22 01:41:06,978:INFO:Importing libraries
2023-07-22 01:41:06,978:INFO:Copying training dataset
2023-07-22 01:41:06,986:INFO:Defining folds
2023-07-22 01:41:06,986:INFO:Declaring metric variables
2023-07-22 01:41:06,990:INFO:Importing untrained model
2023-07-22 01:41:06,995:INFO:Extra Trees Classifier Imported successfully
2023-07-22 01:41:07,001:INFO:Starting cross validation
2023-07-22 01:41:07,205:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:41:08,420:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:41:09,865:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:41:12,260:INFO:Calculating mean and std
2023-07-22 01:41:12,264:INFO:Creating metrics dataframe
2023-07-22 01:41:12,513:INFO:Uploading results into container
2023-07-22 01:41:12,514:INFO:Uploading model into container now
2023-07-22 01:41:12,515:INFO:_master_model_container: 12
2023-07-22 01:41:12,515:INFO:_display_container: 2
2023-07-22 01:41:12,515:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 01:41:12,515:INFO:create_model() successfully completed......................................
2023-07-22 01:41:12,622:INFO:SubProcess create_model() end ==================================
2023-07-22 01:41:12,622:INFO:Creating metrics dataframe
2023-07-22 01:41:12,634:INFO:Initializing Extreme Gradient Boosting
2023-07-22 01:41:12,634:INFO:Total runtime is 0.9259494066238402 minutes
2023-07-22 01:41:12,638:INFO:SubProcess create_model() called ==================================
2023-07-22 01:41:12,638:INFO:Initializing create_model()
2023-07-22 01:41:12,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:12,639:INFO:Checking exceptions
2023-07-22 01:41:12,639:INFO:Importing libraries
2023-07-22 01:41:12,639:INFO:Copying training dataset
2023-07-22 01:41:12,648:INFO:Defining folds
2023-07-22 01:41:12,648:INFO:Declaring metric variables
2023-07-22 01:41:12,653:INFO:Importing untrained model
2023-07-22 01:41:12,657:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 01:41:12,665:INFO:Starting cross validation
2023-07-22 01:41:12,881:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:41:17,288:INFO:Calculating mean and std
2023-07-22 01:41:17,289:INFO:Creating metrics dataframe
2023-07-22 01:41:17,546:INFO:Uploading results into container
2023-07-22 01:41:17,546:INFO:Uploading model into container now
2023-07-22 01:41:17,547:INFO:_master_model_container: 13
2023-07-22 01:41:17,547:INFO:_display_container: 2
2023-07-22 01:41:17,548:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 01:41:17,549:INFO:create_model() successfully completed......................................
2023-07-22 01:41:17,653:INFO:SubProcess create_model() end ==================================
2023-07-22 01:41:17,653:INFO:Creating metrics dataframe
2023-07-22 01:41:17,665:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 01:41:17,665:INFO:Total runtime is 1.0098012844721476 minutes
2023-07-22 01:41:17,670:INFO:SubProcess create_model() called ==================================
2023-07-22 01:41:17,670:INFO:Initializing create_model()
2023-07-22 01:41:17,670:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:17,670:INFO:Checking exceptions
2023-07-22 01:41:17,670:INFO:Importing libraries
2023-07-22 01:41:17,670:INFO:Copying training dataset
2023-07-22 01:41:17,678:INFO:Defining folds
2023-07-22 01:41:17,678:INFO:Declaring metric variables
2023-07-22 01:41:17,683:INFO:Importing untrained model
2023-07-22 01:41:17,688:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:41:17,697:INFO:Starting cross validation
2023-07-22 01:41:17,935:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:41:22,471:INFO:Calculating mean and std
2023-07-22 01:41:22,472:INFO:Creating metrics dataframe
2023-07-22 01:41:22,712:INFO:Uploading results into container
2023-07-22 01:41:22,713:INFO:Uploading model into container now
2023-07-22 01:41:22,714:INFO:_master_model_container: 14
2023-07-22 01:41:22,714:INFO:_display_container: 2
2023-07-22 01:41:22,716:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:41:22,716:INFO:create_model() successfully completed......................................
2023-07-22 01:41:22,821:INFO:SubProcess create_model() end ==================================
2023-07-22 01:41:22,821:INFO:Creating metrics dataframe
2023-07-22 01:41:22,837:INFO:Initializing Dummy Classifier
2023-07-22 01:41:22,837:INFO:Total runtime is 1.0960082252820331 minutes
2023-07-22 01:41:22,841:INFO:SubProcess create_model() called ==================================
2023-07-22 01:41:22,841:INFO:Initializing create_model()
2023-07-22 01:41:22,842:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002654C0CDDB0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:22,842:INFO:Checking exceptions
2023-07-22 01:41:22,842:INFO:Importing libraries
2023-07-22 01:41:22,842:INFO:Copying training dataset
2023-07-22 01:41:22,855:INFO:Defining folds
2023-07-22 01:41:22,855:INFO:Declaring metric variables
2023-07-22 01:41:22,862:INFO:Importing untrained model
2023-07-22 01:41:22,868:INFO:Dummy Classifier Imported successfully
2023-07-22 01:41:22,877:INFO:Starting cross validation
2023-07-22 01:41:23,099:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:41:24,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:26,016:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:26,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:26,090:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:26,146:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:26,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:26,337:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:26,344:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:26,446:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:41:27,636:INFO:Calculating mean and std
2023-07-22 01:41:27,637:INFO:Creating metrics dataframe
2023-07-22 01:41:27,870:INFO:Uploading results into container
2023-07-22 01:41:27,870:INFO:Uploading model into container now
2023-07-22 01:41:27,871:INFO:_master_model_container: 15
2023-07-22 01:41:27,872:INFO:_display_container: 2
2023-07-22 01:41:27,872:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 01:41:27,873:INFO:create_model() successfully completed......................................
2023-07-22 01:41:27,987:INFO:SubProcess create_model() end ==================================
2023-07-22 01:41:27,987:INFO:Creating metrics dataframe
2023-07-22 01:41:28,012:INFO:Initializing create_model()
2023-07-22 01:41:28,012:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:28,012:INFO:Checking exceptions
2023-07-22 01:41:28,015:INFO:Importing libraries
2023-07-22 01:41:28,015:INFO:Copying training dataset
2023-07-22 01:41:28,021:INFO:Defining folds
2023-07-22 01:41:28,022:INFO:Declaring metric variables
2023-07-22 01:41:28,022:INFO:Importing untrained model
2023-07-22 01:41:28,022:INFO:Declaring custom model
2023-07-22 01:41:28,022:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:41:28,222:INFO:Cross validation set to False
2023-07-22 01:41:28,222:INFO:Fitting Model
2023-07-22 01:41:28,897:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:41:28,897:INFO:create_model() successfully completed......................................
2023-07-22 01:41:29,004:INFO:Initializing create_model()
2023-07-22 01:41:29,005:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:29,005:INFO:Checking exceptions
2023-07-22 01:41:29,009:INFO:Importing libraries
2023-07-22 01:41:29,009:INFO:Copying training dataset
2023-07-22 01:41:29,016:INFO:Defining folds
2023-07-22 01:41:29,016:INFO:Declaring metric variables
2023-07-22 01:41:29,016:INFO:Importing untrained model
2023-07-22 01:41:29,016:INFO:Declaring custom model
2023-07-22 01:41:29,017:INFO:Ridge Classifier Imported successfully
2023-07-22 01:41:29,226:INFO:Cross validation set to False
2023-07-22 01:41:29,226:INFO:Fitting Model
2023-07-22 01:41:29,709:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.42454e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:41:29,903:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:41:29,903:INFO:create_model() successfully completed......................................
2023-07-22 01:41:30,016:INFO:Initializing create_model()
2023-07-22 01:41:30,016:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:30,016:INFO:Checking exceptions
2023-07-22 01:41:30,019:INFO:Importing libraries
2023-07-22 01:41:30,019:INFO:Copying training dataset
2023-07-22 01:41:30,025:INFO:Defining folds
2023-07-22 01:41:30,025:INFO:Declaring metric variables
2023-07-22 01:41:30,026:INFO:Importing untrained model
2023-07-22 01:41:30,026:INFO:Declaring custom model
2023-07-22 01:41:30,026:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:41:30,239:INFO:Cross validation set to False
2023-07-22 01:41:30,239:INFO:Fitting Model
2023-07-22 01:41:30,904:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:41:30,904:INFO:create_model() successfully completed......................................
2023-07-22 01:41:31,034:INFO:_master_model_container: 15
2023-07-22 01:41:31,034:INFO:_display_container: 2
2023-07-22 01:41:31,034:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)]
2023-07-22 01:41:31,036:INFO:compare_models() successfully completed......................................
2023-07-22 01:41:36,951:INFO:Initializing create_model()
2023-07-22 01:41:36,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026568D23C10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:41:36,952:INFO:Checking exceptions
2023-07-22 01:41:36,969:INFO:Importing libraries
2023-07-22 01:41:36,969:INFO:Copying training dataset
2023-07-22 01:41:36,977:INFO:Defining folds
2023-07-22 01:41:36,977:INFO:Declaring metric variables
2023-07-22 01:41:36,982:INFO:Importing untrained model
2023-07-22 01:41:36,987:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:41:37,000:INFO:Starting cross validation
2023-07-22 01:41:37,294:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:41:41,674:INFO:Calculating mean and std
2023-07-22 01:41:41,675:INFO:Creating metrics dataframe
2023-07-22 01:41:41,681:INFO:Finalizing model
2023-07-22 01:41:42,435:INFO:Uploading results into container
2023-07-22 01:41:42,437:INFO:Uploading model into container now
2023-07-22 01:41:42,447:INFO:_master_model_container: 16
2023-07-22 01:41:42,447:INFO:_display_container: 3
2023-07-22 01:41:42,448:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:41:42,448:INFO:create_model() successfully completed......................................
2023-07-22 01:42:08,990:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:42:08,991:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:42:08,991:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:42:08,991:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:42:41,993:INFO:PyCaret ClassificationExperiment
2023-07-22 01:42:41,993:INFO:Logging name: balanceado
2023-07-22 01:42:41,993:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:42:41,993:INFO:version 3.0.4
2023-07-22 01:42:41,993:INFO:Initializing setup()
2023-07-22 01:42:41,993:INFO:self.USI: 67a9
2023-07-22 01:42:41,993:INFO:self._variable_keys: {'target_param', 'y_train', 'logging_param', 'fold_shuffle_param', 'is_multiclass', 'html_param', '_ml_usecase', 'data', 'pipeline', '_available_plots', 'memory', 'idx', 'exp_name_log', 'gpu_param', 'fold_groups_param', 'y_test', 'n_jobs_param', 'seed', 'exp_id', 'fold_generator', 'X', 'X_train', 'fix_imbalance', 'USI', 'gpu_n_jobs_param', 'log_plots_param', 'X_test', 'y'}
2023-07-22 01:42:41,993:INFO:Checking environment
2023-07-22 01:42:41,993:INFO:python_version: 3.10.8
2023-07-22 01:42:41,994:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:42:41,994:INFO:machine: AMD64
2023-07-22 01:42:41,994:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:42:41,999:INFO:Memory: svmem(total=16505966592, available=3859333120, percent=76.6, used=12646633472, free=3859333120)
2023-07-22 01:42:41,999:INFO:Physical Core: 6
2023-07-22 01:42:41,999:INFO:Logical Core: 12
2023-07-22 01:42:41,999:INFO:Checking libraries
2023-07-22 01:42:41,999:INFO:System:
2023-07-22 01:42:41,999:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:42:42,000:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:42:42,000:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:42:42,000:INFO:PyCaret required dependencies:
2023-07-22 01:42:42,001:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  ver = LooseVersion(ver)

2023-07-22 01:42:42,003:INFO:                 pip: 22.2.2
2023-07-22 01:42:42,003:INFO:          setuptools: 63.2.0
2023-07-22 01:42:42,003:INFO:             pycaret: 3.0.4
2023-07-22 01:42:42,003:INFO:             IPython: 8.11.0
2023-07-22 01:42:42,003:INFO:          ipywidgets: 8.0.7
2023-07-22 01:42:42,004:INFO:                tqdm: 4.64.1
2023-07-22 01:42:42,004:INFO:               numpy: 1.23.5
2023-07-22 01:42:42,004:INFO:              pandas: 1.5.3
2023-07-22 01:42:42,004:INFO:              jinja2: 3.1.2
2023-07-22 01:42:42,004:INFO:               scipy: 1.9.3
2023-07-22 01:42:42,004:INFO:              joblib: 1.2.0
2023-07-22 01:42:42,004:INFO:             sklearn: 1.2.2
2023-07-22 01:42:42,004:INFO:                pyod: 1.1.0
2023-07-22 01:42:42,004:INFO:            imblearn: 0.10.1
2023-07-22 01:42:42,004:INFO:   category_encoders: 2.6.1
2023-07-22 01:42:42,004:INFO:            lightgbm: 3.3.5
2023-07-22 01:42:42,004:INFO:               numba: 0.57.0
2023-07-22 01:42:42,004:INFO:            requests: 2.28.2
2023-07-22 01:42:42,004:INFO:          matplotlib: 3.7.1
2023-07-22 01:42:42,004:INFO:          scikitplot: 0.3.7
2023-07-22 01:42:42,004:INFO:         yellowbrick: 1.5
2023-07-22 01:42:42,004:INFO:              plotly: 5.15.0
2023-07-22 01:42:42,004:INFO:    plotly-resampler: Not installed
2023-07-22 01:42:42,004:INFO:             kaleido: 0.2.1
2023-07-22 01:42:42,004:INFO:           schemdraw: 0.15
2023-07-22 01:42:42,004:INFO:         statsmodels: 0.13.5
2023-07-22 01:42:42,004:INFO:              sktime: 0.21.0
2023-07-22 01:42:42,004:INFO:               tbats: 1.1.3
2023-07-22 01:42:42,004:INFO:            pmdarima: 2.0.3
2023-07-22 01:42:42,004:INFO:              psutil: 5.9.4
2023-07-22 01:42:42,004:INFO:          markupsafe: 2.1.2
2023-07-22 01:42:42,005:INFO:             pickle5: Not installed
2023-07-22 01:42:42,005:INFO:         cloudpickle: 2.2.1
2023-07-22 01:42:42,005:INFO:         deprecation: 2.1.0
2023-07-22 01:42:42,005:INFO:              xxhash: 3.2.0
2023-07-22 01:42:42,005:INFO:           wurlitzer: Not installed
2023-07-22 01:42:42,005:INFO:PyCaret optional dependencies:
2023-07-22 01:42:42,299:INFO:                shap: 0.41.0
2023-07-22 01:42:42,299:INFO:           interpret: 0.4.2
2023-07-22 01:42:42,299:INFO:                umap: 0.5.3
2023-07-22 01:42:42,299:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:42:42,299:INFO:  explainerdashboard: Not installed
2023-07-22 01:42:42,299:INFO:             autoviz: Not installed
2023-07-22 01:42:42,299:INFO:           fairlearn: Not installed
2023-07-22 01:42:42,299:INFO:          deepchecks: Not installed
2023-07-22 01:42:42,299:INFO:             xgboost: 1.7.6
2023-07-22 01:42:42,299:INFO:            catboost: Not installed
2023-07-22 01:42:42,299:INFO:              kmodes: Not installed
2023-07-22 01:42:42,299:INFO:             mlxtend: Not installed
2023-07-22 01:42:42,299:INFO:       statsforecast: Not installed
2023-07-22 01:42:42,299:INFO:        tune_sklearn: Not installed
2023-07-22 01:42:42,299:INFO:                 ray: Not installed
2023-07-22 01:42:42,299:INFO:            hyperopt: Not installed
2023-07-22 01:42:42,299:INFO:              optuna: 3.2.0
2023-07-22 01:42:42,299:INFO:               skopt: Not installed
2023-07-22 01:42:42,299:INFO:              mlflow: 2.4.2
2023-07-22 01:42:42,299:INFO:              gradio: Not installed
2023-07-22 01:42:42,300:INFO:             fastapi: 0.95.2
2023-07-22 01:42:42,300:INFO:             uvicorn: 0.22.0
2023-07-22 01:42:42,300:INFO:              m2cgen: Not installed
2023-07-22 01:42:42,300:INFO:           evidently: Not installed
2023-07-22 01:42:42,300:INFO:               fugue: Not installed
2023-07-22 01:42:42,300:INFO:           streamlit: Not installed
2023-07-22 01:42:42,300:INFO:             prophet: Not installed
2023-07-22 01:42:42,300:INFO:None
2023-07-22 01:42:42,300:INFO:Set up data.
2023-07-22 01:42:42,322:INFO:Set up train/test split.
2023-07-22 01:42:42,322:INFO:Set up data.
2023-07-22 01:42:42,339:INFO:Set up index.
2023-07-22 01:42:42,340:INFO:Set up folding strategy.
2023-07-22 01:42:42,340:INFO:Assigning column types.
2023-07-22 01:42:42,346:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:42:42,391:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,394:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,432:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:42,435:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:42,480:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,480:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,509:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:42,511:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:42,512:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:42:42,570:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,598:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:42,602:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:42,652:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,681:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:42,684:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:42,684:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:42:42,759:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:42,761:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:42,837:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:42,839:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:42,841:INFO:Preparing preprocessing pipeline...
2023-07-22 01:42:42,843:INFO:Set up iterative imputation.
2023-07-22 01:42:42,843:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,848:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,854:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,941:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:42:42,991:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:42,994:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:43,076:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:43,078:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:43,109:INFO:Set up encoding of categorical features.
2023-07-22 01:42:43,109:INFO:Set up imbalanced handling.
2023-07-22 01:42:43,109:INFO:Set up column transformation.
2023-07-22 01:42:43,109:INFO:Set up feature selection.
2023-07-22 01:42:43,184:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:43,186:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:43,916:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:42:43,945:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:42:43,945:INFO:Creating final display dataframe.
2023-07-22 01:42:45,903:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment         False
31                  Experiment Name    balanceado
32                              USI          67a9
2023-07-22 01:42:46,004:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:46,007:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:46,082:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:42:46,084:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:42:46,085:INFO:setup() successfully completed in 4.25s...............
2023-07-22 01:43:09,009:WARNING:C:\TEMP\ipykernel_15916\2839126861.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).
  ipython.magic("matplotlib inline")

2023-07-22 01:43:11,128:INFO:PyCaret ClassificationExperiment
2023-07-22 01:43:11,128:INFO:Logging name: balanceado
2023-07-22 01:43:11,128:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:43:11,128:INFO:version 3.0.4
2023-07-22 01:43:11,128:INFO:Initializing setup()
2023-07-22 01:43:11,128:INFO:self.USI: 35f6
2023-07-22 01:43:11,128:INFO:self._variable_keys: {'target_param', 'y_train', 'logging_param', 'fold_shuffle_param', 'is_multiclass', 'html_param', '_ml_usecase', 'data', 'pipeline', '_available_plots', 'memory', 'idx', 'exp_name_log', 'gpu_param', 'fold_groups_param', 'y_test', 'n_jobs_param', 'seed', 'exp_id', 'fold_generator', 'X', 'X_train', 'fix_imbalance', 'USI', 'gpu_n_jobs_param', 'log_plots_param', 'X_test', 'y'}
2023-07-22 01:43:11,128:INFO:Checking environment
2023-07-22 01:43:11,128:INFO:python_version: 3.10.8
2023-07-22 01:43:11,128:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:43:11,128:INFO:machine: AMD64
2023-07-22 01:43:11,128:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:43:11,134:INFO:Memory: svmem(total=16505966592, available=3798638592, percent=77.0, used=12707328000, free=3798638592)
2023-07-22 01:43:11,134:INFO:Physical Core: 6
2023-07-22 01:43:11,134:INFO:Logical Core: 12
2023-07-22 01:43:11,134:INFO:Checking libraries
2023-07-22 01:43:11,134:INFO:System:
2023-07-22 01:43:11,134:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:43:11,134:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:43:11,134:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:43:11,134:INFO:PyCaret required dependencies:
2023-07-22 01:43:11,134:INFO:                 pip: 22.2.2
2023-07-22 01:43:11,134:INFO:          setuptools: 63.2.0
2023-07-22 01:43:11,134:INFO:             pycaret: 3.0.4
2023-07-22 01:43:11,135:INFO:             IPython: 8.11.0
2023-07-22 01:43:11,135:INFO:          ipywidgets: 8.0.7
2023-07-22 01:43:11,135:INFO:                tqdm: 4.64.1
2023-07-22 01:43:11,135:INFO:               numpy: 1.23.5
2023-07-22 01:43:11,135:INFO:              pandas: 1.5.3
2023-07-22 01:43:11,135:INFO:              jinja2: 3.1.2
2023-07-22 01:43:11,135:INFO:               scipy: 1.9.3
2023-07-22 01:43:11,135:INFO:              joblib: 1.2.0
2023-07-22 01:43:11,135:INFO:             sklearn: 1.2.2
2023-07-22 01:43:11,135:INFO:                pyod: 1.1.0
2023-07-22 01:43:11,135:INFO:            imblearn: 0.10.1
2023-07-22 01:43:11,135:INFO:   category_encoders: 2.6.1
2023-07-22 01:43:11,135:INFO:            lightgbm: 3.3.5
2023-07-22 01:43:11,135:INFO:               numba: 0.57.0
2023-07-22 01:43:11,135:INFO:            requests: 2.28.2
2023-07-22 01:43:11,135:INFO:          matplotlib: 3.7.1
2023-07-22 01:43:11,135:INFO:          scikitplot: 0.3.7
2023-07-22 01:43:11,135:INFO:         yellowbrick: 1.5
2023-07-22 01:43:11,135:INFO:              plotly: 5.15.0
2023-07-22 01:43:11,135:INFO:    plotly-resampler: Not installed
2023-07-22 01:43:11,135:INFO:             kaleido: 0.2.1
2023-07-22 01:43:11,135:INFO:           schemdraw: 0.15
2023-07-22 01:43:11,135:INFO:         statsmodels: 0.13.5
2023-07-22 01:43:11,135:INFO:              sktime: 0.21.0
2023-07-22 01:43:11,135:INFO:               tbats: 1.1.3
2023-07-22 01:43:11,135:INFO:            pmdarima: 2.0.3
2023-07-22 01:43:11,135:INFO:              psutil: 5.9.4
2023-07-22 01:43:11,136:INFO:          markupsafe: 2.1.2
2023-07-22 01:43:11,136:INFO:             pickle5: Not installed
2023-07-22 01:43:11,136:INFO:         cloudpickle: 2.2.1
2023-07-22 01:43:11,136:INFO:         deprecation: 2.1.0
2023-07-22 01:43:11,136:INFO:              xxhash: 3.2.0
2023-07-22 01:43:11,136:INFO:           wurlitzer: Not installed
2023-07-22 01:43:11,136:INFO:PyCaret optional dependencies:
2023-07-22 01:43:11,136:INFO:                shap: 0.41.0
2023-07-22 01:43:11,136:INFO:           interpret: 0.4.2
2023-07-22 01:43:11,136:INFO:                umap: 0.5.3
2023-07-22 01:43:11,136:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:43:11,136:INFO:  explainerdashboard: Not installed
2023-07-22 01:43:11,136:INFO:             autoviz: Not installed
2023-07-22 01:43:11,136:INFO:           fairlearn: Not installed
2023-07-22 01:43:11,136:INFO:          deepchecks: Not installed
2023-07-22 01:43:11,136:INFO:             xgboost: 1.7.6
2023-07-22 01:43:11,136:INFO:            catboost: Not installed
2023-07-22 01:43:11,136:INFO:              kmodes: Not installed
2023-07-22 01:43:11,136:INFO:             mlxtend: Not installed
2023-07-22 01:43:11,136:INFO:       statsforecast: Not installed
2023-07-22 01:43:11,136:INFO:        tune_sklearn: Not installed
2023-07-22 01:43:11,136:INFO:                 ray: Not installed
2023-07-22 01:43:11,136:INFO:            hyperopt: Not installed
2023-07-22 01:43:11,136:INFO:              optuna: 3.2.0
2023-07-22 01:43:11,136:INFO:               skopt: Not installed
2023-07-22 01:43:11,136:INFO:              mlflow: 2.4.2
2023-07-22 01:43:11,136:INFO:              gradio: Not installed
2023-07-22 01:43:11,136:INFO:             fastapi: 0.95.2
2023-07-22 01:43:11,137:INFO:             uvicorn: 0.22.0
2023-07-22 01:43:11,137:INFO:              m2cgen: Not installed
2023-07-22 01:43:11,137:INFO:           evidently: Not installed
2023-07-22 01:43:11,137:INFO:               fugue: Not installed
2023-07-22 01:43:11,137:INFO:           streamlit: Not installed
2023-07-22 01:43:11,137:INFO:             prophet: Not installed
2023-07-22 01:43:11,137:INFO:None
2023-07-22 01:43:11,137:INFO:Set up data.
2023-07-22 01:43:11,160:INFO:Set up train/test split.
2023-07-22 01:43:11,160:INFO:Set up data.
2023-07-22 01:43:11,180:INFO:Set up index.
2023-07-22 01:43:11,180:INFO:Set up folding strategy.
2023-07-22 01:43:11,180:INFO:Assigning column types.
2023-07-22 01:43:11,187:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:43:11,258:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,260:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,307:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:11,312:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:11,367:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,368:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,400:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:11,403:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:11,403:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:43:11,450:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,485:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:11,487:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:11,538:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,567:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:11,570:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:11,570:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:43:11,647:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:11,650:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:11,729:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:11,732:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:11,733:INFO:Preparing preprocessing pipeline...
2023-07-22 01:43:11,734:INFO:Set up iterative imputation.
2023-07-22 01:43:11,734:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,739:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,744:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,811:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:43:11,859:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:11,861:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:11,943:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:11,946:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:11,981:INFO:Set up encoding of categorical features.
2023-07-22 01:43:11,981:INFO:Set up imbalanced handling.
2023-07-22 01:43:11,981:INFO:Set up column transformation.
2023-07-22 01:43:11,981:INFO:Set up feature selection.
2023-07-22 01:43:12,073:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:12,075:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:12,516:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:43:12,545:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:43:12,545:INFO:Creating final display dataframe.
2023-07-22 01:43:14,426:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment         False
31                  Experiment Name    balanceado
32                              USI          35f6
2023-07-22 01:43:14,525:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:14,529:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:14,605:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:14,608:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:14,608:INFO:setup() successfully completed in 3.69s...............
2023-07-22 01:43:30,537:WARNING:C:\TEMP\ipykernel_15916\1419556991.py:8: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).
  ipython.magic("matplotlib inline")

2023-07-22 01:43:35,462:WARNING:C:\TEMP\ipykernel_15916\3796186640.py:8: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).
  ipython.magic("matplotlib inline")

2023-07-22 01:43:37,606:INFO:PyCaret ClassificationExperiment
2023-07-22 01:43:37,606:INFO:Logging name: balanceado
2023-07-22 01:43:37,606:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:43:37,606:INFO:version 3.0.4
2023-07-22 01:43:37,606:INFO:Initializing setup()
2023-07-22 01:43:37,606:INFO:self.USI: 5600
2023-07-22 01:43:37,606:INFO:self._variable_keys: {'target_param', 'y_train', 'logging_param', 'fold_shuffle_param', 'is_multiclass', 'html_param', '_ml_usecase', 'data', 'pipeline', '_available_plots', 'memory', 'idx', 'exp_name_log', 'gpu_param', 'fold_groups_param', 'y_test', 'n_jobs_param', 'seed', 'exp_id', 'fold_generator', 'X', 'X_train', 'fix_imbalance', 'USI', 'gpu_n_jobs_param', 'log_plots_param', 'X_test', 'y'}
2023-07-22 01:43:37,606:INFO:Checking environment
2023-07-22 01:43:37,606:INFO:python_version: 3.10.8
2023-07-22 01:43:37,606:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:43:37,606:INFO:machine: AMD64
2023-07-22 01:43:37,606:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:43:37,612:INFO:Memory: svmem(total=16505966592, available=3772743680, percent=77.1, used=12733222912, free=3772743680)
2023-07-22 01:43:37,612:INFO:Physical Core: 6
2023-07-22 01:43:37,612:INFO:Logical Core: 12
2023-07-22 01:43:37,612:INFO:Checking libraries
2023-07-22 01:43:37,613:INFO:System:
2023-07-22 01:43:37,613:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:43:37,613:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:43:37,613:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:43:37,613:INFO:PyCaret required dependencies:
2023-07-22 01:43:37,613:INFO:                 pip: 22.2.2
2023-07-22 01:43:37,613:INFO:          setuptools: 63.2.0
2023-07-22 01:43:37,613:INFO:             pycaret: 3.0.4
2023-07-22 01:43:37,613:INFO:             IPython: 8.11.0
2023-07-22 01:43:37,613:INFO:          ipywidgets: 8.0.7
2023-07-22 01:43:37,613:INFO:                tqdm: 4.64.1
2023-07-22 01:43:37,613:INFO:               numpy: 1.23.5
2023-07-22 01:43:37,613:INFO:              pandas: 1.5.3
2023-07-22 01:43:37,613:INFO:              jinja2: 3.1.2
2023-07-22 01:43:37,613:INFO:               scipy: 1.9.3
2023-07-22 01:43:37,613:INFO:              joblib: 1.2.0
2023-07-22 01:43:37,613:INFO:             sklearn: 1.2.2
2023-07-22 01:43:37,613:INFO:                pyod: 1.1.0
2023-07-22 01:43:37,613:INFO:            imblearn: 0.10.1
2023-07-22 01:43:37,613:INFO:   category_encoders: 2.6.1
2023-07-22 01:43:37,613:INFO:            lightgbm: 3.3.5
2023-07-22 01:43:37,613:INFO:               numba: 0.57.0
2023-07-22 01:43:37,613:INFO:            requests: 2.28.2
2023-07-22 01:43:37,613:INFO:          matplotlib: 3.7.1
2023-07-22 01:43:37,613:INFO:          scikitplot: 0.3.7
2023-07-22 01:43:37,614:INFO:         yellowbrick: 1.5
2023-07-22 01:43:37,614:INFO:              plotly: 5.15.0
2023-07-22 01:43:37,614:INFO:    plotly-resampler: Not installed
2023-07-22 01:43:37,614:INFO:             kaleido: 0.2.1
2023-07-22 01:43:37,614:INFO:           schemdraw: 0.15
2023-07-22 01:43:37,614:INFO:         statsmodels: 0.13.5
2023-07-22 01:43:37,614:INFO:              sktime: 0.21.0
2023-07-22 01:43:37,614:INFO:               tbats: 1.1.3
2023-07-22 01:43:37,614:INFO:            pmdarima: 2.0.3
2023-07-22 01:43:37,614:INFO:              psutil: 5.9.4
2023-07-22 01:43:37,614:INFO:          markupsafe: 2.1.2
2023-07-22 01:43:37,614:INFO:             pickle5: Not installed
2023-07-22 01:43:37,614:INFO:         cloudpickle: 2.2.1
2023-07-22 01:43:37,614:INFO:         deprecation: 2.1.0
2023-07-22 01:43:37,614:INFO:              xxhash: 3.2.0
2023-07-22 01:43:37,614:INFO:           wurlitzer: Not installed
2023-07-22 01:43:37,614:INFO:PyCaret optional dependencies:
2023-07-22 01:43:37,614:INFO:                shap: 0.41.0
2023-07-22 01:43:37,614:INFO:           interpret: 0.4.2
2023-07-22 01:43:37,614:INFO:                umap: 0.5.3
2023-07-22 01:43:37,614:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:43:37,614:INFO:  explainerdashboard: Not installed
2023-07-22 01:43:37,614:INFO:             autoviz: Not installed
2023-07-22 01:43:37,614:INFO:           fairlearn: Not installed
2023-07-22 01:43:37,614:INFO:          deepchecks: Not installed
2023-07-22 01:43:37,615:INFO:             xgboost: 1.7.6
2023-07-22 01:43:37,615:INFO:            catboost: Not installed
2023-07-22 01:43:37,615:INFO:              kmodes: Not installed
2023-07-22 01:43:37,615:INFO:             mlxtend: Not installed
2023-07-22 01:43:37,615:INFO:       statsforecast: Not installed
2023-07-22 01:43:37,615:INFO:        tune_sklearn: Not installed
2023-07-22 01:43:37,615:INFO:                 ray: Not installed
2023-07-22 01:43:37,615:INFO:            hyperopt: Not installed
2023-07-22 01:43:37,615:INFO:              optuna: 3.2.0
2023-07-22 01:43:37,615:INFO:               skopt: Not installed
2023-07-22 01:43:37,615:INFO:              mlflow: 2.4.2
2023-07-22 01:43:37,615:INFO:              gradio: Not installed
2023-07-22 01:43:37,615:INFO:             fastapi: 0.95.2
2023-07-22 01:43:37,615:INFO:             uvicorn: 0.22.0
2023-07-22 01:43:37,615:INFO:              m2cgen: Not installed
2023-07-22 01:43:37,615:INFO:           evidently: Not installed
2023-07-22 01:43:37,615:INFO:               fugue: Not installed
2023-07-22 01:43:37,615:INFO:           streamlit: Not installed
2023-07-22 01:43:37,615:INFO:             prophet: Not installed
2023-07-22 01:43:37,615:INFO:None
2023-07-22 01:43:37,615:INFO:Set up data.
2023-07-22 01:43:37,638:INFO:Set up train/test split.
2023-07-22 01:43:37,638:INFO:Set up data.
2023-07-22 01:43:37,657:INFO:Set up index.
2023-07-22 01:43:37,658:INFO:Set up folding strategy.
2023-07-22 01:43:37,658:INFO:Assigning column types.
2023-07-22 01:43:37,665:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:43:37,721:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:43:37,722:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:43:37,752:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:37,756:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:37,805:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:43:37,805:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:43:37,834:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:37,837:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:37,838:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:43:37,886:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:43:37,914:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:37,918:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:37,965:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:43:37,993:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:37,997:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:37,997:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:43:38,073:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:38,076:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:38,160:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:38,163:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:38,164:INFO:Preparing preprocessing pipeline...
2023-07-22 01:43:38,165:INFO:Set up iterative imputation.
2023-07-22 01:43:38,165:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:43:38,170:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:43:38,174:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:43:38,240:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:43:38,289:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:38,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:38,366:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:38,370:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:38,405:INFO:Set up encoding of categorical features.
2023-07-22 01:43:38,405:INFO:Set up imbalanced handling.
2023-07-22 01:43:38,405:INFO:Set up column transformation.
2023-07-22 01:43:38,405:INFO:Set up feature selection.
2023-07-22 01:43:38,481:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:38,484:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:38,920:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:43:38,950:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:43:38,950:INFO:Creating final display dataframe.
2023-07-22 01:43:39,765:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment         False
31                  Experiment Name    balanceado
32                              USI          5600
2023-07-22 01:43:39,867:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:39,869:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:39,946:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:43:39,948:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:43:39,949:INFO:setup() successfully completed in 2.51s...............
2023-07-22 01:44:39,280:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  ver = LooseVersion(dist.metadata["Version"])

2023-07-22 01:44:39,624:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:44:39,624:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:44:39,624:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:44:39,624:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 01:44:39,637:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  mpl_ge_150 = LooseVersion(mpl.__version__) >= "1.5.0"

2023-07-22 01:44:39,645:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)

2023-07-22 01:44:39,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  mpl_ge_150 = LooseVersion(mpl.__version__) >= "1.5.0"

2023-07-22 01:44:39,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning: This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0
  warnings.warn("This module was deprecated in version 0.3.0 and its functions "

2023-07-22 01:44:40,082:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning: 'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680
  import urllib3

2023-07-22 01:44:45,854:INFO:PyCaret ClassificationExperiment
2023-07-22 01:44:45,854:INFO:Logging name: balanceado
2023-07-22 01:44:45,854:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 01:44:45,854:INFO:version 3.0.4
2023-07-22 01:44:45,854:INFO:Initializing setup()
2023-07-22 01:44:45,854:INFO:self.USI: 1806
2023-07-22 01:44:45,854:INFO:self._variable_keys: {'gpu_param', 'html_param', 'log_plots_param', 'X_test', 'gpu_n_jobs_param', '_available_plots', 'memory', 'exp_id', '_ml_usecase', 'fold_shuffle_param', 'fold_groups_param', 'seed', 'is_multiclass', 'fold_generator', 'exp_name_log', 'target_param', 'X_train', 'y', 'X', 'y_train', 'idx', 'pipeline', 'data', 'n_jobs_param', 'logging_param', 'fix_imbalance', 'y_test', 'USI'}
2023-07-22 01:44:45,854:INFO:Checking environment
2023-07-22 01:44:45,854:INFO:python_version: 3.10.8
2023-07-22 01:44:45,854:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 01:44:45,854:INFO:machine: AMD64
2023-07-22 01:44:45,854:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 01:44:45,861:INFO:Memory: svmem(total=16505966592, available=4917891072, percent=70.2, used=11588075520, free=4917891072)
2023-07-22 01:44:45,861:INFO:Physical Core: 6
2023-07-22 01:44:45,861:INFO:Logical Core: 12
2023-07-22 01:44:45,861:INFO:Checking libraries
2023-07-22 01:44:45,861:INFO:System:
2023-07-22 01:44:45,861:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 01:44:45,861:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 01:44:45,861:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 01:44:45,861:INFO:PyCaret required dependencies:
2023-07-22 01:44:45,861:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  ver = LooseVersion(ver)

2023-07-22 01:44:45,864:INFO:                 pip: 22.2.2
2023-07-22 01:44:45,864:INFO:          setuptools: 63.2.0
2023-07-22 01:44:45,864:INFO:             pycaret: 3.0.4
2023-07-22 01:44:45,864:INFO:             IPython: 8.11.0
2023-07-22 01:44:45,864:INFO:          ipywidgets: 8.0.7
2023-07-22 01:44:45,864:INFO:                tqdm: 4.64.1
2023-07-22 01:44:45,864:INFO:               numpy: 1.23.5
2023-07-22 01:44:45,864:INFO:              pandas: 1.5.3
2023-07-22 01:44:45,864:INFO:              jinja2: 3.1.2
2023-07-22 01:44:45,864:INFO:               scipy: 1.9.3
2023-07-22 01:44:45,864:INFO:              joblib: 1.2.0
2023-07-22 01:44:45,864:INFO:             sklearn: 1.2.2
2023-07-22 01:44:45,864:INFO:                pyod: 1.1.0
2023-07-22 01:44:45,864:INFO:            imblearn: 0.10.1
2023-07-22 01:44:45,864:INFO:   category_encoders: 2.6.1
2023-07-22 01:44:45,864:INFO:            lightgbm: 3.3.5
2023-07-22 01:44:45,864:INFO:               numba: 0.57.0
2023-07-22 01:44:45,864:INFO:            requests: 2.28.2
2023-07-22 01:44:45,864:INFO:          matplotlib: 3.7.1
2023-07-22 01:44:45,864:INFO:          scikitplot: 0.3.7
2023-07-22 01:44:45,864:INFO:         yellowbrick: 1.5
2023-07-22 01:44:45,864:INFO:              plotly: 5.15.0
2023-07-22 01:44:45,864:INFO:    plotly-resampler: Not installed
2023-07-22 01:44:45,864:INFO:             kaleido: 0.2.1
2023-07-22 01:44:45,864:INFO:           schemdraw: 0.15
2023-07-22 01:44:45,864:INFO:         statsmodels: 0.13.5
2023-07-22 01:44:45,864:INFO:              sktime: 0.21.0
2023-07-22 01:44:45,864:INFO:               tbats: 1.1.3
2023-07-22 01:44:45,864:INFO:            pmdarima: 2.0.3
2023-07-22 01:44:45,865:INFO:              psutil: 5.9.4
2023-07-22 01:44:45,865:INFO:          markupsafe: 2.1.2
2023-07-22 01:44:45,865:INFO:             pickle5: Not installed
2023-07-22 01:44:45,865:INFO:         cloudpickle: 2.2.1
2023-07-22 01:44:45,865:INFO:         deprecation: 2.1.0
2023-07-22 01:44:45,865:INFO:              xxhash: 3.2.0
2023-07-22 01:44:45,865:INFO:           wurlitzer: Not installed
2023-07-22 01:44:45,865:INFO:PyCaret optional dependencies:
2023-07-22 01:44:46,189:INFO:                shap: 0.41.0
2023-07-22 01:44:46,189:INFO:           interpret: 0.4.2
2023-07-22 01:44:46,189:INFO:                umap: 0.5.3
2023-07-22 01:44:46,189:INFO:    pandas_profiling: 4.1.2
2023-07-22 01:44:46,189:INFO:  explainerdashboard: Not installed
2023-07-22 01:44:46,189:INFO:             autoviz: Not installed
2023-07-22 01:44:46,189:INFO:           fairlearn: Not installed
2023-07-22 01:44:46,189:INFO:          deepchecks: Not installed
2023-07-22 01:44:46,189:INFO:             xgboost: 1.7.6
2023-07-22 01:44:46,189:INFO:            catboost: Not installed
2023-07-22 01:44:46,189:INFO:              kmodes: Not installed
2023-07-22 01:44:46,189:INFO:             mlxtend: Not installed
2023-07-22 01:44:46,189:INFO:       statsforecast: Not installed
2023-07-22 01:44:46,189:INFO:        tune_sklearn: Not installed
2023-07-22 01:44:46,189:INFO:                 ray: Not installed
2023-07-22 01:44:46,189:INFO:            hyperopt: Not installed
2023-07-22 01:44:46,189:INFO:              optuna: 3.2.0
2023-07-22 01:44:46,189:INFO:               skopt: Not installed
2023-07-22 01:44:46,189:INFO:              mlflow: 2.4.2
2023-07-22 01:44:46,189:INFO:              gradio: Not installed
2023-07-22 01:44:46,190:INFO:             fastapi: 0.95.2
2023-07-22 01:44:46,190:INFO:             uvicorn: 0.22.0
2023-07-22 01:44:46,190:INFO:              m2cgen: Not installed
2023-07-22 01:44:46,190:INFO:           evidently: Not installed
2023-07-22 01:44:46,190:INFO:               fugue: Not installed
2023-07-22 01:44:46,190:INFO:           streamlit: Not installed
2023-07-22 01:44:46,190:INFO:             prophet: Not installed
2023-07-22 01:44:46,190:INFO:None
2023-07-22 01:44:46,190:INFO:Set up data.
2023-07-22 01:44:46,210:INFO:Set up train/test split.
2023-07-22 01:44:46,210:INFO:Set up data.
2023-07-22 01:44:46,225:INFO:Set up index.
2023-07-22 01:44:46,226:INFO:Set up folding strategy.
2023-07-22 01:44:46,226:INFO:Assigning column types.
2023-07-22 01:44:46,232:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 01:44:46,277:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,279:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,314:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:46,317:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:46,369:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,369:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,403:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:46,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:46,406:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 01:44:46,452:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,482:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:46,486:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:46,533:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,562:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:46,564:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:46,566:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 01:44:46,646:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:46,652:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:46,735:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:46,738:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:46,739:INFO:Preparing preprocessing pipeline...
2023-07-22 01:44:46,741:INFO:Set up iterative imputation.
2023-07-22 01:44:46,741:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,745:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,750:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,814:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 01:44:46,861:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:46,863:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:46,951:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:46,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:46,984:INFO:Set up encoding of categorical features.
2023-07-22 01:44:46,984:INFO:Set up imbalanced handling.
2023-07-22 01:44:46,984:INFO:Set up column transformation.
2023-07-22 01:44:46,984:INFO:Set up feature selection.
2023-07-22 01:44:47,065:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:47,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:47,516:INFO:Finished creating preprocessing pipeline.
2023-07-22 01:44:47,547:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 01:44:47,547:INFO:Creating final display dataframe.
2023-07-22 01:44:48,369:INFO:Setup _display_container:                         Description         Value
0                        Session id            42
1                            Target  credit_score
2                       Target type        Binary
3               Original data shape   (12500, 22)
4            Transformed data shape   (16736, 21)
5       Transformed train set shape   (14236, 21)
6        Transformed test set shape    (2500, 21)
7                   Ignore features             1
8                  Numeric features            17
9              Categorical features             3
10         Rows with missing values         36.1%
11                       Preprocess          True
12                  Imputation type     iterative
13  Iterative imputation iterations             5
14        Numeric iterative imputer      lightgbm
15    Categorical iterative imputer      lightgbm
16         Maximum one-hot encoding            25
17                  Encoding method          None
18                    Fix imbalance          True
19             Fix imbalance method         SMOTE
20                   Transformation          True
21            Transformation method   yeo-johnson
22                Feature selection          True
23         Feature selection method       classic
24      Feature selection estimator      lightgbm
25      Number of features selected            20
26                   Fold Generator         KFold
27                      Fold Number            10
28                         CPU Jobs            -1
29                          Use GPU         False
30                   Log Experiment         False
31                  Experiment Name    balanceado
32                              USI          1806
2023-07-22 01:44:48,474:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:48,477:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:48,552:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 01:44:48,555:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 01:44:48,555:INFO:setup() successfully completed in 2.85s...............
2023-07-22 01:44:56,118:INFO:Initializing compare_models()
2023-07-22 01:44:56,118:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 01:44:56,118:INFO:Checking exceptions
2023-07-22 01:44:56,124:INFO:Preparing display monitor
2023-07-22 01:44:56,149:INFO:Initializing Logistic Regression
2023-07-22 01:44:56,149:INFO:Total runtime is 0.0 minutes
2023-07-22 01:44:56,152:INFO:SubProcess create_model() called ==================================
2023-07-22 01:44:56,153:INFO:Initializing create_model()
2023-07-22 01:44:56,153:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:44:56,153:INFO:Checking exceptions
2023-07-22 01:44:56,154:INFO:Importing libraries
2023-07-22 01:44:56,154:INFO:Copying training dataset
2023-07-22 01:44:56,164:INFO:Defining folds
2023-07-22 01:44:56,164:INFO:Declaring metric variables
2023-07-22 01:44:56,169:INFO:Importing untrained model
2023-07-22 01:44:56,173:INFO:Logistic Regression Imported successfully
2023-07-22 01:44:56,182:INFO:Starting cross validation
2023-07-22 01:44:56,425:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:45:02,201:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,223:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,234:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,326:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,368:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,576:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,603:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,619:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,654:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,690:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,732:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,768:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,778:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:02,999:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,036:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,039:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,156:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,182:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,194:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,203:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,288:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,329:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,331:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,333:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,366:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,436:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,536:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,619:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,655:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,703:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,707:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,711:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,737:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,953:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,965:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:03,989:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,012:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,067:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,092:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,135:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,207:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,215:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,325:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,428:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,458:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,460:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,484:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,525:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,554:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,576:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,701:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,807:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,843:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,895:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:04,940:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,004:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,032:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,065:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,076:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,199:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,230:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,298:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,606:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,728:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,794:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,862:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,928:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,929:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:05,943:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,037:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,155:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,211:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,231:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,286:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,345:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,423:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,503:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,506:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,537:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,635:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,710:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,719:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,845:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,862:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,877:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,967:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:06,997:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,124:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,143:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,196:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,201:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,203:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,306:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,343:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,346:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,570:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,626:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,826:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,882:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,910:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:07,925:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,047:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,177:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,265:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,278:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,376:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,397:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,507:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,563:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,595:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,635:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,738:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,835:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,848:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:08,864:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,024:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,053:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,136:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,147:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,152:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,341:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,455:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,469:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,872:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,872:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,913:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,919:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:09,983:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,017:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,194:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,274:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,300:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,302:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,386:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,402:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,496:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,581:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,644:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,853:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,881:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,906:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,932:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:10,995:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,133:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,142:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,235:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,255:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,302:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,318:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,367:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,383:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,440:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,541:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,605:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,707:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,779:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,790:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,793:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,842:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,945:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:11,972:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,021:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,153:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,210:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,283:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,309:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,390:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,404:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,503:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,540:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,680:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,741:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,752:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,775:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,779:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,804:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:12,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,907:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:12,911:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,030:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,079:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,139:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,150:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,206:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,231:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,254:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,298:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,333:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,470:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,475:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,495:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,513:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,646:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,655:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,683:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,873:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,874:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,921:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:13,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,015:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,025:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,165:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,217:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,271:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,303:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,356:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,377:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,406:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,458:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,607:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,617:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,656:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,660:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,677:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,748:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:14,847:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,028:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,046:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,048:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,057:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,069:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,126:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,134:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,259:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,381:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,394:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,405:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,412:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,412:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,425:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,500:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:15,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,666:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:15,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,761:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,774:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,860:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:15,966:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,001:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,050:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,057:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,074:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,081:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:16,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,415:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,417:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,484:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,513:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,579:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,721:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,764:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,926:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:16,950:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:17,118:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:17,175:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:17,175:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:17,206:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:17,272:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:17,311:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:17,320:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:17,370:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:17,512:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:17,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:17,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:17,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:17,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:17,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-22 01:45:18,222:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-22 01:45:18,722:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:18,839:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:19,341:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:19,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:19,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:19,511:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:19,574:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:20,022:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-22 01:45:20,051:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:20,620:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:20,830:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:20,845:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:20,917:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:21,403:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:21,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:21,540:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:21,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:21,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:22,031:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:22,156:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 01:45:22,163:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:22,366:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:22,660:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:22,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:22,967:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:23,238:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:23,383:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:23,654:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:23,780:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\numpy\core\_methods.py:236: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

2023-07-22 01:45:24,182:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:24,209:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:24,588:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:24,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:24,810:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:25,009:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 01:45:25,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:25,570:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:25,645:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:25,887:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:26,260:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:26,399:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:26,736:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:26,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:27,020:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:27,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:27,131:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:27,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:27,513:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:27,689:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:27,697:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:28,055:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:45:28,176:INFO:Calculating mean and std
2023-07-22 01:45:28,177:INFO:Creating metrics dataframe
2023-07-22 01:45:28,448:INFO:Uploading results into container
2023-07-22 01:45:28,450:INFO:Uploading model into container now
2023-07-22 01:45:28,450:INFO:_master_model_container: 1
2023-07-22 01:45:28,451:INFO:_display_container: 2
2023-07-22 01:45:28,451:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 01:45:28,452:INFO:create_model() successfully completed......................................
2023-07-22 01:45:28,545:INFO:SubProcess create_model() end ==================================
2023-07-22 01:45:28,545:INFO:Creating metrics dataframe
2023-07-22 01:45:28,554:INFO:Initializing K Neighbors Classifier
2023-07-22 01:45:28,554:INFO:Total runtime is 0.5400853594144185 minutes
2023-07-22 01:45:28,557:INFO:SubProcess create_model() called ==================================
2023-07-22 01:45:28,558:INFO:Initializing create_model()
2023-07-22 01:45:28,558:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:45:28,558:INFO:Checking exceptions
2023-07-22 01:45:28,559:INFO:Importing libraries
2023-07-22 01:45:28,559:INFO:Copying training dataset
2023-07-22 01:45:28,567:INFO:Defining folds
2023-07-22 01:45:28,567:INFO:Declaring metric variables
2023-07-22 01:45:28,572:INFO:Importing untrained model
2023-07-22 01:45:28,577:INFO:K Neighbors Classifier Imported successfully
2023-07-22 01:45:28,584:INFO:Starting cross validation
2023-07-22 01:45:28,791:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:45:32,735:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:32,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:32,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:32,841:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:32,874:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:32,971:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:32,986:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:35,860:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:36,854:INFO:Calculating mean and std
2023-07-22 01:45:36,855:INFO:Creating metrics dataframe
2023-07-22 01:45:37,170:INFO:Uploading results into container
2023-07-22 01:45:37,172:INFO:Uploading model into container now
2023-07-22 01:45:37,173:INFO:_master_model_container: 2
2023-07-22 01:45:37,173:INFO:_display_container: 2
2023-07-22 01:45:37,173:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 01:45:37,173:INFO:create_model() successfully completed......................................
2023-07-22 01:45:37,279:INFO:SubProcess create_model() end ==================================
2023-07-22 01:45:37,279:INFO:Creating metrics dataframe
2023-07-22 01:45:37,292:INFO:Initializing Naive Bayes
2023-07-22 01:45:37,292:INFO:Total runtime is 0.6857269406318665 minutes
2023-07-22 01:45:37,296:INFO:SubProcess create_model() called ==================================
2023-07-22 01:45:37,297:INFO:Initializing create_model()
2023-07-22 01:45:37,297:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:45:37,297:INFO:Checking exceptions
2023-07-22 01:45:37,297:INFO:Importing libraries
2023-07-22 01:45:37,297:INFO:Copying training dataset
2023-07-22 01:45:37,308:INFO:Defining folds
2023-07-22 01:45:37,308:INFO:Declaring metric variables
2023-07-22 01:45:37,312:INFO:Importing untrained model
2023-07-22 01:45:37,317:INFO:Naive Bayes Imported successfully
2023-07-22 01:45:37,326:INFO:Starting cross validation
2023-07-22 01:45:37,569:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:45:41,295:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:41,363:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:41,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:41,487:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:41,569:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:41,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:41,616:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:41,658:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:41,662:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:43,984:INFO:Calculating mean and std
2023-07-22 01:45:43,986:INFO:Creating metrics dataframe
2023-07-22 01:45:44,261:INFO:Uploading results into container
2023-07-22 01:45:44,263:INFO:Uploading model into container now
2023-07-22 01:45:44,263:INFO:_master_model_container: 3
2023-07-22 01:45:44,263:INFO:_display_container: 2
2023-07-22 01:45:44,263:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 01:45:44,263:INFO:create_model() successfully completed......................................
2023-07-22 01:45:44,356:INFO:SubProcess create_model() end ==================================
2023-07-22 01:45:44,357:INFO:Creating metrics dataframe
2023-07-22 01:45:44,368:INFO:Initializing Decision Tree Classifier
2023-07-22 01:45:44,368:INFO:Total runtime is 0.8036519646644592 minutes
2023-07-22 01:45:44,372:INFO:SubProcess create_model() called ==================================
2023-07-22 01:45:44,372:INFO:Initializing create_model()
2023-07-22 01:45:44,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:45:44,373:INFO:Checking exceptions
2023-07-22 01:45:44,373:INFO:Importing libraries
2023-07-22 01:45:44,373:INFO:Copying training dataset
2023-07-22 01:45:44,380:INFO:Defining folds
2023-07-22 01:45:44,380:INFO:Declaring metric variables
2023-07-22 01:45:44,385:INFO:Importing untrained model
2023-07-22 01:45:44,390:INFO:Decision Tree Classifier Imported successfully
2023-07-22 01:45:44,397:INFO:Starting cross validation
2023-07-22 01:45:44,604:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:45:46,406:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:45:47,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 426, in predict
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:45:48,634:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:48,645:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:48,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:48,771:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:48,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:48,821:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:48,826:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:48,839:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:48,842:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:51,254:INFO:Calculating mean and std
2023-07-22 01:45:51,256:INFO:Creating metrics dataframe
2023-07-22 01:45:51,527:INFO:Uploading results into container
2023-07-22 01:45:51,528:INFO:Uploading model into container now
2023-07-22 01:45:51,528:INFO:_master_model_container: 4
2023-07-22 01:45:51,528:INFO:_display_container: 2
2023-07-22 01:45:51,528:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 01:45:51,529:INFO:create_model() successfully completed......................................
2023-07-22 01:45:51,620:INFO:SubProcess create_model() end ==================================
2023-07-22 01:45:51,620:INFO:Creating metrics dataframe
2023-07-22 01:45:51,631:INFO:Initializing SVM - Linear Kernel
2023-07-22 01:45:51,631:INFO:Total runtime is 0.9247080405553182 minutes
2023-07-22 01:45:51,635:INFO:SubProcess create_model() called ==================================
2023-07-22 01:45:51,635:INFO:Initializing create_model()
2023-07-22 01:45:51,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:45:51,635:INFO:Checking exceptions
2023-07-22 01:45:51,635:INFO:Importing libraries
2023-07-22 01:45:51,635:INFO:Copying training dataset
2023-07-22 01:45:51,644:INFO:Defining folds
2023-07-22 01:45:51,644:INFO:Declaring metric variables
2023-07-22 01:45:51,649:INFO:Importing untrained model
2023-07-22 01:45:51,652:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 01:45:51,659:INFO:Starting cross validation
2023-07-22 01:45:51,862:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:45:53,452:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,538:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,570:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,580:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,644:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,650:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,654:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,661:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,667:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,672:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,673:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,679:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,695:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:55,795:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:45:55,803:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 01:45:57,590:INFO:Calculating mean and std
2023-07-22 01:45:57,591:INFO:Creating metrics dataframe
2023-07-22 01:45:57,889:INFO:Uploading results into container
2023-07-22 01:45:57,889:INFO:Uploading model into container now
2023-07-22 01:45:57,890:INFO:_master_model_container: 5
2023-07-22 01:45:57,890:INFO:_display_container: 2
2023-07-22 01:45:57,890:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 01:45:57,890:INFO:create_model() successfully completed......................................
2023-07-22 01:45:57,991:INFO:SubProcess create_model() end ==================================
2023-07-22 01:45:57,991:INFO:Creating metrics dataframe
2023-07-22 01:45:58,003:INFO:Initializing Ridge Classifier
2023-07-22 01:45:58,003:INFO:Total runtime is 1.0309124986330669 minutes
2023-07-22 01:45:58,007:INFO:SubProcess create_model() called ==================================
2023-07-22 01:45:58,008:INFO:Initializing create_model()
2023-07-22 01:45:58,008:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:45:58,008:INFO:Checking exceptions
2023-07-22 01:45:58,008:INFO:Importing libraries
2023-07-22 01:45:58,008:INFO:Copying training dataset
2023-07-22 01:45:58,018:INFO:Defining folds
2023-07-22 01:45:58,018:INFO:Declaring metric variables
2023-07-22 01:45:58,022:INFO:Importing untrained model
2023-07-22 01:45:58,026:INFO:Ridge Classifier Imported successfully
2023-07-22 01:45:58,034:INFO:Starting cross validation
2023-07-22 01:45:58,252:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:45:59,477:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.34677e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:45:59,540:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.45334e-52): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:45:59,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.01964e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:45:59,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=9.95569e-52): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:45:59,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.13602e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:45:59,610:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:45:59,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.73803e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:45:59,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.29609e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:45:59,644:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.83629e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:45:59,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.79901e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:46:01,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:01,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:01,898:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:01,902:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:01,927:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:01,932:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:01,969:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:01,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:02,021:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:02,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:02,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:02,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:02,046:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:02,051:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:02,075:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:02,080:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:02,154:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:02,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 01:46:03,843:INFO:Calculating mean and std
2023-07-22 01:46:03,845:INFO:Creating metrics dataframe
2023-07-22 01:46:04,142:INFO:Uploading results into container
2023-07-22 01:46:04,142:INFO:Uploading model into container now
2023-07-22 01:46:04,143:INFO:_master_model_container: 6
2023-07-22 01:46:04,143:INFO:_display_container: 2
2023-07-22 01:46:04,143:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:46:04,144:INFO:create_model() successfully completed......................................
2023-07-22 01:46:04,238:INFO:SubProcess create_model() end ==================================
2023-07-22 01:46:04,240:INFO:Creating metrics dataframe
2023-07-22 01:46:04,251:INFO:Initializing Random Forest Classifier
2023-07-22 01:46:04,253:INFO:Total runtime is 1.135064446926117 minutes
2023-07-22 01:46:04,256:INFO:SubProcess create_model() called ==================================
2023-07-22 01:46:04,258:INFO:Initializing create_model()
2023-07-22 01:46:04,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:46:04,258:INFO:Checking exceptions
2023-07-22 01:46:04,258:INFO:Importing libraries
2023-07-22 01:46:04,258:INFO:Copying training dataset
2023-07-22 01:46:04,268:INFO:Defining folds
2023-07-22 01:46:04,268:INFO:Declaring metric variables
2023-07-22 01:46:04,273:INFO:Importing untrained model
2023-07-22 01:46:04,278:INFO:Random Forest Classifier Imported successfully
2023-07-22 01:46:04,287:INFO:Starting cross validation
2023-07-22 01:46:04,557:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:46:09,548:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 01:46:09,572:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 01:46:11,296:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:12,265:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:46:13,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:46:14,230:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:14,249:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:14,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:14,855:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:14,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:14,964:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:14,990:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:15,003:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:15,077:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:17,607:INFO:Calculating mean and std
2023-07-22 01:46:17,608:INFO:Creating metrics dataframe
2023-07-22 01:46:17,915:INFO:Uploading results into container
2023-07-22 01:46:17,916:INFO:Uploading model into container now
2023-07-22 01:46:17,917:INFO:_master_model_container: 7
2023-07-22 01:46:17,917:INFO:_display_container: 2
2023-07-22 01:46:17,917:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 01:46:17,917:INFO:create_model() successfully completed......................................
2023-07-22 01:46:18,012:INFO:SubProcess create_model() end ==================================
2023-07-22 01:46:18,012:INFO:Creating metrics dataframe
2023-07-22 01:46:18,023:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 01:46:18,023:INFO:Total runtime is 1.3645694533983868 minutes
2023-07-22 01:46:18,028:INFO:SubProcess create_model() called ==================================
2023-07-22 01:46:18,028:INFO:Initializing create_model()
2023-07-22 01:46:18,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:46:18,028:INFO:Checking exceptions
2023-07-22 01:46:18,028:INFO:Importing libraries
2023-07-22 01:46:18,028:INFO:Copying training dataset
2023-07-22 01:46:18,036:INFO:Defining folds
2023-07-22 01:46:18,036:INFO:Declaring metric variables
2023-07-22 01:46:18,039:INFO:Importing untrained model
2023-07-22 01:46:18,044:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 01:46:18,052:INFO:Starting cross validation
2023-07-22 01:46:18,257:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:46:19,518:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:19,542:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:19,556:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:19,577:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:19,583:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:19,588:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:19,591:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:19,603:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:19,620:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-07-22 01:46:21,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:21,938:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,938:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:21,953:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:21,955:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,955:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,956:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:21,961:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:21,963:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,963:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,963:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:21,970:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:21,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:21,986:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:21,986:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:21,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:21,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:21,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,019:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:22,021:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,021:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,022:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,036:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:22,037:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,038:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,038:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:22,095:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,095:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,096:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,678:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,678:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,679:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,680:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,681:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,681:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,683:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,685:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,691:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:22,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:22,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,700:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,703:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,706:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,706:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,707:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,711:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:22,711:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:22,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,719:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,720:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,724:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,728:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,729:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,729:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,732:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:22,733:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,744:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:22,771:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,771:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,775:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,779:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,779:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,780:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:22,784:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,794:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:22,863:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,864:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-07-22 01:46:22,864:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-07-22 01:46:22,869:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 35, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-07-22 01:46:22,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:46:24,697:INFO:Calculating mean and std
2023-07-22 01:46:24,698:INFO:Creating metrics dataframe
2023-07-22 01:46:25,006:INFO:Uploading results into container
2023-07-22 01:46:25,007:INFO:Uploading model into container now
2023-07-22 01:46:25,008:INFO:_master_model_container: 8
2023-07-22 01:46:25,008:INFO:_display_container: 2
2023-07-22 01:46:25,009:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 01:46:25,009:INFO:create_model() successfully completed......................................
2023-07-22 01:46:25,103:INFO:SubProcess create_model() end ==================================
2023-07-22 01:46:25,104:INFO:Creating metrics dataframe
2023-07-22 01:46:25,115:INFO:Initializing Ada Boost Classifier
2023-07-22 01:46:25,115:INFO:Total runtime is 1.4827680865923565 minutes
2023-07-22 01:46:25,119:INFO:SubProcess create_model() called ==================================
2023-07-22 01:46:25,120:INFO:Initializing create_model()
2023-07-22 01:46:25,120:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:46:25,120:INFO:Checking exceptions
2023-07-22 01:46:25,120:INFO:Importing libraries
2023-07-22 01:46:25,120:INFO:Copying training dataset
2023-07-22 01:46:25,128:INFO:Defining folds
2023-07-22 01:46:25,128:INFO:Declaring metric variables
2023-07-22 01:46:25,133:INFO:Importing untrained model
2023-07-22 01:46:25,137:INFO:Ada Boost Classifier Imported successfully
2023-07-22 01:46:25,146:INFO:Starting cross validation
2023-07-22 01:46:25,354:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:46:28,996:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 879, in predict_proba
    decision = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:46:30,115:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 700, in predict
    pred = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 768, in decision_function
    pred = sum(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 769, in <genexpr>
    _samme_proba(estimator, n_classes, X) for estimator in self.estimators_
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py", line 317, in _samme_proba
    proba = estimator.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 922, in predict_proba
    X = self._validate_X_predict(X, check_input)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\tree\_classes.py", line 392, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:46:31,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:31,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:31,203:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:31,227:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:31,245:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:31,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:31,273:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:31,287:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:31,308:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:33,991:INFO:Calculating mean and std
2023-07-22 01:46:33,993:INFO:Creating metrics dataframe
2023-07-22 01:46:34,292:INFO:Uploading results into container
2023-07-22 01:46:34,293:INFO:Uploading model into container now
2023-07-22 01:46:34,293:INFO:_master_model_container: 9
2023-07-22 01:46:34,293:INFO:_display_container: 2
2023-07-22 01:46:34,294:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 01:46:34,294:INFO:create_model() successfully completed......................................
2023-07-22 01:46:34,384:INFO:SubProcess create_model() end ==================================
2023-07-22 01:46:34,385:INFO:Creating metrics dataframe
2023-07-22 01:46:34,396:INFO:Initializing Gradient Boosting Classifier
2023-07-22 01:46:34,396:INFO:Total runtime is 1.6374603549639386 minutes
2023-07-22 01:46:34,401:INFO:SubProcess create_model() called ==================================
2023-07-22 01:46:34,402:INFO:Initializing create_model()
2023-07-22 01:46:34,402:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:46:34,402:INFO:Checking exceptions
2023-07-22 01:46:34,402:INFO:Importing libraries
2023-07-22 01:46:34,402:INFO:Copying training dataset
2023-07-22 01:46:34,412:INFO:Defining folds
2023-07-22 01:46:34,412:INFO:Declaring metric variables
2023-07-22 01:46:34,416:INFO:Importing untrained model
2023-07-22 01:46:34,422:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 01:46:34,432:INFO:Starting cross validation
2023-07-22 01:46:34,642:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:46:45,111:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1355, in predict_proba
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:46:46,297:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1308, in predict
    raw_predictions = self.decision_function(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_gb.py", line 1261, in decision_function
    X = self._validate_data(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:46:47,327:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:47,389:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:47,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:47,393:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:47,393:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:47,393:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:47,398:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:47,433:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:47,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:50,261:INFO:Calculating mean and std
2023-07-22 01:46:50,262:INFO:Creating metrics dataframe
2023-07-22 01:46:50,578:INFO:Uploading results into container
2023-07-22 01:46:50,579:INFO:Uploading model into container now
2023-07-22 01:46:50,580:INFO:_master_model_container: 10
2023-07-22 01:46:50,581:INFO:_display_container: 2
2023-07-22 01:46:50,581:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 01:46:50,581:INFO:create_model() successfully completed......................................
2023-07-22 01:46:50,672:INFO:SubProcess create_model() end ==================================
2023-07-22 01:46:50,672:INFO:Creating metrics dataframe
2023-07-22 01:46:50,685:INFO:Initializing Linear Discriminant Analysis
2023-07-22 01:46:50,685:INFO:Total runtime is 1.9089343349138899 minutes
2023-07-22 01:46:50,689:INFO:SubProcess create_model() called ==================================
2023-07-22 01:46:50,689:INFO:Initializing create_model()
2023-07-22 01:46:50,689:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:46:50,690:INFO:Checking exceptions
2023-07-22 01:46:50,690:INFO:Importing libraries
2023-07-22 01:46:50,690:INFO:Copying training dataset
2023-07-22 01:46:50,697:INFO:Defining folds
2023-07-22 01:46:50,697:INFO:Declaring metric variables
2023-07-22 01:46:50,702:INFO:Importing untrained model
2023-07-22 01:46:50,707:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 01:46:50,714:INFO:Starting cross validation
2023-07-22 01:46:50,916:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:46:54,499:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:54,508:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:54,528:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:54,530:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:54,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:54,540:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:54,547:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:54,553:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:54,606:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:46:57,310:INFO:Calculating mean and std
2023-07-22 01:46:57,311:INFO:Creating metrics dataframe
2023-07-22 01:46:57,630:INFO:Uploading results into container
2023-07-22 01:46:57,631:INFO:Uploading model into container now
2023-07-22 01:46:57,632:INFO:_master_model_container: 11
2023-07-22 01:46:57,632:INFO:_display_container: 2
2023-07-22 01:46:57,632:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 01:46:57,632:INFO:create_model() successfully completed......................................
2023-07-22 01:46:57,723:INFO:SubProcess create_model() end ==================================
2023-07-22 01:46:57,723:INFO:Creating metrics dataframe
2023-07-22 01:46:57,735:INFO:Initializing Extra Trees Classifier
2023-07-22 01:46:57,735:INFO:Total runtime is 2.0264334718386334 minutes
2023-07-22 01:46:57,739:INFO:SubProcess create_model() called ==================================
2023-07-22 01:46:57,740:INFO:Initializing create_model()
2023-07-22 01:46:57,740:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:46:57,740:INFO:Checking exceptions
2023-07-22 01:46:57,740:INFO:Importing libraries
2023-07-22 01:46:57,740:INFO:Copying training dataset
2023-07-22 01:46:57,748:INFO:Defining folds
2023-07-22 01:46:57,748:INFO:Declaring metric variables
2023-07-22 01:46:57,752:INFO:Importing untrained model
2023-07-22 01:46:57,757:INFO:Extra Trees Classifier Imported successfully
2023-07-22 01:46:57,763:INFO:Starting cross validation
2023-07-22 01:46:57,964:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:47:01,151:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 01:47:01,334:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.47s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:01,589:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 321, in predict_proba
    return self.steps[-1][-1].predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:47:02,666:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\model_selection\_validation.py:794: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 115, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 276, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 312, in predict
    y = self.steps[-1][-1].predict(X, **predict_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 820, in predict
    proba = self.predict_proba(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 862, in predict_proba
    X = self._validate_X_predict(X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\ensemble\_forest.py", line 602, in _validate_X_predict
    X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\base.py", line 565, in _validate_data
    X = check_array(X, input_name="X", **check_params)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input X contains infinity or a value too large for dtype('float32').

  warnings.warn(

2023-07-22 01:47:03,373:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:03,448:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:03,582:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:04,528:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:04,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:06,071:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:06,333:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:06,420:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:06,674:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:07,779:INFO:Calculating mean and std
2023-07-22 01:47:07,780:INFO:Creating metrics dataframe
2023-07-22 01:47:08,110:INFO:Uploading results into container
2023-07-22 01:47:08,111:INFO:Uploading model into container now
2023-07-22 01:47:08,111:INFO:_master_model_container: 12
2023-07-22 01:47:08,112:INFO:_display_container: 2
2023-07-22 01:47:08,112:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 01:47:08,112:INFO:create_model() successfully completed......................................
2023-07-22 01:47:08,204:INFO:SubProcess create_model() end ==================================
2023-07-22 01:47:08,204:INFO:Creating metrics dataframe
2023-07-22 01:47:08,218:INFO:Initializing Extreme Gradient Boosting
2023-07-22 01:47:08,218:INFO:Total runtime is 2.2011508742968244 minutes
2023-07-22 01:47:08,221:INFO:SubProcess create_model() called ==================================
2023-07-22 01:47:08,222:INFO:Initializing create_model()
2023-07-22 01:47:08,222:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:47:08,222:INFO:Checking exceptions
2023-07-22 01:47:08,222:INFO:Importing libraries
2023-07-22 01:47:08,222:INFO:Copying training dataset
2023-07-22 01:47:08,229:INFO:Defining folds
2023-07-22 01:47:08,229:INFO:Declaring metric variables
2023-07-22 01:47:08,233:INFO:Importing untrained model
2023-07-22 01:47:08,238:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 01:47:08,246:INFO:Starting cross validation
2023-07-22 01:47:08,449:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:47:17,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:17,717:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:17,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:17,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:17,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:17,864:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:17,910:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:17,956:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:17,980:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:20,762:INFO:Calculating mean and std
2023-07-22 01:47:20,763:INFO:Creating metrics dataframe
2023-07-22 01:47:21,110:INFO:Uploading results into container
2023-07-22 01:47:21,111:INFO:Uploading model into container now
2023-07-22 01:47:21,112:INFO:_master_model_container: 13
2023-07-22 01:47:21,112:INFO:_display_container: 2
2023-07-22 01:47:21,113:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 01:47:21,114:INFO:create_model() successfully completed......................................
2023-07-22 01:47:21,213:INFO:SubProcess create_model() end ==================================
2023-07-22 01:47:21,213:INFO:Creating metrics dataframe
2023-07-22 01:47:21,223:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 01:47:21,223:INFO:Total runtime is 2.417907257874807 minutes
2023-07-22 01:47:21,227:INFO:SubProcess create_model() called ==================================
2023-07-22 01:47:21,227:INFO:Initializing create_model()
2023-07-22 01:47:21,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:47:21,227:INFO:Checking exceptions
2023-07-22 01:47:21,229:INFO:Importing libraries
2023-07-22 01:47:21,229:INFO:Copying training dataset
2023-07-22 01:47:21,236:INFO:Defining folds
2023-07-22 01:47:21,237:INFO:Declaring metric variables
2023-07-22 01:47:21,241:INFO:Importing untrained model
2023-07-22 01:47:21,245:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:47:21,253:INFO:Starting cross validation
2023-07-22 01:47:21,462:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:47:25,925:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:25,944:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:25,952:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:25,958:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:26,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:26,080:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:26,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:26,110:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:26,111:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:29,124:INFO:Calculating mean and std
2023-07-22 01:47:29,125:INFO:Creating metrics dataframe
2023-07-22 01:47:29,475:INFO:Uploading results into container
2023-07-22 01:47:29,476:INFO:Uploading model into container now
2023-07-22 01:47:29,476:INFO:_master_model_container: 14
2023-07-22 01:47:29,477:INFO:_display_container: 2
2023-07-22 01:47:29,477:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:47:29,478:INFO:create_model() successfully completed......................................
2023-07-22 01:47:29,573:INFO:SubProcess create_model() end ==================================
2023-07-22 01:47:29,573:INFO:Creating metrics dataframe
2023-07-22 01:47:29,586:INFO:Initializing Dummy Classifier
2023-07-22 01:47:29,586:INFO:Total runtime is 2.5572876930236816 minutes
2023-07-22 01:47:29,590:INFO:SubProcess create_model() called ==================================
2023-07-22 01:47:29,590:INFO:Initializing create_model()
2023-07-22 01:47:29,590:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001CD79A6D2A0>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:47:29,591:INFO:Checking exceptions
2023-07-22 01:47:29,591:INFO:Importing libraries
2023-07-22 01:47:29,591:INFO:Copying training dataset
2023-07-22 01:47:29,599:INFO:Defining folds
2023-07-22 01:47:29,600:INFO:Declaring metric variables
2023-07-22 01:47:29,603:INFO:Importing untrained model
2023-07-22 01:47:29,607:INFO:Dummy Classifier Imported successfully
2023-07-22 01:47:29,615:INFO:Starting cross validation
2023-07-22 01:47:29,824:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:47:31,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:33,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:33,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:33,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:33,904:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:33,937:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:33,958:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:34,046:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:34,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:34,379:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:47:34,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:34,687:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:34,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:34,778:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:34,823:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:34,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:34,890:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:35,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:35,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 01:47:37,350:INFO:Calculating mean and std
2023-07-22 01:47:37,352:INFO:Creating metrics dataframe
2023-07-22 01:47:37,788:INFO:Uploading results into container
2023-07-22 01:47:37,789:INFO:Uploading model into container now
2023-07-22 01:47:37,789:INFO:_master_model_container: 15
2023-07-22 01:47:37,790:INFO:_display_container: 2
2023-07-22 01:47:37,790:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 01:47:37,790:INFO:create_model() successfully completed......................................
2023-07-22 01:47:37,887:INFO:SubProcess create_model() end ==================================
2023-07-22 01:47:37,888:INFO:Creating metrics dataframe
2023-07-22 01:47:37,912:INFO:Initializing create_model()
2023-07-22 01:47:37,912:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:47:37,912:INFO:Checking exceptions
2023-07-22 01:47:37,914:INFO:Importing libraries
2023-07-22 01:47:37,914:INFO:Copying training dataset
2023-07-22 01:47:37,922:INFO:Defining folds
2023-07-22 01:47:37,922:INFO:Declaring metric variables
2023-07-22 01:47:37,922:INFO:Importing untrained model
2023-07-22 01:47:37,922:INFO:Declaring custom model
2023-07-22 01:47:37,923:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:47:38,156:INFO:Cross validation set to False
2023-07-22 01:47:38,156:INFO:Fitting Model
2023-07-22 01:47:39,304:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:47:39,304:INFO:create_model() successfully completed......................................
2023-07-22 01:47:39,410:INFO:Initializing create_model()
2023-07-22 01:47:39,410:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:47:39,410:INFO:Checking exceptions
2023-07-22 01:47:39,412:INFO:Importing libraries
2023-07-22 01:47:39,413:INFO:Copying training dataset
2023-07-22 01:47:39,420:INFO:Defining folds
2023-07-22 01:47:39,420:INFO:Declaring metric variables
2023-07-22 01:47:39,420:INFO:Importing untrained model
2023-07-22 01:47:39,420:INFO:Declaring custom model
2023-07-22 01:47:39,422:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 01:47:39,637:INFO:Cross validation set to False
2023-07-22 01:47:39,637:INFO:Fitting Model
2023-07-22 01:47:41,487:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 01:47:41,487:INFO:create_model() successfully completed......................................
2023-07-22 01:47:41,585:INFO:Initializing create_model()
2023-07-22 01:47:41,586:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:47:41,586:INFO:Checking exceptions
2023-07-22 01:47:41,588:INFO:Importing libraries
2023-07-22 01:47:41,588:INFO:Copying training dataset
2023-07-22 01:47:41,594:INFO:Defining folds
2023-07-22 01:47:41,594:INFO:Declaring metric variables
2023-07-22 01:47:41,595:INFO:Importing untrained model
2023-07-22 01:47:41,595:INFO:Declaring custom model
2023-07-22 01:47:41,595:INFO:Ridge Classifier Imported successfully
2023-07-22 01:47:41,802:INFO:Cross validation set to False
2023-07-22 01:47:41,802:INFO:Fitting Model
2023-07-22 01:47:42,281:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.42454e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 01:47:42,545:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 01:47:42,545:INFO:create_model() successfully completed......................................
2023-07-22 01:47:42,675:INFO:_master_model_container: 15
2023-07-22 01:47:42,675:INFO:_display_container: 2
2023-07-22 01:47:42,676:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...), RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)]
2023-07-22 01:47:42,677:INFO:compare_models() successfully completed......................................
2023-07-22 01:47:47,301:INFO:Initializing create_model()
2023-07-22 01:47:47,301:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 01:47:47,301:INFO:Checking exceptions
2023-07-22 01:47:47,317:INFO:Importing libraries
2023-07-22 01:47:47,317:INFO:Copying training dataset
2023-07-22 01:47:47,324:INFO:Defining folds
2023-07-22 01:47:47,324:INFO:Declaring metric variables
2023-07-22 01:47:47,329:INFO:Importing untrained model
2023-07-22 01:47:47,334:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 01:47:47,344:INFO:Starting cross validation
2023-07-22 01:47:47,613:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 01:47:53,030:INFO:Calculating mean and std
2023-07-22 01:47:53,032:INFO:Creating metrics dataframe
2023-07-22 01:47:53,039:INFO:Finalizing model
2023-07-22 01:47:53,949:INFO:Uploading results into container
2023-07-22 01:47:53,950:INFO:Uploading model into container now
2023-07-22 01:47:53,961:INFO:_master_model_container: 16
2023-07-22 01:47:53,962:INFO:_display_container: 3
2023-07-22 01:47:53,962:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 01:47:53,962:INFO:create_model() successfully completed......................................
2023-07-22 01:48:06,665:INFO:Initializing plot_model()
2023-07-22 01:48:06,666:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, system=True)
2023-07-22 01:48:06,666:INFO:Checking exceptions
2023-07-22 01:48:06,673:INFO:Preloading libraries
2023-07-22 01:48:06,680:INFO:Copying training dataset
2023-07-22 01:48:06,680:INFO:Plot type: confusion_matrix
2023-07-22 01:48:07,660:INFO:Fitting Model
2023-07-22 01:48:07,661:INFO:Scoring test/hold-out set
2023-07-22 01:48:07,852:INFO:Visual Rendered Successfully
2023-07-22 01:48:07,976:INFO:plot_model() successfully completed......................................
2023-07-22 01:49:16,189:INFO:Initializing plot_model()
2023-07-22 01:49:16,189:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, system=True)
2023-07-22 01:49:16,189:INFO:Checking exceptions
2023-07-22 01:49:16,195:INFO:Preloading libraries
2023-07-22 01:49:16,202:INFO:Copying training dataset
2023-07-22 01:49:16,202:INFO:Plot type: auc
2023-07-22 01:49:17,192:INFO:Fitting Model
2023-07-22 01:49:17,193:INFO:Scoring test/hold-out set
2023-07-22 01:49:17,243:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\generic.py:831: DeprecationWarning: invalid escape sequence '\P'
  """

2023-07-22 01:49:30,459:INFO:Initializing plot_model()
2023-07-22 01:49:30,459:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, system=True)
2023-07-22 01:49:30,460:INFO:Checking exceptions
2023-07-22 01:49:30,466:INFO:Preloading libraries
2023-07-22 01:49:30,471:INFO:Copying training dataset
2023-07-22 01:49:30,471:INFO:Plot type: auc
2023-07-22 01:49:31,377:INFO:Fitting Model
2023-07-22 01:49:31,379:INFO:Scoring test/hold-out set
2023-07-22 01:49:53,130:INFO:Initializing plot_model()
2023-07-22 01:49:53,130:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>, system=True)
2023-07-22 01:49:53,131:INFO:Checking exceptions
2023-07-22 01:49:53,136:INFO:Preloading libraries
2023-07-22 01:49:53,143:INFO:Copying training dataset
2023-07-22 01:49:53,143:INFO:Plot type: feature
2023-07-22 01:49:53,143:WARNING:No coef_ found. Trying feature_importances_
2023-07-22 01:49:53,512:INFO:Visual Rendered Successfully
2023-07-22 01:49:53,641:INFO:plot_model() successfully completed......................................
2023-07-22 01:50:47,186:INFO:Initializing tune_model()
2023-07-22 01:50:47,186:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=300, custom_grid=None, optimize=F1, custom_scorer=None, search_library=optuna, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CD026C9EA0>)
2023-07-22 01:50:47,186:INFO:Checking exceptions
2023-07-22 01:50:47,186:INFO:Soft dependency imported: optuna: 3.2.0
2023-07-22 01:50:47,210:INFO:Copying training dataset
2023-07-22 01:50:47,216:INFO:Checking base model
2023-07-22 01:50:47,216:INFO:Base model : Light Gradient Boosting Machine
2023-07-22 01:50:47,221:INFO:Declaring metric variables
2023-07-22 01:50:47,227:INFO:Defining Hyperparameters
2023-07-22 01:50:47,639:INFO:Tuning with n_jobs=-1
2023-07-22 01:50:47,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\optuna\samplers\_tpe\sampler.py:278: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.
  warnings.warn(

2023-07-22 01:50:47,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\optuna\samplers\_tpe\sampler.py:297: ExperimentalWarning: ``constant_liar`` option is an experimental feature. The interface can change in the future.
  warnings.warn(

2023-07-22 01:50:47,641:INFO:Initializing optuna.integration.OptunaSearchCV
2023-07-22 01:50:47,648:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:2441: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.
  model_grid = optuna.integration.OptunaSearchCV(  # type: ignore

2023-07-22 01:50:53,074:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:50:54,321:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:50:54,395:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:50:55,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:50:56,327:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:50:58,013:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:50:59,197:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:50:59,990:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:00,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:01,469:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:01,672:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:03,678:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:03,915:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:05,132:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:05,390:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:07,543:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:08,432:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:08,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:15,003:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:15,473:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:16,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:16,927:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:17,043:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:17,152:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:18,661:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:18,689:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:18,871:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:21,594:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:26,445:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:26,619:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:26,763:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:29,357:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:29,895:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:31,924:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:32,310:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:33,296:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:34,274:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:34,385:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:34,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:35,607:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:37,364:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:37,514:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:37,601:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:39,400:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:41,293:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:41,536:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:42,036:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:44,140:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:44,428:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:51,320:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:52,421:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:52,672:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:53,065:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:51:54,378:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:00,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:02,948:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:03,109:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:04,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:08,307:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:09,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:12,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:21,640:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:25,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:27,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:27,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:28,851:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:29,961:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:30,020:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:33,370:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:35,703:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:36,279:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:36,345:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:36,742:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:37,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:37,730:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:40,182:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:41,761:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:44,559:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:47,006:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:48,317:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:48,417:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:48,651:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:49,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:52,051:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:53,484:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:55,274:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:56,474:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:56,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:52:59,996:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:04,644:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:04,934:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:05,378:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:10,300:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:11,054:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:13,635:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:16,035:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:16,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:17,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:17,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:17,850:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:18,914:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:22,599:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:22,750:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:29,817:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:32,454:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:33,982:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:34,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:34,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:35,413:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:41,854:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:42,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:43,409:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:44,323:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:44,525:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:45,590:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:46,950:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:47,999:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:48,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:49,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:51,740:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:53,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:54,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:53:57,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:00,117:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:00,408:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:00,509:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:03,493:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:04,714:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:04,919:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:07,046:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:07,202:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:07,267:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:11,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:11,139:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:13,171:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:13,494:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:13,765:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:15,110:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:16,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:17,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:18,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:21,714:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:23,300:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:24,143:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:24,293:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:26,679:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:26,738:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:26,837:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:29,493:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:30,806:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:31,574:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:32,449:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:32,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:32,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:36,367:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:40,551:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:42,698:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:50,882:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:51,032:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:51,170:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:51,188:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:55,877:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:54:58,308:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:00,207:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:03,284:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:03,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:04,398:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:05,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:06,945:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:10,543:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:11,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:11,497:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:12,660:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:12,806:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:13,894:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:14,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:15,163:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:15,307:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:19,140:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:26,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:28,822:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:29,221:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:30,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:30,663:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:30,775:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:35,847:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:40,444:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:41,569:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:44,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:44,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:46,422:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:47,348:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:49,225:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:49,813:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:52,740:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:53,637:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:54,530:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:56,907:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:58,923:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:55:59,965:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:01,493:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:03,261:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:03,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:04,157:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:04,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:05,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:07,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:09,102:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:14,578:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:16,960:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:16,974:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:21,407:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:22,579:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:22,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:27,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:32,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:32,584:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:32,607:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:34,780:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:34,960:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:34,978:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:36,003:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:36,118:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:37,502:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:37,577:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:40,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:40,779:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:43,311:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:43,698:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:44,523:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:49,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:50,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:50,330:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:50,458:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:53,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:53,971:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:56:58,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:02,697:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:07,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:08,137:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:09,169:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:09,889:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:13,300:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:13,600:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:17,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:21,419:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:22,806:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:24,259:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:25,211:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:25,480:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:26,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:27,963:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:29,201:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:31,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:35,968:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:36,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:37,762:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:39,715:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:40,145:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:41,112:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:41,224:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:41,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:45,848:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:46,097:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:55,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:57:58,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:00,184:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:06,477:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:07,501:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:17,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:17,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:20,967:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:21,667:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:21,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:21,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:28,666:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:29,785:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:35,022:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:37,472:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:39,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:40,314:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:45,047:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:48,214:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:48,474:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:48,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:50,054:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:50,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:51,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:54,320:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:54,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:58,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:58:58,689:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:00,956:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:02,476:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:04,008:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:04,487:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:08,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:14,961:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:16,770:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:28,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:38,788:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:44,781:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:47,597:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:52,515:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:52,969:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:56,941:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 01:59:58,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:00,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:01,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:01,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:03,076:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:03,474:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:05,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:05,751:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:06,592:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:07,005:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:07,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:09,569:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:16,591:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:16,951:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:19,086:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:19,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:24,767:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:30,060:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:32,436:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:35,134:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:35,142:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:37,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:38,015:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:45,046:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:48,456:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:48,691:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:50,597:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:52,115:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:52,770:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:54,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:54,944:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:55,110:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:00:57,604:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:09,421:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:09,460:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:12,005:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:12,182:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:12,383:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:14,835:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:16,951:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:19,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:19,747:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:24,172:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:26,432:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:26,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:26,765:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:27,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:28,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:29,019:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:30,659:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:30,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:31,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:37,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:38,579:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:39,756:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:41,118:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:41,267:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:42,201:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:48,410:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:53,325:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:53,592:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:55,334:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:55,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:56,498:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:01:57,900:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:00,173:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:00,443:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:02,458:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:07,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:07,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:08,855:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:11,107:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:12,174:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:13,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:15,377:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:25,869:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:30,132:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:30,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:34,405:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:40,682:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:45,653:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:45,802:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:45,903:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:50,838:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:51,266:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:54,584:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:56,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:56,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:57,735:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:02:58,516:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:00,771:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:01,397:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:12,823:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:26,433:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:26,750:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:27,121:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:35,605:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:39,046:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:40,091:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:44,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:47,492:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:47,560:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:47,690:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:49,423:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:51,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:52,347:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:52,448:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:52,525:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:55,411:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:56,260:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:58,638:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:03:58,925:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:00,101:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:00,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:04,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:13,579:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:14,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:14,835:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:14,904:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:19,305:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:33,613:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:43,249:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:44,317:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:44,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:52,362:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:57,725:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:04:57,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:02,511:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:06,914:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:08,723:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:11,106:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:15,241:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:15,608:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:20,553:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:21,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:21,737:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:23,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:24,660:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:25,542:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:25,659:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:27,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:28,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:30,122:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:33,135:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:35,987:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:37,535:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:39,183:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:39,824:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:41,433:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:42,311:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:48,005:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:05:48,101:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:10,932:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:16,959:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:26,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:29,650:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:30,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:30,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:34,873:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:43,220:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:46,132:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:47,218:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:51,959:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:52,207:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:54,176:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:54,329:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:06:57,068:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:02,087:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:02,178:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:03,519:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:07,173:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:07,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:10,171:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:11,741:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:11,742:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:12,895:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:15,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:16,301:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:18,217:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:18,842:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:25,540:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:26,828:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:26,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:30,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:34,388:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:35,722:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:37,190:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:37,595:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:39,779:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:53,767:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:07:54,284:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:04,954:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:04,957:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:05,528:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:06,671:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:06,830:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:11,599:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:19,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:19,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:28,043:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:50,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:50,220:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:08:58,455:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:04,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:04,841:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:07,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:08,678:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:10,569:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:18,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:18,396:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:23,855:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:24,096:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:39,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:40,163:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:56,180:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:56,600:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:09:58,143:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:03,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:04,399:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:05,610:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:10,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:15,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:16,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:17,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:18,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:22,805:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:22,881:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:25,495:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:26,076:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:26,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:26,357:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:36,324:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:38,789:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:40,036:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:40,353:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:40,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:41,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:43,752:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:44,844:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:45,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:46,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:47,036:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:47,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:50,281:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:54,852:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:55,870:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:58,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:59,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:10:59,891:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:03,047:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:12,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:17,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:18,386:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:19,109:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:19,374:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:31,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:34,610:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:46,711:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:11:59,923:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:02,701:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:02,706:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:25,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:28,370:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:28,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:30,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:33,226:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:36,312:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:36,845:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:36,984:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:37,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:41,499:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:41,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:43,336:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:51,384:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:53,013:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:55,180:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:55,975:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:56,402:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:12:57,295:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:13:01,002:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:13:14,033:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:45:42,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 02:45:42,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 02:45:42,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 02:45:42,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 02:45:50,729:INFO:PyCaret ClassificationExperiment
2023-07-22 02:45:50,729:INFO:Logging name: clf-default-name
2023-07-22 02:45:50,729:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 02:45:50,729:INFO:version 3.0.4
2023-07-22 02:45:50,729:INFO:Initializing setup()
2023-07-22 02:45:50,729:INFO:self.USI: 0436
2023-07-22 02:45:50,729:INFO:self._variable_keys: {'logging_param', 'gpu_n_jobs_param', 'idx', '_available_plots', 'data', 'exp_id', 'gpu_param', 'exp_name_log', 'memory', 'X', 'fold_groups_param', 'y_test', 'is_multiclass', 'target_param', 'log_plots_param', 'html_param', 'pipeline', 'USI', 'fix_imbalance', 'X_train', '_ml_usecase', 'fold_shuffle_param', 'y', 'n_jobs_param', 'X_test', 'seed', 'y_train', 'fold_generator'}
2023-07-22 02:45:50,729:INFO:Checking environment
2023-07-22 02:45:50,729:INFO:python_version: 3.10.8
2023-07-22 02:45:50,729:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 02:45:50,729:INFO:machine: AMD64
2023-07-22 02:45:50,730:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 02:45:50,735:INFO:Memory: svmem(total=16505966592, available=5071147008, percent=69.3, used=11434819584, free=5071147008)
2023-07-22 02:45:50,735:INFO:Physical Core: 6
2023-07-22 02:45:50,735:INFO:Logical Core: 12
2023-07-22 02:45:50,735:INFO:Checking libraries
2023-07-22 02:45:50,735:INFO:System:
2023-07-22 02:45:50,735:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 02:45:50,735:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 02:45:50,735:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 02:45:50,735:INFO:PyCaret required dependencies:
2023-07-22 02:45:50,739:INFO:                 pip: 22.2.2
2023-07-22 02:45:50,739:INFO:          setuptools: 63.2.0
2023-07-22 02:45:50,739:INFO:             pycaret: 3.0.4
2023-07-22 02:45:50,739:INFO:             IPython: 8.11.0
2023-07-22 02:45:50,739:INFO:          ipywidgets: 8.0.7
2023-07-22 02:45:50,739:INFO:                tqdm: 4.64.1
2023-07-22 02:45:50,739:INFO:               numpy: 1.23.5
2023-07-22 02:45:50,739:INFO:              pandas: 1.5.3
2023-07-22 02:45:50,739:INFO:              jinja2: 3.1.2
2023-07-22 02:45:50,739:INFO:               scipy: 1.9.3
2023-07-22 02:45:50,739:INFO:              joblib: 1.2.0
2023-07-22 02:45:50,740:INFO:             sklearn: 1.2.2
2023-07-22 02:45:50,740:INFO:                pyod: 1.1.0
2023-07-22 02:45:50,740:INFO:            imblearn: 0.10.1
2023-07-22 02:45:50,740:INFO:   category_encoders: 2.6.1
2023-07-22 02:45:50,740:INFO:            lightgbm: 3.3.5
2023-07-22 02:45:50,740:INFO:               numba: 0.57.0
2023-07-22 02:45:50,740:INFO:            requests: 2.28.2
2023-07-22 02:45:50,740:INFO:          matplotlib: 3.7.1
2023-07-22 02:45:50,740:INFO:          scikitplot: 0.3.7
2023-07-22 02:45:50,740:INFO:         yellowbrick: 1.5
2023-07-22 02:45:50,740:INFO:              plotly: 5.15.0
2023-07-22 02:45:50,740:INFO:    plotly-resampler: Not installed
2023-07-22 02:45:50,740:INFO:             kaleido: 0.2.1
2023-07-22 02:45:50,740:INFO:           schemdraw: 0.15
2023-07-22 02:45:50,740:INFO:         statsmodels: 0.13.5
2023-07-22 02:45:50,740:INFO:              sktime: 0.21.0
2023-07-22 02:45:50,740:INFO:               tbats: 1.1.3
2023-07-22 02:45:50,740:INFO:            pmdarima: 2.0.3
2023-07-22 02:45:50,740:INFO:              psutil: 5.9.4
2023-07-22 02:45:50,740:INFO:          markupsafe: 2.1.2
2023-07-22 02:45:50,740:INFO:             pickle5: Not installed
2023-07-22 02:45:50,740:INFO:         cloudpickle: 2.2.1
2023-07-22 02:45:50,740:INFO:         deprecation: 2.1.0
2023-07-22 02:45:50,740:INFO:              xxhash: 3.2.0
2023-07-22 02:45:50,740:INFO:           wurlitzer: Not installed
2023-07-22 02:45:50,740:INFO:PyCaret optional dependencies:
2023-07-22 02:45:51,213:INFO:                shap: 0.41.0
2023-07-22 02:45:51,213:INFO:           interpret: 0.4.2
2023-07-22 02:45:51,213:INFO:                umap: 0.5.3
2023-07-22 02:45:51,213:INFO:    pandas_profiling: 4.1.2
2023-07-22 02:45:51,213:INFO:  explainerdashboard: Not installed
2023-07-22 02:45:51,213:INFO:             autoviz: Not installed
2023-07-22 02:45:51,213:INFO:           fairlearn: Not installed
2023-07-22 02:45:51,213:INFO:          deepchecks: Not installed
2023-07-22 02:45:51,213:INFO:             xgboost: 1.7.6
2023-07-22 02:45:51,213:INFO:            catboost: Not installed
2023-07-22 02:45:51,213:INFO:              kmodes: Not installed
2023-07-22 02:45:51,213:INFO:             mlxtend: Not installed
2023-07-22 02:45:51,213:INFO:       statsforecast: Not installed
2023-07-22 02:45:51,213:INFO:        tune_sklearn: Not installed
2023-07-22 02:45:51,213:INFO:                 ray: Not installed
2023-07-22 02:45:51,213:INFO:            hyperopt: Not installed
2023-07-22 02:45:51,213:INFO:              optuna: 3.2.0
2023-07-22 02:45:51,213:INFO:               skopt: Not installed
2023-07-22 02:45:51,213:INFO:              mlflow: 2.4.2
2023-07-22 02:45:51,213:INFO:              gradio: Not installed
2023-07-22 02:45:51,213:INFO:             fastapi: 0.95.2
2023-07-22 02:45:51,213:INFO:             uvicorn: 0.22.0
2023-07-22 02:45:51,213:INFO:              m2cgen: Not installed
2023-07-22 02:45:51,214:INFO:           evidently: Not installed
2023-07-22 02:45:51,214:INFO:               fugue: Not installed
2023-07-22 02:45:51,214:INFO:           streamlit: Not installed
2023-07-22 02:45:51,214:INFO:             prophet: Not installed
2023-07-22 02:45:51,214:INFO:None
2023-07-22 02:45:51,214:INFO:Set up data.
2023-07-22 02:45:51,235:INFO:Set up train/test split.
2023-07-22 02:45:51,235:INFO:Set up data.
2023-07-22 02:45:51,252:INFO:Set up index.
2023-07-22 02:45:51,253:INFO:Set up folding strategy.
2023-07-22 02:45:51,253:INFO:Assigning column types.
2023-07-22 02:45:51,258:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 02:45:51,305:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,309:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,347:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:51,350:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:51,398:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,398:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,428:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:51,432:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:51,432:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 02:45:51,482:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,514:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:51,516:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:51,563:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,593:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:51,596:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:51,596:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 02:45:51,671:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:51,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:51,748:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:51,752:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:51,753:INFO:Preparing preprocessing pipeline...
2023-07-22 02:45:51,755:INFO:Set up iterative imputation.
2023-07-22 02:45:51,755:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,759:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,764:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,849:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 02:45:51,895:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:51,898:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:51,974:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:51,977:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:52,011:INFO:Set up encoding of categorical features.
2023-07-22 02:45:52,011:INFO:Set up imbalanced handling.
2023-07-22 02:45:52,011:INFO:Set up feature selection.
2023-07-22 02:45:52,089:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:52,092:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:52,848:INFO:Finished creating preprocessing pipeline.
2023-07-22 02:45:52,876:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 02:45:52,876:INFO:Creating final display dataframe.
2023-07-22 02:45:54,730:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             36.1%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              0436
2023-07-22 02:45:54,837:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:54,839:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:54,922:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 02:45:54,925:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 02:45:54,926:INFO:setup() successfully completed in 5.43s...............
2023-07-22 02:46:02,284:INFO:Initializing compare_models()
2023-07-22 02:46:02,284:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 02:46:02,284:INFO:Checking exceptions
2023-07-22 02:46:02,291:INFO:Preparing display monitor
2023-07-22 02:46:02,318:INFO:Initializing Logistic Regression
2023-07-22 02:46:02,319:INFO:Total runtime is 1.6641616821289062e-05 minutes
2023-07-22 02:46:02,323:INFO:SubProcess create_model() called ==================================
2023-07-22 02:46:02,324:INFO:Initializing create_model()
2023-07-22 02:46:02,324:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:46:02,324:INFO:Checking exceptions
2023-07-22 02:46:02,324:INFO:Importing libraries
2023-07-22 02:46:02,324:INFO:Copying training dataset
2023-07-22 02:46:02,331:INFO:Defining folds
2023-07-22 02:46:02,331:INFO:Declaring metric variables
2023-07-22 02:46:02,336:INFO:Importing untrained model
2023-07-22 02:46:02,340:INFO:Logistic Regression Imported successfully
2023-07-22 02:46:02,350:INFO:Starting cross validation
2023-07-22 02:46:02,572:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:46:11,287:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:11,343:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:11,362:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:11,384:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:11,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:11,439:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:11,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:11,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:11,460:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:46:13,826:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:13,862:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:13,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:13,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:14,010:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:14,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:14,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:14,167:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:14,172:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:14,542:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:14,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:14,577:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:14,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:14,770:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:14,863:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:14,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:14,925:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:14,927:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:46:24,221:INFO:Calculating mean and std
2023-07-22 02:46:24,222:INFO:Creating metrics dataframe
2023-07-22 02:46:25,902:INFO:Uploading results into container
2023-07-22 02:46:25,903:INFO:Uploading model into container now
2023-07-22 02:46:25,903:INFO:_master_model_container: 1
2023-07-22 02:46:25,903:INFO:_display_container: 2
2023-07-22 02:46:25,904:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 02:46:25,904:INFO:create_model() successfully completed......................................
2023-07-22 02:46:26,068:INFO:SubProcess create_model() end ==================================
2023-07-22 02:46:26,068:INFO:Creating metrics dataframe
2023-07-22 02:46:26,081:INFO:Initializing K Neighbors Classifier
2023-07-22 02:46:26,081:INFO:Total runtime is 0.3960440556208293 minutes
2023-07-22 02:46:26,084:INFO:SubProcess create_model() called ==================================
2023-07-22 02:46:26,085:INFO:Initializing create_model()
2023-07-22 02:46:26,085:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:46:26,085:INFO:Checking exceptions
2023-07-22 02:46:26,085:INFO:Importing libraries
2023-07-22 02:46:26,085:INFO:Copying training dataset
2023-07-22 02:46:26,096:INFO:Defining folds
2023-07-22 02:46:26,096:INFO:Declaring metric variables
2023-07-22 02:46:26,102:INFO:Importing untrained model
2023-07-22 02:46:26,108:INFO:K Neighbors Classifier Imported successfully
2023-07-22 02:46:26,117:INFO:Starting cross validation
2023-07-22 02:46:26,363:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:46:30,250:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:30,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:30,267:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:30,298:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:30,394:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:30,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:30,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:34,500:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:34,579:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:42,214:INFO:Calculating mean and std
2023-07-22 02:46:42,216:INFO:Creating metrics dataframe
2023-07-22 02:46:44,038:INFO:Uploading results into container
2023-07-22 02:46:44,039:INFO:Uploading model into container now
2023-07-22 02:46:44,040:INFO:_master_model_container: 2
2023-07-22 02:46:44,040:INFO:_display_container: 2
2023-07-22 02:46:44,040:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 02:46:44,041:INFO:create_model() successfully completed......................................
2023-07-22 02:46:44,189:INFO:SubProcess create_model() end ==================================
2023-07-22 02:46:44,190:INFO:Creating metrics dataframe
2023-07-22 02:46:44,200:INFO:Initializing Naive Bayes
2023-07-22 02:46:44,200:INFO:Total runtime is 0.6980351050694784 minutes
2023-07-22 02:46:44,204:INFO:SubProcess create_model() called ==================================
2023-07-22 02:46:44,205:INFO:Initializing create_model()
2023-07-22 02:46:44,205:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:46:44,205:INFO:Checking exceptions
2023-07-22 02:46:44,206:INFO:Importing libraries
2023-07-22 02:46:44,206:INFO:Copying training dataset
2023-07-22 02:46:44,216:INFO:Defining folds
2023-07-22 02:46:44,216:INFO:Declaring metric variables
2023-07-22 02:46:44,221:INFO:Importing untrained model
2023-07-22 02:46:44,226:INFO:Naive Bayes Imported successfully
2023-07-22 02:46:44,234:INFO:Starting cross validation
2023-07-22 02:46:44,461:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:46:48,069:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:48,094:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:48,196:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:48,211:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:48,224:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:48,267:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:48,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:48,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:48,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:46:59,167:INFO:Calculating mean and std
2023-07-22 02:46:59,169:INFO:Creating metrics dataframe
2023-07-22 02:47:00,855:INFO:Uploading results into container
2023-07-22 02:47:00,856:INFO:Uploading model into container now
2023-07-22 02:47:00,856:INFO:_master_model_container: 3
2023-07-22 02:47:00,856:INFO:_display_container: 2
2023-07-22 02:47:00,857:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 02:47:00,857:INFO:create_model() successfully completed......................................
2023-07-22 02:47:01,004:INFO:SubProcess create_model() end ==================================
2023-07-22 02:47:01,005:INFO:Creating metrics dataframe
2023-07-22 02:47:01,015:INFO:Initializing Decision Tree Classifier
2023-07-22 02:47:01,015:INFO:Total runtime is 0.9782853444417319 minutes
2023-07-22 02:47:01,018:INFO:SubProcess create_model() called ==================================
2023-07-22 02:47:01,019:INFO:Initializing create_model()
2023-07-22 02:47:01,019:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:47:01,020:INFO:Checking exceptions
2023-07-22 02:47:01,020:INFO:Importing libraries
2023-07-22 02:47:01,020:INFO:Copying training dataset
2023-07-22 02:47:01,029:INFO:Defining folds
2023-07-22 02:47:01,029:INFO:Declaring metric variables
2023-07-22 02:47:01,033:INFO:Importing untrained model
2023-07-22 02:47:01,039:INFO:Decision Tree Classifier Imported successfully
2023-07-22 02:47:01,048:INFO:Starting cross validation
2023-07-22 02:47:01,251:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:47:05,186:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:05,227:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:05,247:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:05,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:05,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:05,537:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:05,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:05,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:05,591:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:15,845:INFO:Calculating mean and std
2023-07-22 02:47:15,846:INFO:Creating metrics dataframe
2023-07-22 02:47:17,598:INFO:Uploading results into container
2023-07-22 02:47:17,600:INFO:Uploading model into container now
2023-07-22 02:47:17,600:INFO:_master_model_container: 4
2023-07-22 02:47:17,601:INFO:_display_container: 2
2023-07-22 02:47:17,601:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 02:47:17,601:INFO:create_model() successfully completed......................................
2023-07-22 02:47:17,739:INFO:SubProcess create_model() end ==================================
2023-07-22 02:47:17,739:INFO:Creating metrics dataframe
2023-07-22 02:47:17,750:INFO:Initializing SVM - Linear Kernel
2023-07-22 02:47:17,751:INFO:Total runtime is 1.2572161237398785 minutes
2023-07-22 02:47:17,756:INFO:SubProcess create_model() called ==================================
2023-07-22 02:47:17,756:INFO:Initializing create_model()
2023-07-22 02:47:17,756:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:47:17,756:INFO:Checking exceptions
2023-07-22 02:47:17,756:INFO:Importing libraries
2023-07-22 02:47:17,756:INFO:Copying training dataset
2023-07-22 02:47:17,765:INFO:Defining folds
2023-07-22 02:47:17,765:INFO:Declaring metric variables
2023-07-22 02:47:17,769:INFO:Importing untrained model
2023-07-22 02:47:17,775:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 02:47:17,781:INFO:Starting cross validation
2023-07-22 02:47:17,979:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:47:19,106:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:21,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:21,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:21,433:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:21,433:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:21,441:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:21,447:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:21,447:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:21,447:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:21,454:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:21,458:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:21,536:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:21,543:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:21,632:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:21,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:21,707:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:21,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 02:47:31,853:INFO:Calculating mean and std
2023-07-22 02:47:31,854:INFO:Creating metrics dataframe
2023-07-22 02:47:33,584:INFO:Uploading results into container
2023-07-22 02:47:33,585:INFO:Uploading model into container now
2023-07-22 02:47:33,586:INFO:_master_model_container: 5
2023-07-22 02:47:33,586:INFO:_display_container: 2
2023-07-22 02:47:33,586:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 02:47:33,588:INFO:create_model() successfully completed......................................
2023-07-22 02:47:33,726:INFO:SubProcess create_model() end ==================================
2023-07-22 02:47:33,726:INFO:Creating metrics dataframe
2023-07-22 02:47:33,738:INFO:Initializing Ridge Classifier
2023-07-22 02:47:33,738:INFO:Total runtime is 1.5236693461736044 minutes
2023-07-22 02:47:33,742:INFO:SubProcess create_model() called ==================================
2023-07-22 02:47:33,742:INFO:Initializing create_model()
2023-07-22 02:47:33,743:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:47:33,743:INFO:Checking exceptions
2023-07-22 02:47:33,743:INFO:Importing libraries
2023-07-22 02:47:33,743:INFO:Copying training dataset
2023-07-22 02:47:33,753:INFO:Defining folds
2023-07-22 02:47:33,753:INFO:Declaring metric variables
2023-07-22 02:47:33,757:INFO:Importing untrained model
2023-07-22 02:47:33,762:INFO:Ridge Classifier Imported successfully
2023-07-22 02:47:33,770:INFO:Starting cross validation
2023-07-22 02:47:33,977:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:47:35,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.15082e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 02:47:35,253:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.42057e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 02:47:35,281:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.27927e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 02:47:35,283:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:35,284:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.22781e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 02:47:35,297:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.0595e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 02:47:35,322:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.24674e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 02:47:35,334:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.94805e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 02:47:35,395:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.3881e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 02:47:37,600:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,601:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,605:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:37,605:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:37,762:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:37,768:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,774:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:37,775:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,780:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:37,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:37,838:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:37,892:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:37,942:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:37,948:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 02:47:48,891:INFO:Calculating mean and std
2023-07-22 02:47:48,892:INFO:Creating metrics dataframe
2023-07-22 02:47:50,688:INFO:Uploading results into container
2023-07-22 02:47:50,689:INFO:Uploading model into container now
2023-07-22 02:47:50,689:INFO:_master_model_container: 6
2023-07-22 02:47:50,689:INFO:_display_container: 2
2023-07-22 02:47:50,690:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 02:47:50,690:INFO:create_model() successfully completed......................................
2023-07-22 02:47:50,839:INFO:SubProcess create_model() end ==================================
2023-07-22 02:47:50,839:INFO:Creating metrics dataframe
2023-07-22 02:47:50,850:INFO:Initializing Random Forest Classifier
2023-07-22 02:47:50,850:INFO:Total runtime is 1.808856189250946 minutes
2023-07-22 02:47:50,855:INFO:SubProcess create_model() called ==================================
2023-07-22 02:47:50,855:INFO:Initializing create_model()
2023-07-22 02:47:50,855:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:47:50,855:INFO:Checking exceptions
2023-07-22 02:47:50,855:INFO:Importing libraries
2023-07-22 02:47:50,855:INFO:Copying training dataset
2023-07-22 02:47:50,864:INFO:Defining folds
2023-07-22 02:47:50,864:INFO:Declaring metric variables
2023-07-22 02:47:50,868:INFO:Importing untrained model
2023-07-22 02:47:50,873:INFO:Random Forest Classifier Imported successfully
2023-07-22 02:47:50,881:INFO:Starting cross validation
2023-07-22 02:47:51,089:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:47:53,580:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 02:47:53,926:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-22 02:47:55,523:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 02:47:56,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 02:47:56,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:56,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 02:47:59,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:47:59,685:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:00,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:00,384:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:00,515:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:00,916:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:00,974:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:01,021:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:01,040:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:11,144:INFO:Calculating mean and std
2023-07-22 02:48:11,145:INFO:Creating metrics dataframe
2023-07-22 02:48:12,877:INFO:Uploading results into container
2023-07-22 02:48:12,878:INFO:Uploading model into container now
2023-07-22 02:48:12,879:INFO:_master_model_container: 7
2023-07-22 02:48:12,879:INFO:_display_container: 2
2023-07-22 02:48:12,880:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 02:48:12,881:INFO:create_model() successfully completed......................................
2023-07-22 02:48:13,019:INFO:SubProcess create_model() end ==================================
2023-07-22 02:48:13,020:INFO:Creating metrics dataframe
2023-07-22 02:48:13,030:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 02:48:13,031:INFO:Total runtime is 2.1785384456316628 minutes
2023-07-22 02:48:13,034:INFO:SubProcess create_model() called ==================================
2023-07-22 02:48:13,035:INFO:Initializing create_model()
2023-07-22 02:48:13,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:48:13,035:INFO:Checking exceptions
2023-07-22 02:48:13,035:INFO:Importing libraries
2023-07-22 02:48:13,035:INFO:Copying training dataset
2023-07-22 02:48:13,043:INFO:Defining folds
2023-07-22 02:48:13,044:INFO:Declaring metric variables
2023-07-22 02:48:13,048:INFO:Importing untrained model
2023-07-22 02:48:13,052:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 02:48:13,062:INFO:Starting cross validation
2023-07-22 02:48:13,264:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:48:16,684:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:16,729:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:16,752:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:16,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:16,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:16,946:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:17,071:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:17,117:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:27,392:INFO:Calculating mean and std
2023-07-22 02:48:27,394:INFO:Creating metrics dataframe
2023-07-22 02:48:29,143:INFO:Uploading results into container
2023-07-22 02:48:29,144:INFO:Uploading model into container now
2023-07-22 02:48:29,145:INFO:_master_model_container: 8
2023-07-22 02:48:29,145:INFO:_display_container: 2
2023-07-22 02:48:29,146:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 02:48:29,146:INFO:create_model() successfully completed......................................
2023-07-22 02:48:29,293:INFO:SubProcess create_model() end ==================================
2023-07-22 02:48:29,294:INFO:Creating metrics dataframe
2023-07-22 02:48:29,306:INFO:Initializing Ada Boost Classifier
2023-07-22 02:48:29,306:INFO:Total runtime is 2.449796187877655 minutes
2023-07-22 02:48:29,309:INFO:SubProcess create_model() called ==================================
2023-07-22 02:48:29,309:INFO:Initializing create_model()
2023-07-22 02:48:29,310:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:48:29,310:INFO:Checking exceptions
2023-07-22 02:48:29,310:INFO:Importing libraries
2023-07-22 02:48:29,310:INFO:Copying training dataset
2023-07-22 02:48:29,319:INFO:Defining folds
2023-07-22 02:48:29,319:INFO:Declaring metric variables
2023-07-22 02:48:29,324:INFO:Importing untrained model
2023-07-22 02:48:29,329:INFO:Ada Boost Classifier Imported successfully
2023-07-22 02:48:29,338:INFO:Starting cross validation
2023-07-22 02:48:29,543:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:48:35,253:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:35,334:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:35,492:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:35,492:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:35,492:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:35,514:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:35,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:35,636:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:36,018:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:48:46,536:INFO:Calculating mean and std
2023-07-22 02:48:46,538:INFO:Creating metrics dataframe
2023-07-22 02:48:48,311:INFO:Uploading results into container
2023-07-22 02:48:48,312:INFO:Uploading model into container now
2023-07-22 02:48:48,313:INFO:_master_model_container: 9
2023-07-22 02:48:48,313:INFO:_display_container: 2
2023-07-22 02:48:48,313:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 02:48:48,313:INFO:create_model() successfully completed......................................
2023-07-22 02:48:48,457:INFO:SubProcess create_model() end ==================================
2023-07-22 02:48:48,458:INFO:Creating metrics dataframe
2023-07-22 02:48:48,468:INFO:Initializing Gradient Boosting Classifier
2023-07-22 02:48:48,468:INFO:Total runtime is 2.7691702405611673 minutes
2023-07-22 02:48:48,472:INFO:SubProcess create_model() called ==================================
2023-07-22 02:48:48,473:INFO:Initializing create_model()
2023-07-22 02:48:48,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:48:48,473:INFO:Checking exceptions
2023-07-22 02:48:48,473:INFO:Importing libraries
2023-07-22 02:48:48,473:INFO:Copying training dataset
2023-07-22 02:48:48,481:INFO:Defining folds
2023-07-22 02:48:48,481:INFO:Declaring metric variables
2023-07-22 02:48:48,485:INFO:Importing untrained model
2023-07-22 02:48:48,489:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 02:48:48,498:INFO:Starting cross validation
2023-07-22 02:48:48,698:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:49:00,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:00,770:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:00,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:00,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:00,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:00,892:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:01,023:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:01,066:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:01,117:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:11,994:INFO:Calculating mean and std
2023-07-22 02:49:11,995:INFO:Creating metrics dataframe
2023-07-22 02:49:13,927:INFO:Uploading results into container
2023-07-22 02:49:13,927:INFO:Uploading model into container now
2023-07-22 02:49:13,928:INFO:_master_model_container: 10
2023-07-22 02:49:13,928:INFO:_display_container: 2
2023-07-22 02:49:13,929:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 02:49:13,929:INFO:create_model() successfully completed......................................
2023-07-22 02:49:14,069:INFO:SubProcess create_model() end ==================================
2023-07-22 02:49:14,069:INFO:Creating metrics dataframe
2023-07-22 02:49:14,080:INFO:Initializing Linear Discriminant Analysis
2023-07-22 02:49:14,082:INFO:Total runtime is 3.196031963825226 minutes
2023-07-22 02:49:14,086:INFO:SubProcess create_model() called ==================================
2023-07-22 02:49:14,086:INFO:Initializing create_model()
2023-07-22 02:49:14,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:49:14,087:INFO:Checking exceptions
2023-07-22 02:49:14,087:INFO:Importing libraries
2023-07-22 02:49:14,087:INFO:Copying training dataset
2023-07-22 02:49:14,097:INFO:Defining folds
2023-07-22 02:49:14,097:INFO:Declaring metric variables
2023-07-22 02:49:14,102:INFO:Importing untrained model
2023-07-22 02:49:14,106:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 02:49:14,115:INFO:Starting cross validation
2023-07-22 02:49:14,400:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:49:18,054:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:18,173:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:18,321:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:18,348:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:18,366:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:18,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:18,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:28,907:INFO:Calculating mean and std
2023-07-22 02:49:28,909:INFO:Creating metrics dataframe
2023-07-22 02:49:30,738:INFO:Uploading results into container
2023-07-22 02:49:30,739:INFO:Uploading model into container now
2023-07-22 02:49:30,739:INFO:_master_model_container: 11
2023-07-22 02:49:30,740:INFO:_display_container: 2
2023-07-22 02:49:30,740:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 02:49:30,740:INFO:create_model() successfully completed......................................
2023-07-22 02:49:30,878:INFO:SubProcess create_model() end ==================================
2023-07-22 02:49:30,878:INFO:Creating metrics dataframe
2023-07-22 02:49:30,889:INFO:Initializing Extra Trees Classifier
2023-07-22 02:49:30,890:INFO:Total runtime is 3.4761858542760216 minutes
2023-07-22 02:49:30,894:INFO:SubProcess create_model() called ==================================
2023-07-22 02:49:30,895:INFO:Initializing create_model()
2023-07-22 02:49:30,895:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:49:30,895:INFO:Checking exceptions
2023-07-22 02:49:30,895:INFO:Importing libraries
2023-07-22 02:49:30,895:INFO:Copying training dataset
2023-07-22 02:49:30,904:INFO:Defining folds
2023-07-22 02:49:30,905:INFO:Declaring metric variables
2023-07-22 02:49:30,909:INFO:Importing untrained model
2023-07-22 02:49:30,913:INFO:Extra Trees Classifier Imported successfully
2023-07-22 02:49:30,922:INFO:Starting cross validation
2023-07-22 02:49:31,126:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:49:34,038:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-22 02:49:34,348:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:36,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:36,490:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:36,738:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:36,750:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:36,882:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:37,859:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:37,908:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:38,768:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:39,096:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:47,645:INFO:Calculating mean and std
2023-07-22 02:49:47,647:INFO:Creating metrics dataframe
2023-07-22 02:49:49,321:INFO:Uploading results into container
2023-07-22 02:49:49,322:INFO:Uploading model into container now
2023-07-22 02:49:49,322:INFO:_master_model_container: 12
2023-07-22 02:49:49,322:INFO:_display_container: 2
2023-07-22 02:49:49,323:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 02:49:49,323:INFO:create_model() successfully completed......................................
2023-07-22 02:49:49,459:INFO:SubProcess create_model() end ==================================
2023-07-22 02:49:49,460:INFO:Creating metrics dataframe
2023-07-22 02:49:49,473:INFO:Initializing Extreme Gradient Boosting
2023-07-22 02:49:49,473:INFO:Total runtime is 3.7859071453412376 minutes
2023-07-22 02:49:49,476:INFO:SubProcess create_model() called ==================================
2023-07-22 02:49:49,477:INFO:Initializing create_model()
2023-07-22 02:49:49,477:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:49:49,477:INFO:Checking exceptions
2023-07-22 02:49:49,477:INFO:Importing libraries
2023-07-22 02:49:49,477:INFO:Copying training dataset
2023-07-22 02:49:49,487:INFO:Defining folds
2023-07-22 02:49:49,487:INFO:Declaring metric variables
2023-07-22 02:49:49,491:INFO:Importing untrained model
2023-07-22 02:49:49,495:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 02:49:49,502:INFO:Starting cross validation
2023-07-22 02:49:49,699:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:49:57,619:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:57,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:57,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:57,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:57,758:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:57,794:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:57,809:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:57,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:49:57,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:08,832:INFO:Calculating mean and std
2023-07-22 02:50:08,834:INFO:Creating metrics dataframe
2023-07-22 02:50:10,733:INFO:Uploading results into container
2023-07-22 02:50:10,734:INFO:Uploading model into container now
2023-07-22 02:50:10,735:INFO:_master_model_container: 13
2023-07-22 02:50:10,735:INFO:_display_container: 2
2023-07-22 02:50:10,736:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 02:50:10,736:INFO:create_model() successfully completed......................................
2023-07-22 02:50:10,925:INFO:SubProcess create_model() end ==================================
2023-07-22 02:50:10,925:INFO:Creating metrics dataframe
2023-07-22 02:50:10,938:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 02:50:10,938:INFO:Total runtime is 4.143664987881978 minutes
2023-07-22 02:50:10,942:INFO:SubProcess create_model() called ==================================
2023-07-22 02:50:10,943:INFO:Initializing create_model()
2023-07-22 02:50:10,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:50:10,943:INFO:Checking exceptions
2023-07-22 02:50:10,943:INFO:Importing libraries
2023-07-22 02:50:10,943:INFO:Copying training dataset
2023-07-22 02:50:10,953:INFO:Defining folds
2023-07-22 02:50:10,953:INFO:Declaring metric variables
2023-07-22 02:50:10,957:INFO:Importing untrained model
2023-07-22 02:50:10,961:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 02:50:10,971:INFO:Starting cross validation
2023-07-22 02:50:11,180:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:50:15,700:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:15,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:15,781:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:15,900:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:15,900:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:15,918:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:15,961:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:16,008:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:27,729:INFO:Calculating mean and std
2023-07-22 02:50:27,731:INFO:Creating metrics dataframe
2023-07-22 02:50:29,725:INFO:Uploading results into container
2023-07-22 02:50:29,726:INFO:Uploading model into container now
2023-07-22 02:50:29,726:INFO:_master_model_container: 14
2023-07-22 02:50:29,727:INFO:_display_container: 2
2023-07-22 02:50:29,727:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 02:50:29,728:INFO:create_model() successfully completed......................................
2023-07-22 02:50:29,888:INFO:SubProcess create_model() end ==================================
2023-07-22 02:50:29,888:INFO:Creating metrics dataframe
2023-07-22 02:50:29,899:INFO:Initializing Dummy Classifier
2023-07-22 02:50:29,899:INFO:Total runtime is 4.459685893853505 minutes
2023-07-22 02:50:29,904:INFO:SubProcess create_model() called ==================================
2023-07-22 02:50:29,905:INFO:Initializing create_model()
2023-07-22 02:50:29,905:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002533A47B850>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:50:29,905:INFO:Checking exceptions
2023-07-22 02:50:29,905:INFO:Importing libraries
2023-07-22 02:50:29,905:INFO:Copying training dataset
2023-07-22 02:50:29,914:INFO:Defining folds
2023-07-22 02:50:29,914:INFO:Declaring metric variables
2023-07-22 02:50:29,918:INFO:Importing untrained model
2023-07-22 02:50:29,922:INFO:Dummy Classifier Imported successfully
2023-07-22 02:50:29,929:INFO:Starting cross validation
2023-07-22 02:50:30,135:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 02:50:31,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:33,635:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:33,654:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:33,678:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:33,681:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:33,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:33,715:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:33,725:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:33,764:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:33,894:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 02:50:34,327:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:34,366:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:34,390:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:34,417:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:34,428:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:34,446:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:34,455:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:34,506:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:34,607:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:50:45,013:INFO:Calculating mean and std
2023-07-22 02:50:45,014:INFO:Creating metrics dataframe
2023-07-22 02:50:47,049:INFO:Uploading results into container
2023-07-22 02:50:47,051:INFO:Uploading model into container now
2023-07-22 02:50:47,051:INFO:_master_model_container: 15
2023-07-22 02:50:47,051:INFO:_display_container: 2
2023-07-22 02:50:47,052:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 02:50:47,052:INFO:create_model() successfully completed......................................
2023-07-22 02:50:47,205:INFO:SubProcess create_model() end ==================================
2023-07-22 02:50:47,206:INFO:Creating metrics dataframe
2023-07-22 02:50:47,232:INFO:Initializing create_model()
2023-07-22 02:50:47,271:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:50:47,271:INFO:Checking exceptions
2023-07-22 02:50:47,273:INFO:Importing libraries
2023-07-22 02:50:47,273:INFO:Copying training dataset
2023-07-22 02:50:47,282:INFO:Defining folds
2023-07-22 02:50:47,282:INFO:Declaring metric variables
2023-07-22 02:50:47,282:INFO:Importing untrained model
2023-07-22 02:50:47,282:INFO:Declaring custom model
2023-07-22 02:50:47,282:INFO:Random Forest Classifier Imported successfully
2023-07-22 02:50:47,516:INFO:Cross validation set to False
2023-07-22 02:50:47,516:INFO:Fitting Model
2023-07-22 02:50:50,045:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 02:50:50,045:INFO:create_model() successfully completed......................................
2023-07-22 02:50:50,240:INFO:Initializing create_model()
2023-07-22 02:50:50,241:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:50:50,241:INFO:Checking exceptions
2023-07-22 02:50:50,243:INFO:Importing libraries
2023-07-22 02:50:50,243:INFO:Copying training dataset
2023-07-22 02:50:50,252:INFO:Defining folds
2023-07-22 02:50:50,252:INFO:Declaring metric variables
2023-07-22 02:50:50,252:INFO:Importing untrained model
2023-07-22 02:50:50,252:INFO:Declaring custom model
2023-07-22 02:50:50,253:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 02:50:50,493:INFO:Cross validation set to False
2023-07-22 02:50:50,493:INFO:Fitting Model
2023-07-22 02:50:59,932:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 02:50:59,932:INFO:create_model() successfully completed......................................
2023-07-22 02:51:00,089:INFO:Initializing create_model()
2023-07-22 02:51:00,089:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002533C917550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 02:51:00,090:INFO:Checking exceptions
2023-07-22 02:51:00,093:INFO:Importing libraries
2023-07-22 02:51:00,093:INFO:Copying training dataset
2023-07-22 02:51:00,100:INFO:Defining folds
2023-07-22 02:51:00,100:INFO:Declaring metric variables
2023-07-22 02:51:00,100:INFO:Importing untrained model
2023-07-22 02:51:00,100:INFO:Declaring custom model
2023-07-22 02:51:00,101:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 02:51:00,339:INFO:Cross validation set to False
2023-07-22 02:51:00,339:INFO:Fitting Model
2023-07-22 02:51:02,274:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 02:51:02,274:INFO:create_model() successfully completed......................................
2023-07-22 02:51:02,478:INFO:_master_model_container: 15
2023-07-22 02:51:02,479:INFO:_display_container: 2
2023-07-22 02:51:02,480:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)]
2023-07-22 02:51:02,480:INFO:compare_models() successfully completed......................................
2023-07-22 02:55:03,116:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:55:03,136:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:03,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:03,224:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:55:03,244:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:03,254:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:55:03,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 02:55:03,273:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:03,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:03,290:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:03,319:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:03,335:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:04,262:WARNING:lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression

2023-07-22 02:55:04,274:WARNING:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

2023-07-22 02:55:04,277:WARNING:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

2023-07-22 02:55:04,279:WARNING:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

2023-07-22 02:55:14,020:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:14,085:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:14,135:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:14,147:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:14,405:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:20,384:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 02:55:29,047:WARNING:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

2023-07-22 02:55:29,050:WARNING:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

2023-07-22 02:55:29,051:WARNING:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.

2023-07-22 17:41:30,340:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 17:41:32,043:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 17:41:32,043:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 17:41:32,043:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 17:41:32,043:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 17:41:32,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 17:41:32,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 17:41:32,064:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 17:41:32,606:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning:

This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0


2023-07-22 17:41:32,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning:

'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680


2023-07-22 17:41:36,986:INFO:PyCaret ClassificationExperiment
2023-07-22 17:41:36,987:INFO:Logging name: clf-default-name
2023-07-22 17:41:36,987:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 17:41:36,987:INFO:version 3.0.4
2023-07-22 17:41:36,987:INFO:Initializing setup()
2023-07-22 17:41:36,987:INFO:self.USI: 3628
2023-07-22 17:41:36,987:INFO:self._variable_keys: {'data', 'memory', 'idx', 'fold_generator', 'pipeline', 'USI', '_available_plots', 'gpu_param', 'y', 'logging_param', 'X_train', 'X_test', 'is_multiclass', 'gpu_n_jobs_param', 'log_plots_param', 'fold_groups_param', 'exp_id', 'y_test', 'target_param', 'fix_imbalance', 'X', 'n_jobs_param', 'html_param', 'fold_shuffle_param', 'seed', 'y_train', 'exp_name_log', '_ml_usecase'}
2023-07-22 17:41:36,987:INFO:Checking environment
2023-07-22 17:41:36,987:INFO:python_version: 3.10.8
2023-07-22 17:41:36,987:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 17:41:36,987:INFO:machine: AMD64
2023-07-22 17:41:36,987:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 17:41:36,994:INFO:Memory: svmem(total=16505966592, available=7956733952, percent=51.8, used=8549232640, free=7956733952)
2023-07-22 17:41:36,994:INFO:Physical Core: 6
2023-07-22 17:41:36,994:INFO:Logical Core: 12
2023-07-22 17:41:36,994:INFO:Checking libraries
2023-07-22 17:41:36,994:INFO:System:
2023-07-22 17:41:36,994:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 17:41:36,994:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 17:41:36,994:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 17:41:36,994:INFO:PyCaret required dependencies:
2023-07-22 17:41:36,994:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 17:41:36,997:INFO:                 pip: 22.2.2
2023-07-22 17:41:36,997:INFO:          setuptools: 63.2.0
2023-07-22 17:41:36,997:INFO:             pycaret: 3.0.4
2023-07-22 17:41:36,997:INFO:             IPython: 8.11.0
2023-07-22 17:41:36,997:INFO:          ipywidgets: 8.0.7
2023-07-22 17:41:36,997:INFO:                tqdm: 4.64.1
2023-07-22 17:41:36,997:INFO:               numpy: 1.23.5
2023-07-22 17:41:36,997:INFO:              pandas: 1.5.3
2023-07-22 17:41:36,997:INFO:              jinja2: 3.1.2
2023-07-22 17:41:36,998:INFO:               scipy: 1.9.3
2023-07-22 17:41:36,998:INFO:              joblib: 1.2.0
2023-07-22 17:41:36,998:INFO:             sklearn: 1.2.2
2023-07-22 17:41:36,998:INFO:                pyod: 1.1.0
2023-07-22 17:41:36,998:INFO:            imblearn: 0.10.1
2023-07-22 17:41:36,998:INFO:   category_encoders: 2.6.1
2023-07-22 17:41:36,998:INFO:            lightgbm: 3.3.5
2023-07-22 17:41:36,998:INFO:               numba: 0.57.0
2023-07-22 17:41:36,998:INFO:            requests: 2.28.2
2023-07-22 17:41:36,998:INFO:          matplotlib: 3.7.1
2023-07-22 17:41:36,998:INFO:          scikitplot: 0.3.7
2023-07-22 17:41:36,998:INFO:         yellowbrick: 1.5
2023-07-22 17:41:36,998:INFO:              plotly: 5.15.0
2023-07-22 17:41:36,998:INFO:    plotly-resampler: Not installed
2023-07-22 17:41:36,998:INFO:             kaleido: 0.2.1
2023-07-22 17:41:36,998:INFO:           schemdraw: 0.15
2023-07-22 17:41:36,998:INFO:         statsmodels: 0.13.5
2023-07-22 17:41:36,998:INFO:              sktime: 0.21.0
2023-07-22 17:41:36,999:INFO:               tbats: 1.1.3
2023-07-22 17:41:36,999:INFO:            pmdarima: 2.0.3
2023-07-22 17:41:36,999:INFO:              psutil: 5.9.4
2023-07-22 17:41:36,999:INFO:          markupsafe: 2.1.2
2023-07-22 17:41:36,999:INFO:             pickle5: Not installed
2023-07-22 17:41:36,999:INFO:         cloudpickle: 2.2.1
2023-07-22 17:41:36,999:INFO:         deprecation: 2.1.0
2023-07-22 17:41:36,999:INFO:              xxhash: 3.2.0
2023-07-22 17:41:36,999:INFO:           wurlitzer: Not installed
2023-07-22 17:41:36,999:INFO:PyCaret optional dependencies:
2023-07-22 17:41:37,810:INFO:                shap: 0.41.0
2023-07-22 17:41:37,810:INFO:           interpret: 0.4.2
2023-07-22 17:41:37,810:INFO:                umap: 0.5.3
2023-07-22 17:41:37,810:INFO:    pandas_profiling: 4.1.2
2023-07-22 17:41:37,810:INFO:  explainerdashboard: Not installed
2023-07-22 17:41:37,810:INFO:             autoviz: Not installed
2023-07-22 17:41:37,810:INFO:           fairlearn: Not installed
2023-07-22 17:41:37,810:INFO:          deepchecks: Not installed
2023-07-22 17:41:37,810:INFO:             xgboost: 1.7.6
2023-07-22 17:41:37,810:INFO:            catboost: Not installed
2023-07-22 17:41:37,810:INFO:              kmodes: Not installed
2023-07-22 17:41:37,810:INFO:             mlxtend: Not installed
2023-07-22 17:41:37,810:INFO:       statsforecast: Not installed
2023-07-22 17:41:37,811:INFO:        tune_sklearn: Not installed
2023-07-22 17:41:37,811:INFO:                 ray: Not installed
2023-07-22 17:41:37,811:INFO:            hyperopt: Not installed
2023-07-22 17:41:37,811:INFO:              optuna: 3.2.0
2023-07-22 17:41:37,811:INFO:               skopt: Not installed
2023-07-22 17:41:37,811:INFO:              mlflow: 2.4.2
2023-07-22 17:41:37,811:INFO:              gradio: Not installed
2023-07-22 17:41:37,811:INFO:             fastapi: 0.95.2
2023-07-22 17:41:37,811:INFO:             uvicorn: 0.22.0
2023-07-22 17:41:37,811:INFO:              m2cgen: Not installed
2023-07-22 17:41:37,811:INFO:           evidently: Not installed
2023-07-22 17:41:37,811:INFO:               fugue: Not installed
2023-07-22 17:41:37,811:INFO:           streamlit: Not installed
2023-07-22 17:41:37,811:INFO:             prophet: Not installed
2023-07-22 17:41:37,811:INFO:None
2023-07-22 17:41:37,811:INFO:Set up data.
2023-07-22 17:41:37,834:INFO:Set up train/test split.
2023-07-22 17:41:37,835:INFO:Set up data.
2023-07-22 17:41:37,852:INFO:Set up index.
2023-07-22 17:41:37,852:INFO:Set up folding strategy.
2023-07-22 17:41:37,853:INFO:Assigning column types.
2023-07-22 17:41:37,859:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 17:41:37,912:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 17:41:37,915:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 17:41:37,962:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:37,967:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:38,020:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 17:41:38,021:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 17:41:38,055:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:38,059:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:38,059:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 17:41:38,115:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 17:41:38,147:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:38,151:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:38,209:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 17:41:38,243:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:38,246:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:38,247:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 17:41:38,336:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:38,340:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:38,422:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:38,424:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:38,426:INFO:Preparing preprocessing pipeline...
2023-07-22 17:41:38,455:INFO:Set up iterative imputation.
2023-07-22 17:41:38,456:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 17:41:38,461:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 17:41:38,467:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 17:41:38,544:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 17:41:38,593:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:38,597:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:38,674:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:38,677:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:38,715:INFO:Set up encoding of categorical features.
2023-07-22 17:41:38,715:INFO:Set up imbalanced handling.
2023-07-22 17:41:38,715:INFO:Set up feature selection.
2023-07-22 17:41:38,802:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:38,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:39,299:INFO:Finished creating preprocessing pipeline.
2023-07-22 17:41:39,328:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 17:41:39,328:INFO:Creating final display dataframe.
2023-07-22 17:41:40,299:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             36.1%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              3628
2023-07-22 17:41:40,407:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:40,410:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:40,505:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 17:41:40,508:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 17:41:40,509:INFO:setup() successfully completed in 6.17s...............
2023-07-22 17:41:40,587:INFO:Initializing compare_models()
2023-07-22 17:41:40,587:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 17:41:40,588:INFO:Checking exceptions
2023-07-22 17:41:40,597:INFO:Preparing display monitor
2023-07-22 17:41:40,630:INFO:Initializing Logistic Regression
2023-07-22 17:41:40,631:INFO:Total runtime is 1.671314239501953e-05 minutes
2023-07-22 17:41:40,637:INFO:SubProcess create_model() called ==================================
2023-07-22 17:41:40,637:INFO:Initializing create_model()
2023-07-22 17:41:40,637:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:41:40,637:INFO:Checking exceptions
2023-07-22 17:41:40,638:INFO:Importing libraries
2023-07-22 17:41:40,638:INFO:Copying training dataset
2023-07-22 17:41:40,645:INFO:Defining folds
2023-07-22 17:41:40,645:INFO:Declaring metric variables
2023-07-22 17:41:40,649:INFO:Importing untrained model
2023-07-22 17:41:40,654:INFO:Logistic Regression Imported successfully
2023-07-22 17:41:40,665:INFO:Starting cross validation
2023-07-22 17:41:40,876:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:41:48,169:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:48,241:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:48,368:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:48,443:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:48,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:48,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:49,399:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:49,604:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:49,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:41:49,842:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:41:49,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:41:50,056:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 17:41:50,183:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:41:50,275:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:41:50,279:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:41:51,201:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:41:51,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:41:51,921:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:42:12,643:INFO:Calculating mean and std
2023-07-22 17:42:12,645:INFO:Creating metrics dataframe
2023-07-22 17:42:15,292:INFO:Uploading results into container
2023-07-22 17:42:15,293:INFO:Uploading model into container now
2023-07-22 17:42:15,295:INFO:_master_model_container: 1
2023-07-22 17:42:15,295:INFO:_display_container: 2
2023-07-22 17:42:15,295:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 17:42:15,296:INFO:create_model() successfully completed......................................
2023-07-22 17:42:15,421:INFO:SubProcess create_model() end ==================================
2023-07-22 17:42:15,421:INFO:Creating metrics dataframe
2023-07-22 17:42:15,430:INFO:Initializing K Neighbors Classifier
2023-07-22 17:42:15,431:INFO:Total runtime is 0.5800085703531901 minutes
2023-07-22 17:42:15,434:INFO:SubProcess create_model() called ==================================
2023-07-22 17:42:15,434:INFO:Initializing create_model()
2023-07-22 17:42:15,434:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:42:15,435:INFO:Checking exceptions
2023-07-22 17:42:15,435:INFO:Importing libraries
2023-07-22 17:42:15,435:INFO:Copying training dataset
2023-07-22 17:42:15,443:INFO:Defining folds
2023-07-22 17:42:15,443:INFO:Declaring metric variables
2023-07-22 17:42:15,448:INFO:Importing untrained model
2023-07-22 17:42:15,454:INFO:K Neighbors Classifier Imported successfully
2023-07-22 17:42:15,462:INFO:Starting cross validation
2023-07-22 17:42:15,676:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:42:38,339:INFO:Calculating mean and std
2023-07-22 17:42:38,341:INFO:Creating metrics dataframe
2023-07-22 17:42:40,559:INFO:Uploading results into container
2023-07-22 17:42:40,560:INFO:Uploading model into container now
2023-07-22 17:42:40,560:INFO:_master_model_container: 2
2023-07-22 17:42:40,561:INFO:_display_container: 2
2023-07-22 17:42:40,561:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 17:42:40,561:INFO:create_model() successfully completed......................................
2023-07-22 17:42:40,677:INFO:SubProcess create_model() end ==================================
2023-07-22 17:42:40,678:INFO:Creating metrics dataframe
2023-07-22 17:42:40,688:INFO:Initializing Naive Bayes
2023-07-22 17:42:40,688:INFO:Total runtime is 1.0009768684705098 minutes
2023-07-22 17:42:40,694:INFO:SubProcess create_model() called ==================================
2023-07-22 17:42:40,694:INFO:Initializing create_model()
2023-07-22 17:42:40,694:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:42:40,695:INFO:Checking exceptions
2023-07-22 17:42:40,695:INFO:Importing libraries
2023-07-22 17:42:40,695:INFO:Copying training dataset
2023-07-22 17:42:40,705:INFO:Defining folds
2023-07-22 17:42:40,706:INFO:Declaring metric variables
2023-07-22 17:42:40,711:INFO:Importing untrained model
2023-07-22 17:42:40,716:INFO:Naive Bayes Imported successfully
2023-07-22 17:42:40,726:INFO:Starting cross validation
2023-07-22 17:42:40,955:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:43:03,383:INFO:Calculating mean and std
2023-07-22 17:43:03,385:INFO:Creating metrics dataframe
2023-07-22 17:43:05,921:INFO:Uploading results into container
2023-07-22 17:43:05,922:INFO:Uploading model into container now
2023-07-22 17:43:05,922:INFO:_master_model_container: 3
2023-07-22 17:43:05,923:INFO:_display_container: 2
2023-07-22 17:43:05,923:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 17:43:05,923:INFO:create_model() successfully completed......................................
2023-07-22 17:43:06,036:INFO:SubProcess create_model() end ==================================
2023-07-22 17:43:06,036:INFO:Creating metrics dataframe
2023-07-22 17:43:06,046:INFO:Initializing Decision Tree Classifier
2023-07-22 17:43:06,046:INFO:Total runtime is 1.4236080646514893 minutes
2023-07-22 17:43:06,051:INFO:SubProcess create_model() called ==================================
2023-07-22 17:43:06,051:INFO:Initializing create_model()
2023-07-22 17:43:06,051:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:43:06,051:INFO:Checking exceptions
2023-07-22 17:43:06,051:INFO:Importing libraries
2023-07-22 17:43:06,051:INFO:Copying training dataset
2023-07-22 17:43:06,059:INFO:Defining folds
2023-07-22 17:43:06,060:INFO:Declaring metric variables
2023-07-22 17:43:06,063:INFO:Importing untrained model
2023-07-22 17:43:06,070:INFO:Decision Tree Classifier Imported successfully
2023-07-22 17:43:06,079:INFO:Starting cross validation
2023-07-22 17:43:06,311:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:43:28,477:INFO:Calculating mean and std
2023-07-22 17:43:28,478:INFO:Creating metrics dataframe
2023-07-22 17:43:30,860:INFO:Uploading results into container
2023-07-22 17:43:30,862:INFO:Uploading model into container now
2023-07-22 17:43:30,863:INFO:_master_model_container: 4
2023-07-22 17:43:30,863:INFO:_display_container: 2
2023-07-22 17:43:30,864:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 17:43:30,864:INFO:create_model() successfully completed......................................
2023-07-22 17:43:30,987:INFO:SubProcess create_model() end ==================================
2023-07-22 17:43:30,987:INFO:Creating metrics dataframe
2023-07-22 17:43:30,999:INFO:Initializing SVM - Linear Kernel
2023-07-22 17:43:30,999:INFO:Total runtime is 1.8394917130470276 minutes
2023-07-22 17:43:31,004:INFO:SubProcess create_model() called ==================================
2023-07-22 17:43:31,004:INFO:Initializing create_model()
2023-07-22 17:43:31,004:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:43:31,005:INFO:Checking exceptions
2023-07-22 17:43:31,005:INFO:Importing libraries
2023-07-22 17:43:31,005:INFO:Copying training dataset
2023-07-22 17:43:31,012:INFO:Defining folds
2023-07-22 17:43:31,013:INFO:Declaring metric variables
2023-07-22 17:43:31,017:INFO:Importing untrained model
2023-07-22 17:43:31,023:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 17:43:31,033:INFO:Starting cross validation
2023-07-22 17:43:31,253:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:43:31,938:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,120:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,131:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,158:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,164:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,166:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,207:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:33,211:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 17:43:52,969:INFO:Calculating mean and std
2023-07-22 17:43:52,971:INFO:Creating metrics dataframe
2023-07-22 17:43:55,403:INFO:Uploading results into container
2023-07-22 17:43:55,404:INFO:Uploading model into container now
2023-07-22 17:43:55,404:INFO:_master_model_container: 5
2023-07-22 17:43:55,404:INFO:_display_container: 2
2023-07-22 17:43:55,405:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 17:43:55,405:INFO:create_model() successfully completed......................................
2023-07-22 17:43:55,518:INFO:SubProcess create_model() end ==================================
2023-07-22 17:43:55,518:INFO:Creating metrics dataframe
2023-07-22 17:43:55,530:INFO:Initializing Ridge Classifier
2023-07-22 17:43:55,530:INFO:Total runtime is 2.248332687218984 minutes
2023-07-22 17:43:55,535:INFO:SubProcess create_model() called ==================================
2023-07-22 17:43:55,535:INFO:Initializing create_model()
2023-07-22 17:43:55,535:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:43:55,535:INFO:Checking exceptions
2023-07-22 17:43:55,535:INFO:Importing libraries
2023-07-22 17:43:55,535:INFO:Copying training dataset
2023-07-22 17:43:55,545:INFO:Defining folds
2023-07-22 17:43:55,546:INFO:Declaring metric variables
2023-07-22 17:43:55,551:INFO:Importing untrained model
2023-07-22 17:43:55,558:INFO:Ridge Classifier Imported successfully
2023-07-22 17:43:55,566:INFO:Starting cross validation
2023-07-22 17:43:55,787:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:43:56,400:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:56,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.81648e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:56,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.27927e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:56,940:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.94805e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:56,959:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.24674e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:56,963:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.22781e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:56,981:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.15082e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:57,024:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.3881e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:57,132:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.0595e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:57,154:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.42057e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 17:43:57,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:57,696:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:57,696:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:57,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:57,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:57,759:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:57,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:57,914:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:43:57,934:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 17:44:17,782:INFO:Calculating mean and std
2023-07-22 17:44:17,784:INFO:Creating metrics dataframe
2023-07-22 17:44:20,200:INFO:Uploading results into container
2023-07-22 17:44:20,201:INFO:Uploading model into container now
2023-07-22 17:44:20,201:INFO:_master_model_container: 6
2023-07-22 17:44:20,202:INFO:_display_container: 2
2023-07-22 17:44:20,202:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 17:44:20,203:INFO:create_model() successfully completed......................................
2023-07-22 17:44:20,317:INFO:SubProcess create_model() end ==================================
2023-07-22 17:44:20,317:INFO:Creating metrics dataframe
2023-07-22 17:44:20,329:INFO:Initializing Random Forest Classifier
2023-07-22 17:44:20,329:INFO:Total runtime is 2.6616470495859783 minutes
2023-07-22 17:44:20,334:INFO:SubProcess create_model() called ==================================
2023-07-22 17:44:20,334:INFO:Initializing create_model()
2023-07-22 17:44:20,335:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:44:20,335:INFO:Checking exceptions
2023-07-22 17:44:20,335:INFO:Importing libraries
2023-07-22 17:44:20,335:INFO:Copying training dataset
2023-07-22 17:44:20,345:INFO:Defining folds
2023-07-22 17:44:20,345:INFO:Declaring metric variables
2023-07-22 17:44:20,350:INFO:Importing untrained model
2023-07-22 17:44:20,355:INFO:Random Forest Classifier Imported successfully
2023-07-22 17:44:20,363:INFO:Starting cross validation
2023-07-22 17:44:20,585:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:44:43,364:INFO:Calculating mean and std
2023-07-22 17:44:43,365:INFO:Creating metrics dataframe
2023-07-22 17:44:45,888:INFO:Uploading results into container
2023-07-22 17:44:45,889:INFO:Uploading model into container now
2023-07-22 17:44:45,889:INFO:_master_model_container: 7
2023-07-22 17:44:45,890:INFO:_display_container: 2
2023-07-22 17:44:45,891:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 17:44:45,891:INFO:create_model() successfully completed......................................
2023-07-22 17:44:46,002:INFO:SubProcess create_model() end ==================================
2023-07-22 17:44:46,002:INFO:Creating metrics dataframe
2023-07-22 17:44:46,014:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 17:44:46,014:INFO:Total runtime is 3.0897321184476216 minutes
2023-07-22 17:44:46,019:INFO:SubProcess create_model() called ==================================
2023-07-22 17:44:46,019:INFO:Initializing create_model()
2023-07-22 17:44:46,019:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:44:46,019:INFO:Checking exceptions
2023-07-22 17:44:46,020:INFO:Importing libraries
2023-07-22 17:44:46,020:INFO:Copying training dataset
2023-07-22 17:44:46,029:INFO:Defining folds
2023-07-22 17:44:46,029:INFO:Declaring metric variables
2023-07-22 17:44:46,034:INFO:Importing untrained model
2023-07-22 17:44:46,041:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 17:44:46,050:INFO:Starting cross validation
2023-07-22 17:44:46,279:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:45:08,461:INFO:Calculating mean and std
2023-07-22 17:45:08,462:INFO:Creating metrics dataframe
2023-07-22 17:45:10,909:INFO:Uploading results into container
2023-07-22 17:45:10,911:INFO:Uploading model into container now
2023-07-22 17:45:10,911:INFO:_master_model_container: 8
2023-07-22 17:45:10,912:INFO:_display_container: 2
2023-07-22 17:45:10,912:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 17:45:10,912:INFO:create_model() successfully completed......................................
2023-07-22 17:45:11,024:INFO:SubProcess create_model() end ==================================
2023-07-22 17:45:11,025:INFO:Creating metrics dataframe
2023-07-22 17:45:11,037:INFO:Initializing Ada Boost Classifier
2023-07-22 17:45:11,037:INFO:Total runtime is 3.506793514887492 minutes
2023-07-22 17:45:11,041:INFO:SubProcess create_model() called ==================================
2023-07-22 17:45:11,042:INFO:Initializing create_model()
2023-07-22 17:45:11,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:45:11,042:INFO:Checking exceptions
2023-07-22 17:45:11,042:INFO:Importing libraries
2023-07-22 17:45:11,042:INFO:Copying training dataset
2023-07-22 17:45:11,051:INFO:Defining folds
2023-07-22 17:45:11,052:INFO:Declaring metric variables
2023-07-22 17:45:11,056:INFO:Importing untrained model
2023-07-22 17:45:11,061:INFO:Ada Boost Classifier Imported successfully
2023-07-22 17:45:11,070:INFO:Starting cross validation
2023-07-22 17:45:11,284:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:45:33,695:INFO:Calculating mean and std
2023-07-22 17:45:33,696:INFO:Creating metrics dataframe
2023-07-22 17:45:36,171:INFO:Uploading results into container
2023-07-22 17:45:36,172:INFO:Uploading model into container now
2023-07-22 17:45:36,172:INFO:_master_model_container: 9
2023-07-22 17:45:36,173:INFO:_display_container: 2
2023-07-22 17:45:36,174:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 17:45:36,175:INFO:create_model() successfully completed......................................
2023-07-22 17:45:36,297:INFO:SubProcess create_model() end ==================================
2023-07-22 17:45:36,297:INFO:Creating metrics dataframe
2023-07-22 17:45:36,308:INFO:Initializing Gradient Boosting Classifier
2023-07-22 17:45:36,309:INFO:Total runtime is 3.9279851516087847 minutes
2023-07-22 17:45:36,313:INFO:SubProcess create_model() called ==================================
2023-07-22 17:45:36,313:INFO:Initializing create_model()
2023-07-22 17:45:36,313:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:45:36,314:INFO:Checking exceptions
2023-07-22 17:45:36,314:INFO:Importing libraries
2023-07-22 17:45:36,314:INFO:Copying training dataset
2023-07-22 17:45:36,324:INFO:Defining folds
2023-07-22 17:45:36,324:INFO:Declaring metric variables
2023-07-22 17:45:36,328:INFO:Importing untrained model
2023-07-22 17:45:36,333:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 17:45:36,342:INFO:Starting cross validation
2023-07-22 17:45:36,549:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:45:40,071:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:45:40,085:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:45:40,111:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:45:40,140:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:45:40,153:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:45:40,161:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:45:40,187:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:45:40,199:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:45:40,255:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-22 17:46:00,174:INFO:Calculating mean and std
2023-07-22 17:46:00,176:INFO:Creating metrics dataframe
2023-07-22 17:46:02,549:INFO:Uploading results into container
2023-07-22 17:46:02,552:INFO:Uploading model into container now
2023-07-22 17:46:02,553:INFO:_master_model_container: 10
2023-07-22 17:46:02,553:INFO:_display_container: 2
2023-07-22 17:46:02,554:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 17:46:02,554:INFO:create_model() successfully completed......................................
2023-07-22 17:46:02,674:INFO:SubProcess create_model() end ==================================
2023-07-22 17:46:02,674:INFO:Creating metrics dataframe
2023-07-22 17:46:02,686:INFO:Initializing Linear Discriminant Analysis
2023-07-22 17:46:02,686:INFO:Total runtime is 4.367608992258708 minutes
2023-07-22 17:46:02,691:INFO:SubProcess create_model() called ==================================
2023-07-22 17:46:02,692:INFO:Initializing create_model()
2023-07-22 17:46:02,692:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:46:02,692:INFO:Checking exceptions
2023-07-22 17:46:02,692:INFO:Importing libraries
2023-07-22 17:46:02,692:INFO:Copying training dataset
2023-07-22 17:46:02,700:INFO:Defining folds
2023-07-22 17:46:02,700:INFO:Declaring metric variables
2023-07-22 17:46:02,708:INFO:Importing untrained model
2023-07-22 17:46:02,714:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 17:46:02,722:INFO:Starting cross validation
2023-07-22 17:46:02,949:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:46:24,996:INFO:Calculating mean and std
2023-07-22 17:46:24,997:INFO:Creating metrics dataframe
2023-07-22 17:46:27,271:INFO:Uploading results into container
2023-07-22 17:46:27,272:INFO:Uploading model into container now
2023-07-22 17:46:27,272:INFO:_master_model_container: 11
2023-07-22 17:46:27,272:INFO:_display_container: 2
2023-07-22 17:46:27,273:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 17:46:27,273:INFO:create_model() successfully completed......................................
2023-07-22 17:46:27,393:INFO:SubProcess create_model() end ==================================
2023-07-22 17:46:27,393:INFO:Creating metrics dataframe
2023-07-22 17:46:27,407:INFO:Initializing Extra Trees Classifier
2023-07-22 17:46:27,407:INFO:Total runtime is 4.779628245035807 minutes
2023-07-22 17:46:27,412:INFO:SubProcess create_model() called ==================================
2023-07-22 17:46:27,412:INFO:Initializing create_model()
2023-07-22 17:46:27,413:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:46:27,413:INFO:Checking exceptions
2023-07-22 17:46:27,413:INFO:Importing libraries
2023-07-22 17:46:27,413:INFO:Copying training dataset
2023-07-22 17:46:27,421:INFO:Defining folds
2023-07-22 17:46:27,422:INFO:Declaring metric variables
2023-07-22 17:46:27,426:INFO:Importing untrained model
2023-07-22 17:46:27,431:INFO:Extra Trees Classifier Imported successfully
2023-07-22 17:46:27,440:INFO:Starting cross validation
2023-07-22 17:46:27,648:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:46:50,631:INFO:Calculating mean and std
2023-07-22 17:46:50,632:INFO:Creating metrics dataframe
2023-07-22 17:46:53,004:INFO:Uploading results into container
2023-07-22 17:46:53,005:INFO:Uploading model into container now
2023-07-22 17:46:53,005:INFO:_master_model_container: 12
2023-07-22 17:46:53,006:INFO:_display_container: 2
2023-07-22 17:46:53,007:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 17:46:53,007:INFO:create_model() successfully completed......................................
2023-07-22 17:46:53,115:INFO:SubProcess create_model() end ==================================
2023-07-22 17:46:53,115:INFO:Creating metrics dataframe
2023-07-22 17:46:53,128:INFO:Initializing Extreme Gradient Boosting
2023-07-22 17:46:53,129:INFO:Total runtime is 5.208316135406494 minutes
2023-07-22 17:46:53,133:INFO:SubProcess create_model() called ==================================
2023-07-22 17:46:53,134:INFO:Initializing create_model()
2023-07-22 17:46:53,134:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:46:53,134:INFO:Checking exceptions
2023-07-22 17:46:53,134:INFO:Importing libraries
2023-07-22 17:46:53,134:INFO:Copying training dataset
2023-07-22 17:46:53,146:INFO:Defining folds
2023-07-22 17:46:53,146:INFO:Declaring metric variables
2023-07-22 17:46:53,153:INFO:Importing untrained model
2023-07-22 17:46:53,159:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 17:46:53,167:INFO:Starting cross validation
2023-07-22 17:46:53,388:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:47:15,619:INFO:Calculating mean and std
2023-07-22 17:47:15,621:INFO:Creating metrics dataframe
2023-07-22 17:47:17,871:INFO:Uploading results into container
2023-07-22 17:47:17,872:INFO:Uploading model into container now
2023-07-22 17:47:17,873:INFO:_master_model_container: 13
2023-07-22 17:47:17,873:INFO:_display_container: 2
2023-07-22 17:47:17,874:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 17:47:17,874:INFO:create_model() successfully completed......................................
2023-07-22 17:47:17,990:INFO:SubProcess create_model() end ==================================
2023-07-22 17:47:17,990:INFO:Creating metrics dataframe
2023-07-22 17:47:18,003:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 17:47:18,004:INFO:Total runtime is 5.622901805241902 minutes
2023-07-22 17:47:18,008:INFO:SubProcess create_model() called ==================================
2023-07-22 17:47:18,009:INFO:Initializing create_model()
2023-07-22 17:47:18,009:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:47:18,009:INFO:Checking exceptions
2023-07-22 17:47:18,009:INFO:Importing libraries
2023-07-22 17:47:18,009:INFO:Copying training dataset
2023-07-22 17:47:18,019:INFO:Defining folds
2023-07-22 17:47:18,019:INFO:Declaring metric variables
2023-07-22 17:47:18,023:INFO:Importing untrained model
2023-07-22 17:47:18,029:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 17:47:18,040:INFO:Starting cross validation
2023-07-22 17:47:18,261:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:47:40,345:INFO:Calculating mean and std
2023-07-22 17:47:40,346:INFO:Creating metrics dataframe
2023-07-22 17:47:42,591:INFO:Uploading results into container
2023-07-22 17:47:42,592:INFO:Uploading model into container now
2023-07-22 17:47:42,592:INFO:_master_model_container: 14
2023-07-22 17:47:42,592:INFO:_display_container: 2
2023-07-22 17:47:42,593:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 17:47:42,593:INFO:create_model() successfully completed......................................
2023-07-22 17:47:42,705:INFO:SubProcess create_model() end ==================================
2023-07-22 17:47:42,705:INFO:Creating metrics dataframe
2023-07-22 17:47:42,720:INFO:Initializing Dummy Classifier
2023-07-22 17:47:42,720:INFO:Total runtime is 6.034835954507192 minutes
2023-07-22 17:47:42,725:INFO:SubProcess create_model() called ==================================
2023-07-22 17:47:42,725:INFO:Initializing create_model()
2023-07-22 17:47:42,725:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A9A80B3220>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:47:42,725:INFO:Checking exceptions
2023-07-22 17:47:42,725:INFO:Importing libraries
2023-07-22 17:47:42,726:INFO:Copying training dataset
2023-07-22 17:47:42,737:INFO:Defining folds
2023-07-22 17:47:42,737:INFO:Declaring metric variables
2023-07-22 17:47:42,742:INFO:Importing untrained model
2023-07-22 17:47:42,747:INFO:Dummy Classifier Imported successfully
2023-07-22 17:47:42,757:INFO:Starting cross validation
2023-07-22 17:47:42,982:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:47:43,778:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:47:45,550:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:47:45,569:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:47:45,588:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:47:45,613:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:47:45,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:47:45,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:47:45,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 17:48:05,031:INFO:Calculating mean and std
2023-07-22 17:48:05,033:INFO:Creating metrics dataframe
2023-07-22 17:48:07,426:INFO:Uploading results into container
2023-07-22 17:48:07,427:INFO:Uploading model into container now
2023-07-22 17:48:07,427:INFO:_master_model_container: 15
2023-07-22 17:48:07,427:INFO:_display_container: 2
2023-07-22 17:48:07,428:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 17:48:07,428:INFO:create_model() successfully completed......................................
2023-07-22 17:48:07,541:INFO:SubProcess create_model() end ==================================
2023-07-22 17:48:07,541:INFO:Creating metrics dataframe
2023-07-22 17:48:07,566:INFO:Initializing create_model()
2023-07-22 17:48:07,566:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:48:07,566:INFO:Checking exceptions
2023-07-22 17:48:07,569:INFO:Importing libraries
2023-07-22 17:48:07,569:INFO:Copying training dataset
2023-07-22 17:48:07,578:INFO:Defining folds
2023-07-22 17:48:07,579:INFO:Declaring metric variables
2023-07-22 17:48:07,579:INFO:Importing untrained model
2023-07-22 17:48:07,579:INFO:Declaring custom model
2023-07-22 17:48:07,580:INFO:Random Forest Classifier Imported successfully
2023-07-22 17:48:07,806:INFO:Cross validation set to False
2023-07-22 17:48:07,806:INFO:Fitting Model
2023-07-22 17:48:10,177:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 17:48:10,177:INFO:create_model() successfully completed......................................
2023-07-22 17:48:10,294:INFO:Initializing create_model()
2023-07-22 17:48:10,294:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:48:10,294:INFO:Checking exceptions
2023-07-22 17:48:10,297:INFO:Importing libraries
2023-07-22 17:48:10,297:INFO:Copying training dataset
2023-07-22 17:48:10,305:INFO:Defining folds
2023-07-22 17:48:10,305:INFO:Declaring metric variables
2023-07-22 17:48:10,305:INFO:Importing untrained model
2023-07-22 17:48:10,305:INFO:Declaring custom model
2023-07-22 17:48:10,306:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 17:48:10,519:INFO:Cross validation set to False
2023-07-22 17:48:10,519:INFO:Fitting Model
2023-07-22 17:48:12,705:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 17:48:12,705:INFO:create_model() successfully completed......................................
2023-07-22 17:48:12,834:INFO:Initializing create_model()
2023-07-22 17:48:12,881:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:48:12,881:INFO:Checking exceptions
2023-07-22 17:48:12,883:INFO:Importing libraries
2023-07-22 17:48:12,883:INFO:Copying training dataset
2023-07-22 17:48:12,892:INFO:Defining folds
2023-07-22 17:48:12,892:INFO:Declaring metric variables
2023-07-22 17:48:12,893:INFO:Importing untrained model
2023-07-22 17:48:12,893:INFO:Declaring custom model
2023-07-22 17:48:12,893:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 17:48:13,112:INFO:Cross validation set to False
2023-07-22 17:48:13,112:INFO:Fitting Model
2023-07-22 17:48:15,395:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 17:48:15,395:INFO:create_model() successfully completed......................................
2023-07-22 17:48:15,538:INFO:_master_model_container: 15
2023-07-22 17:48:15,539:INFO:_display_container: 2
2023-07-22 17:48:15,540:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)]
2023-07-22 17:48:15,540:INFO:compare_models() successfully completed......................................
2023-07-22 17:48:15,738:INFO:Initializing create_model()
2023-07-22 17:48:15,739:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9A66A3280>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 17:48:15,739:INFO:Checking exceptions
2023-07-22 17:48:15,757:INFO:Importing libraries
2023-07-22 17:48:15,757:INFO:Copying training dataset
2023-07-22 17:48:15,767:INFO:Defining folds
2023-07-22 17:48:15,767:INFO:Declaring metric variables
2023-07-22 17:48:15,772:INFO:Importing untrained model
2023-07-22 17:48:15,778:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 17:48:15,789:INFO:Starting cross validation
2023-07-22 17:48:16,030:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 17:48:38,480:INFO:Calculating mean and std
2023-07-22 17:48:38,482:INFO:Creating metrics dataframe
2023-07-22 17:48:38,489:INFO:Finalizing model
2023-07-22 17:48:41,124:INFO:Uploading results into container
2023-07-22 17:48:41,126:INFO:Uploading model into container now
2023-07-22 17:48:41,140:INFO:_master_model_container: 16
2023-07-22 17:48:41,140:INFO:_display_container: 3
2023-07-22 17:48:41,141:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 17:48:41,141:INFO:create_model() successfully completed......................................
2023-07-22 17:58:53,511:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\preprocessing\_encoders.py:868: FutureWarning:

`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.


2023-07-22 17:59:29,175:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\preprocessing\_encoders.py:868: FutureWarning:

`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.


2023-07-22 18:00:28,646:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:35: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:54: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:63: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,695:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:69: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,695:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:77: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,698:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:5: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,698:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:10: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,698:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:15: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,698:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:20: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:363: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:385: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:428: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:439: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:186: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:197: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_image.py:175: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,734:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\explainers\_partition.py:676: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:00:28,761:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-22 18:00:28,761:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-22 18:00:28,828:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-22 18:00:28,833:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-22 18:00:28,840:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-22 18:00:29,058:WARNING:`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.

2023-07-22 18:00:29,467:WARNING:`np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations

2023-07-22 18:01:04,088:WARNING:`np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations

2023-07-22 18:03:21,339:WARNING:`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.

2023-07-22 18:03:21,826:WARNING:`np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations

2023-07-22 18:03:57,335:WARNING:`np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations

2023-07-22 18:44:31,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 18:44:32,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 18:44:32,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 18:44:32,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 18:44:32,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-22 18:44:32,203:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 18:44:32,203:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 18:44:32,205:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 18:44:32,489:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning:

This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0


2023-07-22 18:44:32,704:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning:

'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680


2023-07-22 18:44:35,554:INFO:PyCaret ClassificationExperiment
2023-07-22 18:44:35,554:INFO:Logging name: clf-default-name
2023-07-22 18:44:35,554:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-22 18:44:35,554:INFO:version 3.0.4
2023-07-22 18:44:35,554:INFO:Initializing setup()
2023-07-22 18:44:35,554:INFO:self.USI: bd55
2023-07-22 18:44:35,554:INFO:self._variable_keys: {'X', 'fold_generator', 'USI', 'fold_groups_param', '_ml_usecase', 'exp_id', 'y_test', 'seed', 'n_jobs_param', 'html_param', 'log_plots_param', 'logging_param', 'memory', 'y_train', 'target_param', 'pipeline', 'exp_name_log', 'y', 'gpu_n_jobs_param', 'idx', 'X_test', 'fix_imbalance', 'fold_shuffle_param', 'X_train', 'gpu_param', 'is_multiclass', 'data', '_available_plots'}
2023-07-22 18:44:35,554:INFO:Checking environment
2023-07-22 18:44:35,554:INFO:python_version: 3.10.8
2023-07-22 18:44:35,554:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-22 18:44:35,554:INFO:machine: AMD64
2023-07-22 18:44:35,554:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-22 18:44:35,559:INFO:Memory: svmem(total=16505966592, available=6129119232, percent=62.9, used=10376847360, free=6129119232)
2023-07-22 18:44:35,559:INFO:Physical Core: 6
2023-07-22 18:44:35,560:INFO:Logical Core: 12
2023-07-22 18:44:35,560:INFO:Checking libraries
2023-07-22 18:44:35,560:INFO:System:
2023-07-22 18:44:35,560:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-22 18:44:35,560:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-22 18:44:35,560:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-22 18:44:35,560:INFO:PyCaret required dependencies:
2023-07-22 18:44:35,560:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-22 18:44:35,562:INFO:                 pip: 22.2.2
2023-07-22 18:44:35,562:INFO:          setuptools: 63.2.0
2023-07-22 18:44:35,562:INFO:             pycaret: 3.0.4
2023-07-22 18:44:35,562:INFO:             IPython: 8.11.0
2023-07-22 18:44:35,562:INFO:          ipywidgets: 8.0.7
2023-07-22 18:44:35,562:INFO:                tqdm: 4.64.1
2023-07-22 18:44:35,562:INFO:               numpy: 1.23.5
2023-07-22 18:44:35,562:INFO:              pandas: 1.5.3
2023-07-22 18:44:35,562:INFO:              jinja2: 3.1.2
2023-07-22 18:44:35,562:INFO:               scipy: 1.9.3
2023-07-22 18:44:35,563:INFO:              joblib: 1.2.0
2023-07-22 18:44:35,563:INFO:             sklearn: 1.2.2
2023-07-22 18:44:35,563:INFO:                pyod: 1.1.0
2023-07-22 18:44:35,563:INFO:            imblearn: 0.10.1
2023-07-22 18:44:35,563:INFO:   category_encoders: 2.6.1
2023-07-22 18:44:35,563:INFO:            lightgbm: 3.3.5
2023-07-22 18:44:35,563:INFO:               numba: 0.57.0
2023-07-22 18:44:35,563:INFO:            requests: 2.28.2
2023-07-22 18:44:35,563:INFO:          matplotlib: 3.7.1
2023-07-22 18:44:35,563:INFO:          scikitplot: 0.3.7
2023-07-22 18:44:35,563:INFO:         yellowbrick: 1.5
2023-07-22 18:44:35,563:INFO:              plotly: 5.15.0
2023-07-22 18:44:35,563:INFO:    plotly-resampler: Not installed
2023-07-22 18:44:35,563:INFO:             kaleido: 0.2.1
2023-07-22 18:44:35,563:INFO:           schemdraw: 0.15
2023-07-22 18:44:35,563:INFO:         statsmodels: 0.13.5
2023-07-22 18:44:35,563:INFO:              sktime: 0.21.0
2023-07-22 18:44:35,563:INFO:               tbats: 1.1.3
2023-07-22 18:44:35,563:INFO:            pmdarima: 2.0.3
2023-07-22 18:44:35,563:INFO:              psutil: 5.9.4
2023-07-22 18:44:35,563:INFO:          markupsafe: 2.1.2
2023-07-22 18:44:35,563:INFO:             pickle5: Not installed
2023-07-22 18:44:35,563:INFO:         cloudpickle: 2.2.1
2023-07-22 18:44:35,563:INFO:         deprecation: 2.1.0
2023-07-22 18:44:35,563:INFO:              xxhash: 3.2.0
2023-07-22 18:44:35,563:INFO:           wurlitzer: Not installed
2023-07-22 18:44:35,563:INFO:PyCaret optional dependencies:
2023-07-22 18:44:35,816:INFO:                shap: 0.41.0
2023-07-22 18:44:35,816:INFO:           interpret: 0.4.2
2023-07-22 18:44:35,817:INFO:                umap: 0.5.3
2023-07-22 18:44:35,817:INFO:    pandas_profiling: 4.1.2
2023-07-22 18:44:35,817:INFO:  explainerdashboard: Not installed
2023-07-22 18:44:35,817:INFO:             autoviz: Not installed
2023-07-22 18:44:35,817:INFO:           fairlearn: Not installed
2023-07-22 18:44:35,817:INFO:          deepchecks: Not installed
2023-07-22 18:44:35,817:INFO:             xgboost: 1.7.6
2023-07-22 18:44:35,817:INFO:            catboost: Not installed
2023-07-22 18:44:35,817:INFO:              kmodes: Not installed
2023-07-22 18:44:35,817:INFO:             mlxtend: Not installed
2023-07-22 18:44:35,817:INFO:       statsforecast: Not installed
2023-07-22 18:44:35,817:INFO:        tune_sklearn: Not installed
2023-07-22 18:44:35,817:INFO:                 ray: Not installed
2023-07-22 18:44:35,817:INFO:            hyperopt: Not installed
2023-07-22 18:44:35,817:INFO:              optuna: 3.2.0
2023-07-22 18:44:35,817:INFO:               skopt: Not installed
2023-07-22 18:44:35,817:INFO:              mlflow: 2.4.2
2023-07-22 18:44:35,817:INFO:              gradio: Not installed
2023-07-22 18:44:35,817:INFO:             fastapi: 0.95.2
2023-07-22 18:44:35,817:INFO:             uvicorn: 0.22.0
2023-07-22 18:44:35,817:INFO:              m2cgen: Not installed
2023-07-22 18:44:35,818:INFO:           evidently: Not installed
2023-07-22 18:44:35,818:INFO:               fugue: Not installed
2023-07-22 18:44:35,818:INFO:           streamlit: Not installed
2023-07-22 18:44:35,818:INFO:             prophet: Not installed
2023-07-22 18:44:35,818:INFO:None
2023-07-22 18:44:35,818:INFO:Set up data.
2023-07-22 18:44:35,839:INFO:Set up train/test split.
2023-07-22 18:44:35,839:INFO:Set up data.
2023-07-22 18:44:35,857:INFO:Set up index.
2023-07-22 18:44:35,858:INFO:Set up folding strategy.
2023-07-22 18:44:35,858:INFO:Assigning column types.
2023-07-22 18:44:35,864:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-22 18:44:35,919:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 18:44:35,922:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 18:44:35,967:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:35,971:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:36,030:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-22 18:44:36,031:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 18:44:36,065:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:36,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:36,069:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-22 18:44:36,116:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 18:44:36,145:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:36,148:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:36,204:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-22 18:44:36,236:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:36,239:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:36,240:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-22 18:44:36,326:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:36,331:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:36,418:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:36,422:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:36,424:INFO:Preparing preprocessing pipeline...
2023-07-22 18:44:36,425:INFO:Set up iterative imputation.
2023-07-22 18:44:36,425:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-22 18:44:36,431:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-22 18:44:36,438:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-22 18:44:36,514:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-22 18:44:36,571:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:36,575:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:36,661:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:36,664:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:36,693:INFO:Set up encoding of categorical features.
2023-07-22 18:44:36,693:INFO:Set up imbalanced handling.
2023-07-22 18:44:36,693:INFO:Set up feature selection.
2023-07-22 18:44:36,768:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:36,771:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:37,197:INFO:Finished creating preprocessing pipeline.
2023-07-22 18:44:37,222:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-22 18:44:37,222:INFO:Creating final display dataframe.
2023-07-22 18:44:38,056:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             36.1%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              bd55
2023-07-22 18:44:38,167:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:38,170:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:38,260:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-22 18:44:38,263:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-22 18:44:38,264:INFO:setup() successfully completed in 4.75s...............
2023-07-22 18:44:38,344:INFO:Initializing compare_models()
2023-07-22 18:44:38,344:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-22 18:44:38,344:INFO:Checking exceptions
2023-07-22 18:44:38,353:INFO:Preparing display monitor
2023-07-22 18:44:38,382:INFO:Initializing Logistic Regression
2023-07-22 18:44:38,382:INFO:Total runtime is 0.0 minutes
2023-07-22 18:44:38,387:INFO:SubProcess create_model() called ==================================
2023-07-22 18:44:38,387:INFO:Initializing create_model()
2023-07-22 18:44:38,388:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:44:38,388:INFO:Checking exceptions
2023-07-22 18:44:38,388:INFO:Importing libraries
2023-07-22 18:44:38,388:INFO:Copying training dataset
2023-07-22 18:44:38,397:INFO:Defining folds
2023-07-22 18:44:38,397:INFO:Declaring metric variables
2023-07-22 18:44:38,402:INFO:Importing untrained model
2023-07-22 18:44:38,408:INFO:Logistic Regression Imported successfully
2023-07-22 18:44:38,417:INFO:Starting cross validation
2023-07-22 18:44:38,704:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:44:41,642:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:41,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:41,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:41,721:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:41,857:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:41,923:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:41,965:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:41,970:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:41,994:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-22 18:44:43,209:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:44:43,271:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:44:43,301:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:44:43,397:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:44:43,583:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:44:43,634:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:44:43,661:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:44:43,673:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:44:43,704:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:45:03,664:INFO:Calculating mean and std
2023-07-22 18:45:03,666:INFO:Creating metrics dataframe
2023-07-22 18:45:06,324:INFO:Uploading results into container
2023-07-22 18:45:06,325:INFO:Uploading model into container now
2023-07-22 18:45:06,326:INFO:_master_model_container: 1
2023-07-22 18:45:06,326:INFO:_display_container: 2
2023-07-22 18:45:06,327:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-22 18:45:06,327:INFO:create_model() successfully completed......................................
2023-07-22 18:45:06,438:INFO:SubProcess create_model() end ==================================
2023-07-22 18:45:06,438:INFO:Creating metrics dataframe
2023-07-22 18:45:06,448:INFO:Initializing K Neighbors Classifier
2023-07-22 18:45:06,448:INFO:Total runtime is 0.46776092847188316 minutes
2023-07-22 18:45:06,454:INFO:SubProcess create_model() called ==================================
2023-07-22 18:45:06,454:INFO:Initializing create_model()
2023-07-22 18:45:06,454:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:45:06,454:INFO:Checking exceptions
2023-07-22 18:45:06,454:INFO:Importing libraries
2023-07-22 18:45:06,454:INFO:Copying training dataset
2023-07-22 18:45:06,466:INFO:Defining folds
2023-07-22 18:45:06,466:INFO:Declaring metric variables
2023-07-22 18:45:06,474:INFO:Importing untrained model
2023-07-22 18:45:06,482:INFO:K Neighbors Classifier Imported successfully
2023-07-22 18:45:06,493:INFO:Starting cross validation
2023-07-22 18:45:06,720:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:45:30,071:INFO:Calculating mean and std
2023-07-22 18:45:30,072:INFO:Creating metrics dataframe
2023-07-22 18:45:32,621:INFO:Uploading results into container
2023-07-22 18:45:32,622:INFO:Uploading model into container now
2023-07-22 18:45:32,622:INFO:_master_model_container: 2
2023-07-22 18:45:32,623:INFO:_display_container: 2
2023-07-22 18:45:32,623:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-22 18:45:32,624:INFO:create_model() successfully completed......................................
2023-07-22 18:45:32,733:INFO:SubProcess create_model() end ==================================
2023-07-22 18:45:32,733:INFO:Creating metrics dataframe
2023-07-22 18:45:32,745:INFO:Initializing Naive Bayes
2023-07-22 18:45:32,745:INFO:Total runtime is 0.9060444434483846 minutes
2023-07-22 18:45:32,749:INFO:SubProcess create_model() called ==================================
2023-07-22 18:45:32,750:INFO:Initializing create_model()
2023-07-22 18:45:32,750:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:45:32,750:INFO:Checking exceptions
2023-07-22 18:45:32,750:INFO:Importing libraries
2023-07-22 18:45:32,750:INFO:Copying training dataset
2023-07-22 18:45:32,760:INFO:Defining folds
2023-07-22 18:45:32,760:INFO:Declaring metric variables
2023-07-22 18:45:32,767:INFO:Importing untrained model
2023-07-22 18:45:32,773:INFO:Naive Bayes Imported successfully
2023-07-22 18:45:32,783:INFO:Starting cross validation
2023-07-22 18:45:32,996:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:45:57,671:INFO:Calculating mean and std
2023-07-22 18:45:57,673:INFO:Creating metrics dataframe
2023-07-22 18:46:00,319:INFO:Uploading results into container
2023-07-22 18:46:00,320:INFO:Uploading model into container now
2023-07-22 18:46:00,320:INFO:_master_model_container: 3
2023-07-22 18:46:00,320:INFO:_display_container: 2
2023-07-22 18:46:00,321:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-22 18:46:00,321:INFO:create_model() successfully completed......................................
2023-07-22 18:46:00,430:INFO:SubProcess create_model() end ==================================
2023-07-22 18:46:00,430:INFO:Creating metrics dataframe
2023-07-22 18:46:00,442:INFO:Initializing Decision Tree Classifier
2023-07-22 18:46:00,442:INFO:Total runtime is 1.367666478951772 minutes
2023-07-22 18:46:00,447:INFO:SubProcess create_model() called ==================================
2023-07-22 18:46:00,448:INFO:Initializing create_model()
2023-07-22 18:46:00,448:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:46:00,448:INFO:Checking exceptions
2023-07-22 18:46:00,448:INFO:Importing libraries
2023-07-22 18:46:00,448:INFO:Copying training dataset
2023-07-22 18:46:00,461:INFO:Defining folds
2023-07-22 18:46:00,461:INFO:Declaring metric variables
2023-07-22 18:46:00,467:INFO:Importing untrained model
2023-07-22 18:46:00,471:INFO:Decision Tree Classifier Imported successfully
2023-07-22 18:46:00,481:INFO:Starting cross validation
2023-07-22 18:46:00,701:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:46:24,657:INFO:Calculating mean and std
2023-07-22 18:46:24,660:INFO:Creating metrics dataframe
2023-07-22 18:46:27,264:INFO:Uploading results into container
2023-07-22 18:46:27,265:INFO:Uploading model into container now
2023-07-22 18:46:27,266:INFO:_master_model_container: 4
2023-07-22 18:46:27,266:INFO:_display_container: 2
2023-07-22 18:46:27,267:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-22 18:46:27,267:INFO:create_model() successfully completed......................................
2023-07-22 18:46:27,386:INFO:SubProcess create_model() end ==================================
2023-07-22 18:46:27,386:INFO:Creating metrics dataframe
2023-07-22 18:46:27,398:INFO:Initializing SVM - Linear Kernel
2023-07-22 18:46:27,398:INFO:Total runtime is 1.81693221728007 minutes
2023-07-22 18:46:27,402:INFO:SubProcess create_model() called ==================================
2023-07-22 18:46:27,403:INFO:Initializing create_model()
2023-07-22 18:46:27,403:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:46:27,403:INFO:Checking exceptions
2023-07-22 18:46:27,403:INFO:Importing libraries
2023-07-22 18:46:27,403:INFO:Copying training dataset
2023-07-22 18:46:27,412:INFO:Defining folds
2023-07-22 18:46:27,413:INFO:Declaring metric variables
2023-07-22 18:46:27,418:INFO:Importing untrained model
2023-07-22 18:46:27,423:INFO:SVM - Linear Kernel Imported successfully
2023-07-22 18:46:27,432:INFO:Starting cross validation
2023-07-22 18:46:27,661:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:46:28,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:29,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:29,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:29,794:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:30,111:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:30,148:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:30,295:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:30,356:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:30,403:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:30,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-22 18:46:52,505:INFO:Calculating mean and std
2023-07-22 18:46:52,508:INFO:Creating metrics dataframe
2023-07-22 18:46:55,218:INFO:Uploading results into container
2023-07-22 18:46:55,219:INFO:Uploading model into container now
2023-07-22 18:46:55,220:INFO:_master_model_container: 5
2023-07-22 18:46:55,220:INFO:_display_container: 2
2023-07-22 18:46:55,221:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-22 18:46:55,221:INFO:create_model() successfully completed......................................
2023-07-22 18:46:55,344:INFO:SubProcess create_model() end ==================================
2023-07-22 18:46:55,344:INFO:Creating metrics dataframe
2023-07-22 18:46:55,355:INFO:Initializing Ridge Classifier
2023-07-22 18:46:55,356:INFO:Total runtime is 2.2828911900520326 minutes
2023-07-22 18:46:55,359:INFO:SubProcess create_model() called ==================================
2023-07-22 18:46:55,359:INFO:Initializing create_model()
2023-07-22 18:46:55,359:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:46:55,359:INFO:Checking exceptions
2023-07-22 18:46:55,359:INFO:Importing libraries
2023-07-22 18:46:55,359:INFO:Copying training dataset
2023-07-22 18:46:55,369:INFO:Defining folds
2023-07-22 18:46:55,369:INFO:Declaring metric variables
2023-07-22 18:46:55,374:INFO:Importing untrained model
2023-07-22 18:46:55,379:INFO:Ridge Classifier Imported successfully
2023-07-22 18:46:55,388:INFO:Starting cross validation
2023-07-22 18:46:55,607:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:46:56,271:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:56,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.0595e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:56,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.81648e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:56,918:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.3881e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:56,929:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.27927e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:56,947:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.22781e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:56,954:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.15082e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:56,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.94805e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:56,997:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=6.42057e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:57,011:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=7.24674e-51): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-07-22 18:46:57,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:57,721:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:57,745:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:57,758:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:57,822:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:57,829:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:57,838:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:57,871:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:46:57,884:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-22 18:47:19,098:INFO:Calculating mean and std
2023-07-22 18:47:19,099:INFO:Creating metrics dataframe
2023-07-22 18:47:21,595:INFO:Uploading results into container
2023-07-22 18:47:21,595:INFO:Uploading model into container now
2023-07-22 18:47:21,596:INFO:_master_model_container: 6
2023-07-22 18:47:21,597:INFO:_display_container: 2
2023-07-22 18:47:21,597:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-22 18:47:21,597:INFO:create_model() successfully completed......................................
2023-07-22 18:47:21,709:INFO:SubProcess create_model() end ==================================
2023-07-22 18:47:21,709:INFO:Creating metrics dataframe
2023-07-22 18:47:21,723:INFO:Initializing Random Forest Classifier
2023-07-22 18:47:21,723:INFO:Total runtime is 2.7223472476005557 minutes
2023-07-22 18:47:21,728:INFO:SubProcess create_model() called ==================================
2023-07-22 18:47:21,729:INFO:Initializing create_model()
2023-07-22 18:47:21,729:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:47:21,729:INFO:Checking exceptions
2023-07-22 18:47:21,729:INFO:Importing libraries
2023-07-22 18:47:21,729:INFO:Copying training dataset
2023-07-22 18:47:21,737:INFO:Defining folds
2023-07-22 18:47:21,737:INFO:Declaring metric variables
2023-07-22 18:47:21,741:INFO:Importing untrained model
2023-07-22 18:47:21,747:INFO:Random Forest Classifier Imported successfully
2023-07-22 18:47:21,759:INFO:Starting cross validation
2023-07-22 18:47:21,971:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:47:46,583:INFO:Calculating mean and std
2023-07-22 18:47:46,585:INFO:Creating metrics dataframe
2023-07-22 18:47:49,189:INFO:Uploading results into container
2023-07-22 18:47:49,190:INFO:Uploading model into container now
2023-07-22 18:47:49,191:INFO:_master_model_container: 7
2023-07-22 18:47:49,191:INFO:_display_container: 2
2023-07-22 18:47:49,192:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 18:47:49,193:INFO:create_model() successfully completed......................................
2023-07-22 18:47:49,339:INFO:SubProcess create_model() end ==================================
2023-07-22 18:47:49,340:INFO:Creating metrics dataframe
2023-07-22 18:47:49,355:INFO:Initializing Quadratic Discriminant Analysis
2023-07-22 18:47:49,356:INFO:Total runtime is 3.1828962167104087 minutes
2023-07-22 18:47:49,361:INFO:SubProcess create_model() called ==================================
2023-07-22 18:47:49,362:INFO:Initializing create_model()
2023-07-22 18:47:49,362:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:47:49,362:INFO:Checking exceptions
2023-07-22 18:47:49,362:INFO:Importing libraries
2023-07-22 18:47:49,363:INFO:Copying training dataset
2023-07-22 18:47:49,374:INFO:Defining folds
2023-07-22 18:47:49,374:INFO:Declaring metric variables
2023-07-22 18:47:49,379:INFO:Importing untrained model
2023-07-22 18:47:49,387:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-22 18:47:49,400:INFO:Starting cross validation
2023-07-22 18:47:49,738:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:48:16,465:INFO:Calculating mean and std
2023-07-22 18:48:16,466:INFO:Creating metrics dataframe
2023-07-22 18:48:19,359:INFO:Uploading results into container
2023-07-22 18:48:19,360:INFO:Uploading model into container now
2023-07-22 18:48:19,361:INFO:_master_model_container: 8
2023-07-22 18:48:19,361:INFO:_display_container: 2
2023-07-22 18:48:19,361:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-22 18:48:19,362:INFO:create_model() successfully completed......................................
2023-07-22 18:48:19,482:INFO:SubProcess create_model() end ==================================
2023-07-22 18:48:19,482:INFO:Creating metrics dataframe
2023-07-22 18:48:19,497:INFO:Initializing Ada Boost Classifier
2023-07-22 18:48:19,497:INFO:Total runtime is 3.6852454026540125 minutes
2023-07-22 18:48:19,504:INFO:SubProcess create_model() called ==================================
2023-07-22 18:48:19,505:INFO:Initializing create_model()
2023-07-22 18:48:19,505:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:48:19,505:INFO:Checking exceptions
2023-07-22 18:48:19,505:INFO:Importing libraries
2023-07-22 18:48:19,505:INFO:Copying training dataset
2023-07-22 18:48:19,518:INFO:Defining folds
2023-07-22 18:48:19,518:INFO:Declaring metric variables
2023-07-22 18:48:19,525:INFO:Importing untrained model
2023-07-22 18:48:19,532:INFO:Ada Boost Classifier Imported successfully
2023-07-22 18:48:19,543:INFO:Starting cross validation
2023-07-22 18:48:19,800:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:48:44,034:INFO:Calculating mean and std
2023-07-22 18:48:44,035:INFO:Creating metrics dataframe
2023-07-22 18:48:46,477:INFO:Uploading results into container
2023-07-22 18:48:46,479:INFO:Uploading model into container now
2023-07-22 18:48:46,480:INFO:_master_model_container: 9
2023-07-22 18:48:46,480:INFO:_display_container: 2
2023-07-22 18:48:46,480:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-22 18:48:46,481:INFO:create_model() successfully completed......................................
2023-07-22 18:48:46,592:INFO:SubProcess create_model() end ==================================
2023-07-22 18:48:46,592:INFO:Creating metrics dataframe
2023-07-22 18:48:46,606:INFO:Initializing Gradient Boosting Classifier
2023-07-22 18:48:46,606:INFO:Total runtime is 4.1370536446571355 minutes
2023-07-22 18:48:46,612:INFO:SubProcess create_model() called ==================================
2023-07-22 18:48:46,612:INFO:Initializing create_model()
2023-07-22 18:48:46,613:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:48:46,613:INFO:Checking exceptions
2023-07-22 18:48:46,613:INFO:Importing libraries
2023-07-22 18:48:46,613:INFO:Copying training dataset
2023-07-22 18:48:46,625:INFO:Defining folds
2023-07-22 18:48:46,625:INFO:Declaring metric variables
2023-07-22 18:48:46,629:INFO:Importing untrained model
2023-07-22 18:48:46,634:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 18:48:46,641:INFO:Starting cross validation
2023-07-22 18:48:46,852:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:49:10,660:INFO:Calculating mean and std
2023-07-22 18:49:10,662:INFO:Creating metrics dataframe
2023-07-22 18:49:13,393:INFO:Uploading results into container
2023-07-22 18:49:13,394:INFO:Uploading model into container now
2023-07-22 18:49:13,394:INFO:_master_model_container: 10
2023-07-22 18:49:13,394:INFO:_display_container: 2
2023-07-22 18:49:13,395:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 18:49:13,395:INFO:create_model() successfully completed......................................
2023-07-22 18:49:13,510:INFO:SubProcess create_model() end ==================================
2023-07-22 18:49:13,511:INFO:Creating metrics dataframe
2023-07-22 18:49:13,527:INFO:Initializing Linear Discriminant Analysis
2023-07-22 18:49:13,527:INFO:Total runtime is 4.585748267173767 minutes
2023-07-22 18:49:13,532:INFO:SubProcess create_model() called ==================================
2023-07-22 18:49:13,533:INFO:Initializing create_model()
2023-07-22 18:49:13,533:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:49:13,533:INFO:Checking exceptions
2023-07-22 18:49:13,533:INFO:Importing libraries
2023-07-22 18:49:13,533:INFO:Copying training dataset
2023-07-22 18:49:13,541:INFO:Defining folds
2023-07-22 18:49:13,541:INFO:Declaring metric variables
2023-07-22 18:49:13,546:INFO:Importing untrained model
2023-07-22 18:49:13,551:INFO:Linear Discriminant Analysis Imported successfully
2023-07-22 18:49:13,561:INFO:Starting cross validation
2023-07-22 18:49:13,776:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:49:38,121:INFO:Calculating mean and std
2023-07-22 18:49:38,123:INFO:Creating metrics dataframe
2023-07-22 18:49:40,537:INFO:Uploading results into container
2023-07-22 18:49:40,539:INFO:Uploading model into container now
2023-07-22 18:49:40,540:INFO:_master_model_container: 11
2023-07-22 18:49:40,540:INFO:_display_container: 2
2023-07-22 18:49:40,540:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-22 18:49:40,540:INFO:create_model() successfully completed......................................
2023-07-22 18:49:40,663:INFO:SubProcess create_model() end ==================================
2023-07-22 18:49:40,664:INFO:Creating metrics dataframe
2023-07-22 18:49:40,678:INFO:Initializing Extra Trees Classifier
2023-07-22 18:49:40,678:INFO:Total runtime is 5.038265935579936 minutes
2023-07-22 18:49:40,683:INFO:SubProcess create_model() called ==================================
2023-07-22 18:49:40,683:INFO:Initializing create_model()
2023-07-22 18:49:40,684:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:49:40,684:INFO:Checking exceptions
2023-07-22 18:49:40,684:INFO:Importing libraries
2023-07-22 18:49:40,684:INFO:Copying training dataset
2023-07-22 18:49:40,695:INFO:Defining folds
2023-07-22 18:49:40,695:INFO:Declaring metric variables
2023-07-22 18:49:40,701:INFO:Importing untrained model
2023-07-22 18:49:40,707:INFO:Extra Trees Classifier Imported successfully
2023-07-22 18:49:40,716:INFO:Starting cross validation
2023-07-22 18:49:40,944:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:50:06,119:INFO:Calculating mean and std
2023-07-22 18:50:06,122:INFO:Creating metrics dataframe
2023-07-22 18:50:08,706:INFO:Uploading results into container
2023-07-22 18:50:08,707:INFO:Uploading model into container now
2023-07-22 18:50:08,708:INFO:_master_model_container: 12
2023-07-22 18:50:08,708:INFO:_display_container: 2
2023-07-22 18:50:08,709:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-22 18:50:08,709:INFO:create_model() successfully completed......................................
2023-07-22 18:50:08,828:INFO:SubProcess create_model() end ==================================
2023-07-22 18:50:08,828:INFO:Creating metrics dataframe
2023-07-22 18:50:08,845:INFO:Initializing Extreme Gradient Boosting
2023-07-22 18:50:08,845:INFO:Total runtime is 5.507705382506053 minutes
2023-07-22 18:50:08,849:INFO:SubProcess create_model() called ==================================
2023-07-22 18:50:08,850:INFO:Initializing create_model()
2023-07-22 18:50:08,850:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:50:08,850:INFO:Checking exceptions
2023-07-22 18:50:08,850:INFO:Importing libraries
2023-07-22 18:50:08,850:INFO:Copying training dataset
2023-07-22 18:50:08,860:INFO:Defining folds
2023-07-22 18:50:08,860:INFO:Declaring metric variables
2023-07-22 18:50:08,864:INFO:Importing untrained model
2023-07-22 18:50:08,869:INFO:Extreme Gradient Boosting Imported successfully
2023-07-22 18:50:08,879:INFO:Starting cross validation
2023-07-22 18:50:09,095:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:50:33,065:INFO:Calculating mean and std
2023-07-22 18:50:33,066:INFO:Creating metrics dataframe
2023-07-22 18:50:35,594:INFO:Uploading results into container
2023-07-22 18:50:35,595:INFO:Uploading model into container now
2023-07-22 18:50:35,596:INFO:_master_model_container: 13
2023-07-22 18:50:35,596:INFO:_display_container: 2
2023-07-22 18:50:35,598:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-22 18:50:35,598:INFO:create_model() successfully completed......................................
2023-07-22 18:50:35,717:INFO:SubProcess create_model() end ==================================
2023-07-22 18:50:35,718:INFO:Creating metrics dataframe
2023-07-22 18:50:35,734:INFO:Initializing Light Gradient Boosting Machine
2023-07-22 18:50:35,734:INFO:Total runtime is 5.955868661403657 minutes
2023-07-22 18:50:35,739:INFO:SubProcess create_model() called ==================================
2023-07-22 18:50:35,739:INFO:Initializing create_model()
2023-07-22 18:50:35,739:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:50:35,739:INFO:Checking exceptions
2023-07-22 18:50:35,740:INFO:Importing libraries
2023-07-22 18:50:35,740:INFO:Copying training dataset
2023-07-22 18:50:35,750:INFO:Defining folds
2023-07-22 18:50:35,750:INFO:Declaring metric variables
2023-07-22 18:50:35,755:INFO:Importing untrained model
2023-07-22 18:50:35,761:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 18:50:35,771:INFO:Starting cross validation
2023-07-22 18:50:35,998:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:51:00,223:INFO:Calculating mean and std
2023-07-22 18:51:00,224:INFO:Creating metrics dataframe
2023-07-22 18:51:02,756:INFO:Uploading results into container
2023-07-22 18:51:02,757:INFO:Uploading model into container now
2023-07-22 18:51:02,757:INFO:_master_model_container: 14
2023-07-22 18:51:02,757:INFO:_display_container: 2
2023-07-22 18:51:02,758:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 18:51:02,758:INFO:create_model() successfully completed......................................
2023-07-22 18:51:02,874:INFO:SubProcess create_model() end ==================================
2023-07-22 18:51:02,874:INFO:Creating metrics dataframe
2023-07-22 18:51:02,889:INFO:Initializing Dummy Classifier
2023-07-22 18:51:02,889:INFO:Total runtime is 6.408439954121908 minutes
2023-07-22 18:51:02,894:INFO:SubProcess create_model() called ==================================
2023-07-22 18:51:02,895:INFO:Initializing create_model()
2023-07-22 18:51:02,895:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E4101300>, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:51:02,895:INFO:Checking exceptions
2023-07-22 18:51:02,895:INFO:Importing libraries
2023-07-22 18:51:02,895:INFO:Copying training dataset
2023-07-22 18:51:02,905:INFO:Defining folds
2023-07-22 18:51:02,905:INFO:Declaring metric variables
2023-07-22 18:51:02,910:INFO:Importing untrained model
2023-07-22 18:51:02,916:INFO:Dummy Classifier Imported successfully
2023-07-22 18:51:02,930:INFO:Starting cross validation
2023-07-22 18:51:03,170:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-22 18:51:03,989:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:05,891:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:06,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:06,016:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:06,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:06,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:06,206:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:06,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:06,384:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:06,494:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-22 18:51:28,023:INFO:Calculating mean and std
2023-07-22 18:51:28,024:INFO:Creating metrics dataframe
2023-07-22 18:51:30,546:INFO:Uploading results into container
2023-07-22 18:51:30,547:INFO:Uploading model into container now
2023-07-22 18:51:30,549:INFO:_master_model_container: 15
2023-07-22 18:51:30,549:INFO:_display_container: 2
2023-07-22 18:51:30,549:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-22 18:51:30,549:INFO:create_model() successfully completed......................................
2023-07-22 18:51:30,663:INFO:SubProcess create_model() end ==================================
2023-07-22 18:51:30,663:INFO:Creating metrics dataframe
2023-07-22 18:51:30,690:INFO:Initializing create_model()
2023-07-22 18:51:30,690:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:51:30,690:INFO:Checking exceptions
2023-07-22 18:51:30,692:INFO:Importing libraries
2023-07-22 18:51:30,692:INFO:Copying training dataset
2023-07-22 18:51:30,701:INFO:Defining folds
2023-07-22 18:51:30,702:INFO:Declaring metric variables
2023-07-22 18:51:30,702:INFO:Importing untrained model
2023-07-22 18:51:30,702:INFO:Declaring custom model
2023-07-22 18:51:30,702:INFO:Random Forest Classifier Imported successfully
2023-07-22 18:51:30,921:INFO:Cross validation set to False
2023-07-22 18:51:30,921:INFO:Fitting Model
2023-07-22 18:51:33,447:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-22 18:51:33,447:INFO:create_model() successfully completed......................................
2023-07-22 18:51:33,577:INFO:Initializing create_model()
2023-07-22 18:51:33,578:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:51:33,578:INFO:Checking exceptions
2023-07-22 18:51:33,581:INFO:Importing libraries
2023-07-22 18:51:33,581:INFO:Copying training dataset
2023-07-22 18:51:33,594:INFO:Defining folds
2023-07-22 18:51:33,595:INFO:Declaring metric variables
2023-07-22 18:51:33,595:INFO:Importing untrained model
2023-07-22 18:51:33,596:INFO:Declaring custom model
2023-07-22 18:51:33,596:INFO:Gradient Boosting Classifier Imported successfully
2023-07-22 18:51:33,835:INFO:Cross validation set to False
2023-07-22 18:51:33,836:INFO:Fitting Model
2023-07-22 18:51:36,332:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-22 18:51:36,332:INFO:create_model() successfully completed......................................
2023-07-22 18:51:36,450:INFO:Initializing create_model()
2023-07-22 18:51:36,450:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E41F5B40>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-22 18:51:36,451:INFO:Checking exceptions
2023-07-22 18:51:36,452:INFO:Importing libraries
2023-07-22 18:51:36,453:INFO:Copying training dataset
2023-07-22 18:51:36,461:INFO:Defining folds
2023-07-22 18:51:36,461:INFO:Declaring metric variables
2023-07-22 18:51:36,461:INFO:Importing untrained model
2023-07-22 18:51:36,462:INFO:Declaring custom model
2023-07-22 18:51:36,462:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-22 18:51:36,681:INFO:Cross validation set to False
2023-07-22 18:51:36,681:INFO:Fitting Model
2023-07-22 18:51:39,148:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-22 18:51:39,148:INFO:create_model() successfully completed......................................
2023-07-22 18:51:39,303:INFO:_master_model_container: 15
2023-07-22 18:51:39,303:INFO:_display_container: 2
2023-07-22 18:51:39,305:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)]
2023-07-22 18:51:39,305:INFO:compare_models() successfully completed......................................
2023-07-22 18:51:53,561:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:35: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,566:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:54: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:63: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:69: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:77: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:5: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:10: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:15: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,572:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:20: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,572:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:363: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,573:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:385: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,573:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:428: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,573:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:439: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,585:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:186: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,585:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:197: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,590:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_image.py:175: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,608:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\explainers\_partition.py:676: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-22 18:51:53,618:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-22 18:51:53,618:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-22 18:51:53,691:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-22 18:51:53,695:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-22 18:51:53,701:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 00:58:34,887:INFO:PyCaret ClassificationExperiment
2023-07-23 00:58:34,887:INFO:Logging name: clf-default-name
2023-07-23 00:58:34,887:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-23 00:58:34,887:INFO:version 3.0.4
2023-07-23 00:58:34,887:INFO:Initializing setup()
2023-07-23 00:58:34,887:INFO:self.USI: 6de5
2023-07-23 00:58:34,887:INFO:self._variable_keys: {'X', 'fold_generator', 'USI', 'fold_groups_param', '_ml_usecase', 'exp_id', 'y_test', 'seed', 'n_jobs_param', 'html_param', 'log_plots_param', 'logging_param', 'memory', 'y_train', 'target_param', 'pipeline', 'exp_name_log', 'y', 'gpu_n_jobs_param', 'idx', 'X_test', 'fix_imbalance', 'fold_shuffle_param', 'X_train', 'gpu_param', 'is_multiclass', 'data', '_available_plots'}
2023-07-23 00:58:34,887:INFO:Checking environment
2023-07-23 00:58:34,887:INFO:python_version: 3.10.8
2023-07-23 00:58:34,887:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-23 00:58:34,887:INFO:machine: AMD64
2023-07-23 00:58:34,887:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-23 00:58:34,895:INFO:Memory: svmem(total=16505966592, available=4617281536, percent=72.0, used=11888685056, free=4617281536)
2023-07-23 00:58:34,895:INFO:Physical Core: 6
2023-07-23 00:58:34,895:INFO:Logical Core: 12
2023-07-23 00:58:34,895:INFO:Checking libraries
2023-07-23 00:58:34,895:INFO:System:
2023-07-23 00:58:34,895:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-23 00:58:34,895:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-23 00:58:34,895:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-23 00:58:34,895:INFO:PyCaret required dependencies:
2023-07-23 00:58:34,895:INFO:                 pip: 22.2.2
2023-07-23 00:58:34,895:INFO:          setuptools: 63.2.0
2023-07-23 00:58:34,895:INFO:             pycaret: 3.0.4
2023-07-23 00:58:34,895:INFO:             IPython: 8.11.0
2023-07-23 00:58:34,896:INFO:          ipywidgets: 8.0.7
2023-07-23 00:58:34,896:INFO:                tqdm: 4.64.1
2023-07-23 00:58:34,896:INFO:               numpy: 1.23.5
2023-07-23 00:58:34,896:INFO:              pandas: 1.5.3
2023-07-23 00:58:34,896:INFO:              jinja2: 3.1.2
2023-07-23 00:58:34,896:INFO:               scipy: 1.9.3
2023-07-23 00:58:34,896:INFO:              joblib: 1.2.0
2023-07-23 00:58:34,896:INFO:             sklearn: 1.2.2
2023-07-23 00:58:34,896:INFO:                pyod: 1.1.0
2023-07-23 00:58:34,896:INFO:            imblearn: 0.10.1
2023-07-23 00:58:34,896:INFO:   category_encoders: 2.6.1
2023-07-23 00:58:34,896:INFO:            lightgbm: 3.3.5
2023-07-23 00:58:34,896:INFO:               numba: 0.57.0
2023-07-23 00:58:34,896:INFO:            requests: 2.28.2
2023-07-23 00:58:34,896:INFO:          matplotlib: 3.7.1
2023-07-23 00:58:34,896:INFO:          scikitplot: 0.3.7
2023-07-23 00:58:34,896:INFO:         yellowbrick: 1.5
2023-07-23 00:58:34,896:INFO:              plotly: 5.15.0
2023-07-23 00:58:34,896:INFO:    plotly-resampler: Not installed
2023-07-23 00:58:34,896:INFO:             kaleido: 0.2.1
2023-07-23 00:58:34,896:INFO:           schemdraw: 0.15
2023-07-23 00:58:34,896:INFO:         statsmodels: 0.13.5
2023-07-23 00:58:34,896:INFO:              sktime: 0.21.0
2023-07-23 00:58:34,897:INFO:               tbats: 1.1.3
2023-07-23 00:58:34,897:INFO:            pmdarima: 2.0.3
2023-07-23 00:58:34,897:INFO:              psutil: 5.9.4
2023-07-23 00:58:34,897:INFO:          markupsafe: 2.1.2
2023-07-23 00:58:34,897:INFO:             pickle5: Not installed
2023-07-23 00:58:34,897:INFO:         cloudpickle: 2.2.1
2023-07-23 00:58:34,897:INFO:         deprecation: 2.1.0
2023-07-23 00:58:34,897:INFO:              xxhash: 3.2.0
2023-07-23 00:58:34,897:INFO:           wurlitzer: Not installed
2023-07-23 00:58:34,897:INFO:PyCaret optional dependencies:
2023-07-23 00:58:34,897:INFO:                shap: 0.41.0
2023-07-23 00:58:34,897:INFO:           interpret: 0.4.2
2023-07-23 00:58:34,897:INFO:                umap: 0.5.3
2023-07-23 00:58:34,897:INFO:    pandas_profiling: 4.1.2
2023-07-23 00:58:34,897:INFO:  explainerdashboard: Not installed
2023-07-23 00:58:34,897:INFO:             autoviz: Not installed
2023-07-23 00:58:34,898:INFO:           fairlearn: Not installed
2023-07-23 00:58:34,898:INFO:          deepchecks: Not installed
2023-07-23 00:58:34,898:INFO:             xgboost: 1.7.6
2023-07-23 00:58:34,898:INFO:            catboost: Not installed
2023-07-23 00:58:34,898:INFO:              kmodes: Not installed
2023-07-23 00:58:34,898:INFO:             mlxtend: Not installed
2023-07-23 00:58:34,898:INFO:       statsforecast: Not installed
2023-07-23 00:58:34,898:INFO:        tune_sklearn: Not installed
2023-07-23 00:58:34,898:INFO:                 ray: Not installed
2023-07-23 00:58:34,898:INFO:            hyperopt: Not installed
2023-07-23 00:58:34,898:INFO:              optuna: 3.2.0
2023-07-23 00:58:34,898:INFO:               skopt: Not installed
2023-07-23 00:58:34,898:INFO:              mlflow: 2.4.2
2023-07-23 00:58:34,898:INFO:              gradio: Not installed
2023-07-23 00:58:34,898:INFO:             fastapi: 0.95.2
2023-07-23 00:58:34,898:INFO:             uvicorn: 0.22.0
2023-07-23 00:58:34,898:INFO:              m2cgen: Not installed
2023-07-23 00:58:34,898:INFO:           evidently: Not installed
2023-07-23 00:58:34,898:INFO:               fugue: Not installed
2023-07-23 00:58:34,898:INFO:           streamlit: Not installed
2023-07-23 00:58:34,898:INFO:             prophet: Not installed
2023-07-23 00:58:34,898:INFO:None
2023-07-23 00:58:34,898:INFO:Set up data.
2023-07-23 00:58:34,925:INFO:Set up train/test split.
2023-07-23 00:58:34,925:INFO:Set up data.
2023-07-23 00:58:34,946:INFO:Set up index.
2023-07-23 00:58:34,947:INFO:Set up folding strategy.
2023-07-23 00:58:34,948:INFO:Assigning column types.
2023-07-23 00:58:34,956:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-23 00:58:35,012:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,013:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,045:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,047:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:35,104:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,105:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,139:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,143:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:35,144:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-23 00:58:35,205:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,241:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,245:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:35,297:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,328:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,331:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:35,332:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-23 00:58:35,419:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,421:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:35,511:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,514:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:35,516:INFO:Preparing preprocessing pipeline...
2023-07-23 00:58:35,517:INFO:Set up iterative imputation.
2023-07-23 00:58:35,517:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,523:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,527:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,601:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-23 00:58:35,655:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,659:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:35,748:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,751:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:35,794:INFO:Set up encoding of categorical features.
2023-07-23 00:58:35,794:INFO:Set up imbalanced handling.
2023-07-23 00:58:35,794:INFO:Set up feature selection.
2023-07-23 00:58:35,886:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:35,889:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:40,158:INFO:Finished creating preprocessing pipeline.
2023-07-23 00:58:40,192:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-23 00:58:40,192:INFO:Creating final display dataframe.
2023-07-23 00:58:41,499:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             49.4%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              6de5
2023-07-23 00:58:41,611:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:41,616:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:41,705:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 00:58:41,708:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 00:58:41,709:INFO:setup() successfully completed in 10.21s...............
2023-07-23 00:58:44,244:INFO:Initializing compare_models()
2023-07-23 00:58:44,244:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-23 00:58:44,244:INFO:Checking exceptions
2023-07-23 00:58:44,255:INFO:Preparing display monitor
2023-07-23 00:58:44,290:INFO:Initializing Logistic Regression
2023-07-23 00:58:44,290:INFO:Total runtime is 0.0 minutes
2023-07-23 00:58:44,295:INFO:SubProcess create_model() called ==================================
2023-07-23 00:58:44,296:INFO:Initializing create_model()
2023-07-23 00:58:44,296:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 00:58:44,296:INFO:Checking exceptions
2023-07-23 00:58:44,296:INFO:Importing libraries
2023-07-23 00:58:44,296:INFO:Copying training dataset
2023-07-23 00:58:44,303:INFO:Defining folds
2023-07-23 00:58:44,303:INFO:Declaring metric variables
2023-07-23 00:58:44,307:INFO:Importing untrained model
2023-07-23 00:58:44,312:INFO:Logistic Regression Imported successfully
2023-07-23 00:58:44,320:INFO:Starting cross validation
2023-07-23 00:58:44,422:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 00:58:51,512:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,521:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,532:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,545:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,553:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,555:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,879:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,896:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,900:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,921:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,926:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:51,997:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,007:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,014:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,037:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,215:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,238:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,249:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,250:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,267:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,448:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,465:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,475:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,502:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,535:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,582:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,590:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,610:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,611:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,611:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,908:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,917:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,942:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:52,982:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,205:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,255:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,290:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,295:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,308:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,357:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,364:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,389:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,536:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,605:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,612:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,616:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,633:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,816:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,821:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,905:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,967:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,971:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,978:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:53,989:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,023:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,261:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,295:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,301:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,319:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,333:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,347:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,355:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,358:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,631:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,644:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,678:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,692:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,692:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,735:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,755:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,771:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:54,851:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,010:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,022:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,048:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,064:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,065:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,136:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,174:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,250:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,370:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,402:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,427:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,430:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,472:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,493:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,551:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,582:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,592:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,758:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,869:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:55,943:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,018:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,053:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,054:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,074:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,134:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,194:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,206:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,243:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,401:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,477:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,479:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,505:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,537:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,860:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,870:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,943:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:56,959:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:57,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:57,308:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:58:57,327:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 00:59:26,747:INFO:Calculating mean and std
2023-07-23 00:59:26,749:INFO:Creating metrics dataframe
2023-07-23 00:59:29,868:INFO:Uploading results into container
2023-07-23 00:59:29,869:INFO:Uploading model into container now
2023-07-23 00:59:29,870:INFO:_master_model_container: 1
2023-07-23 00:59:29,870:INFO:_display_container: 2
2023-07-23 00:59:29,870:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-23 00:59:29,870:INFO:create_model() successfully completed......................................
2023-07-23 00:59:30,347:INFO:SubProcess create_model() end ==================================
2023-07-23 00:59:30,348:INFO:Creating metrics dataframe
2023-07-23 00:59:30,360:INFO:Initializing K Neighbors Classifier
2023-07-23 00:59:30,361:INFO:Total runtime is 0.7678452134132385 minutes
2023-07-23 00:59:30,366:INFO:SubProcess create_model() called ==================================
2023-07-23 00:59:30,367:INFO:Initializing create_model()
2023-07-23 00:59:30,367:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 00:59:30,367:INFO:Checking exceptions
2023-07-23 00:59:30,367:INFO:Importing libraries
2023-07-23 00:59:30,367:INFO:Copying training dataset
2023-07-23 00:59:30,379:INFO:Defining folds
2023-07-23 00:59:30,379:INFO:Declaring metric variables
2023-07-23 00:59:30,384:INFO:Importing untrained model
2023-07-23 00:59:30,390:INFO:K Neighbors Classifier Imported successfully
2023-07-23 00:59:30,400:INFO:Starting cross validation
2023-07-23 00:59:30,571:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 00:59:32,720:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 00:59:32,842:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:00:00,191:INFO:Calculating mean and std
2023-07-23 01:00:00,193:INFO:Creating metrics dataframe
2023-07-23 01:00:03,309:INFO:Uploading results into container
2023-07-23 01:00:03,310:INFO:Uploading model into container now
2023-07-23 01:00:03,310:INFO:_master_model_container: 2
2023-07-23 01:00:03,311:INFO:_display_container: 2
2023-07-23 01:00:03,311:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-23 01:00:03,312:INFO:create_model() successfully completed......................................
2023-07-23 01:00:03,681:INFO:SubProcess create_model() end ==================================
2023-07-23 01:00:03,681:INFO:Creating metrics dataframe
2023-07-23 01:00:03,696:INFO:Initializing Naive Bayes
2023-07-23 01:00:03,696:INFO:Total runtime is 1.3234231114387511 minutes
2023-07-23 01:00:03,702:INFO:SubProcess create_model() called ==================================
2023-07-23 01:00:03,702:INFO:Initializing create_model()
2023-07-23 01:00:03,702:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:00:03,703:INFO:Checking exceptions
2023-07-23 01:00:03,703:INFO:Importing libraries
2023-07-23 01:00:03,703:INFO:Copying training dataset
2023-07-23 01:00:03,757:INFO:Defining folds
2023-07-23 01:00:03,757:INFO:Declaring metric variables
2023-07-23 01:00:03,759:INFO:Importing untrained model
2023-07-23 01:00:03,772:INFO:Naive Bayes Imported successfully
2023-07-23 01:00:03,780:INFO:Starting cross validation
2023-07-23 01:00:03,890:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:00:33,663:INFO:Calculating mean and std
2023-07-23 01:00:33,664:INFO:Creating metrics dataframe
2023-07-23 01:00:36,802:INFO:Uploading results into container
2023-07-23 01:00:36,802:INFO:Uploading model into container now
2023-07-23 01:00:36,802:INFO:_master_model_container: 3
2023-07-23 01:00:36,802:INFO:_display_container: 2
2023-07-23 01:00:36,802:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-23 01:00:36,802:INFO:create_model() successfully completed......................................
2023-07-23 01:00:37,176:INFO:SubProcess create_model() end ==================================
2023-07-23 01:00:37,176:INFO:Creating metrics dataframe
2023-07-23 01:00:37,192:INFO:Initializing Decision Tree Classifier
2023-07-23 01:00:37,192:INFO:Total runtime is 1.881698453426361 minutes
2023-07-23 01:00:37,197:INFO:SubProcess create_model() called ==================================
2023-07-23 01:00:37,197:INFO:Initializing create_model()
2023-07-23 01:00:37,198:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:00:37,198:INFO:Checking exceptions
2023-07-23 01:00:37,198:INFO:Importing libraries
2023-07-23 01:00:37,198:INFO:Copying training dataset
2023-07-23 01:00:37,207:INFO:Defining folds
2023-07-23 01:00:37,207:INFO:Declaring metric variables
2023-07-23 01:00:37,213:INFO:Importing untrained model
2023-07-23 01:00:37,218:INFO:Decision Tree Classifier Imported successfully
2023-07-23 01:00:37,229:INFO:Starting cross validation
2023-07-23 01:00:37,336:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:00:39,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:00:39,957:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:00:40,079:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:00:40,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:00:40,183:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:08,101:INFO:Calculating mean and std
2023-07-23 01:01:08,101:INFO:Creating metrics dataframe
2023-07-23 01:01:11,251:INFO:Uploading results into container
2023-07-23 01:01:11,252:INFO:Uploading model into container now
2023-07-23 01:01:11,253:INFO:_master_model_container: 4
2023-07-23 01:01:11,253:INFO:_display_container: 2
2023-07-23 01:01:11,254:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-23 01:01:11,254:INFO:create_model() successfully completed......................................
2023-07-23 01:01:11,621:INFO:SubProcess create_model() end ==================================
2023-07-23 01:01:11,645:INFO:Creating metrics dataframe
2023-07-23 01:01:11,656:INFO:Initializing SVM - Linear Kernel
2023-07-23 01:01:11,656:INFO:Total runtime is 2.456095953782399 minutes
2023-07-23 01:01:11,666:INFO:SubProcess create_model() called ==================================
2023-07-23 01:01:11,666:INFO:Initializing create_model()
2023-07-23 01:01:11,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:01:11,667:INFO:Checking exceptions
2023-07-23 01:01:11,667:INFO:Importing libraries
2023-07-23 01:01:11,667:INFO:Copying training dataset
2023-07-23 01:01:11,677:INFO:Defining folds
2023-07-23 01:01:11,677:INFO:Declaring metric variables
2023-07-23 01:01:11,688:INFO:Importing untrained model
2023-07-23 01:01:11,694:INFO:SVM - Linear Kernel Imported successfully
2023-07-23 01:01:11,705:INFO:Starting cross validation
2023-07-23 01:01:11,802:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:01:14,033:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:14,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:14,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:14,209:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:14,216:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:14,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:14,281:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:14,288:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:14,301:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:14,316:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:14,361:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:14,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:01:41,259:INFO:Calculating mean and std
2023-07-23 01:01:41,261:INFO:Creating metrics dataframe
2023-07-23 01:01:44,483:INFO:Uploading results into container
2023-07-23 01:01:44,484:INFO:Uploading model into container now
2023-07-23 01:01:44,485:INFO:_master_model_container: 5
2023-07-23 01:01:44,485:INFO:_display_container: 2
2023-07-23 01:01:44,486:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-23 01:01:44,486:INFO:create_model() successfully completed......................................
2023-07-23 01:01:44,830:INFO:SubProcess create_model() end ==================================
2023-07-23 01:01:44,830:INFO:Creating metrics dataframe
2023-07-23 01:01:44,847:INFO:Initializing Ridge Classifier
2023-07-23 01:01:44,847:INFO:Total runtime is 3.0092745105425514 minutes
2023-07-23 01:01:44,851:INFO:SubProcess create_model() called ==================================
2023-07-23 01:01:44,851:INFO:Initializing create_model()
2023-07-23 01:01:44,851:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:01:44,851:INFO:Checking exceptions
2023-07-23 01:01:44,851:INFO:Importing libraries
2023-07-23 01:01:44,851:INFO:Copying training dataset
2023-07-23 01:01:44,868:INFO:Defining folds
2023-07-23 01:01:44,868:INFO:Declaring metric variables
2023-07-23 01:01:44,876:INFO:Importing untrained model
2023-07-23 01:01:44,879:INFO:Ridge Classifier Imported successfully
2023-07-23 01:01:44,892:INFO:Starting cross validation
2023-07-23 01:01:44,995:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:01:46,835:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:46,840:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,091:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:47,135:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,142:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:47,168:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,170:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,306:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:47,316:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,346:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:01:47,352:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,358:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:01:47,409:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:02:13,989:INFO:Calculating mean and std
2023-07-23 01:02:13,991:INFO:Creating metrics dataframe
2023-07-23 01:02:17,179:INFO:Uploading results into container
2023-07-23 01:02:17,179:INFO:Uploading model into container now
2023-07-23 01:02:17,179:INFO:_master_model_container: 6
2023-07-23 01:02:17,179:INFO:_display_container: 2
2023-07-23 01:02:17,179:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-23 01:02:17,179:INFO:create_model() successfully completed......................................
2023-07-23 01:02:17,522:INFO:SubProcess create_model() end ==================================
2023-07-23 01:02:17,522:INFO:Creating metrics dataframe
2023-07-23 01:02:17,538:INFO:Initializing Random Forest Classifier
2023-07-23 01:02:17,538:INFO:Total runtime is 3.5541222532590226 minutes
2023-07-23 01:02:17,542:INFO:SubProcess create_model() called ==================================
2023-07-23 01:02:17,542:INFO:Initializing create_model()
2023-07-23 01:02:17,542:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:02:17,542:INFO:Checking exceptions
2023-07-23 01:02:17,543:INFO:Importing libraries
2023-07-23 01:02:17,543:INFO:Copying training dataset
2023-07-23 01:02:17,552:INFO:Defining folds
2023-07-23 01:02:17,552:INFO:Declaring metric variables
2023-07-23 01:02:17,557:INFO:Importing untrained model
2023-07-23 01:02:17,563:INFO:Random Forest Classifier Imported successfully
2023-07-23 01:02:17,571:INFO:Starting cross validation
2023-07-23 01:02:17,680:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:02:25,175:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:25,388:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:25,679:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:25,687:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:25,748:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:25,819:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:26,350:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:26,401:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:26,480:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:26,631:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:02:53,126:INFO:Calculating mean and std
2023-07-23 01:02:53,127:INFO:Creating metrics dataframe
2023-07-23 01:02:56,252:INFO:Uploading results into container
2023-07-23 01:02:56,254:INFO:Uploading model into container now
2023-07-23 01:02:56,254:INFO:_master_model_container: 7
2023-07-23 01:02:56,254:INFO:_display_container: 2
2023-07-23 01:02:56,255:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-23 01:02:56,255:INFO:create_model() successfully completed......................................
2023-07-23 01:02:56,617:INFO:SubProcess create_model() end ==================================
2023-07-23 01:02:56,617:INFO:Creating metrics dataframe
2023-07-23 01:02:56,635:INFO:Initializing Quadratic Discriminant Analysis
2023-07-23 01:02:56,635:INFO:Total runtime is 4.205737396081288 minutes
2023-07-23 01:02:56,641:INFO:SubProcess create_model() called ==================================
2023-07-23 01:02:56,641:INFO:Initializing create_model()
2023-07-23 01:02:56,641:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:02:56,641:INFO:Checking exceptions
2023-07-23 01:02:56,642:INFO:Importing libraries
2023-07-23 01:02:56,642:INFO:Copying training dataset
2023-07-23 01:02:56,654:INFO:Defining folds
2023-07-23 01:02:56,654:INFO:Declaring metric variables
2023-07-23 01:02:56,659:INFO:Importing untrained model
2023-07-23 01:02:56,665:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-23 01:02:56,674:INFO:Starting cross validation
2023-07-23 01:02:56,776:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:03:26,928:INFO:Calculating mean and std
2023-07-23 01:03:26,929:INFO:Creating metrics dataframe
2023-07-23 01:03:30,222:INFO:Uploading results into container
2023-07-23 01:03:30,227:INFO:Uploading model into container now
2023-07-23 01:03:30,227:INFO:_master_model_container: 8
2023-07-23 01:03:30,227:INFO:_display_container: 2
2023-07-23 01:03:30,229:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-23 01:03:30,229:INFO:create_model() successfully completed......................................
2023-07-23 01:03:30,584:INFO:SubProcess create_model() end ==================================
2023-07-23 01:03:30,584:INFO:Creating metrics dataframe
2023-07-23 01:03:30,597:INFO:Initializing Ada Boost Classifier
2023-07-23 01:03:30,597:INFO:Total runtime is 4.771777037779489 minutes
2023-07-23 01:03:30,602:INFO:SubProcess create_model() called ==================================
2023-07-23 01:03:30,602:INFO:Initializing create_model()
2023-07-23 01:03:30,603:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:03:30,603:INFO:Checking exceptions
2023-07-23 01:03:30,603:INFO:Importing libraries
2023-07-23 01:03:30,604:INFO:Copying training dataset
2023-07-23 01:03:30,615:INFO:Defining folds
2023-07-23 01:03:30,616:INFO:Declaring metric variables
2023-07-23 01:03:30,622:INFO:Importing untrained model
2023-07-23 01:03:30,627:INFO:Ada Boost Classifier Imported successfully
2023-07-23 01:03:30,638:INFO:Starting cross validation
2023-07-23 01:03:30,743:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:03:34,852:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:03:34,915:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:03:35,090:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:03:35,118:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:03:35,217:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:03:35,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:02,874:INFO:Calculating mean and std
2023-07-23 01:04:02,876:INFO:Creating metrics dataframe
2023-07-23 01:04:06,065:INFO:Uploading results into container
2023-07-23 01:04:06,067:INFO:Uploading model into container now
2023-07-23 01:04:06,067:INFO:_master_model_container: 9
2023-07-23 01:04:06,068:INFO:_display_container: 2
2023-07-23 01:04:06,068:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-23 01:04:06,068:INFO:create_model() successfully completed......................................
2023-07-23 01:04:06,405:INFO:SubProcess create_model() end ==================================
2023-07-23 01:04:06,405:INFO:Creating metrics dataframe
2023-07-23 01:04:06,417:INFO:Initializing Gradient Boosting Classifier
2023-07-23 01:04:06,417:INFO:Total runtime is 5.368771131833393 minutes
2023-07-23 01:04:06,424:INFO:SubProcess create_model() called ==================================
2023-07-23 01:04:06,424:INFO:Initializing create_model()
2023-07-23 01:04:06,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:04:06,425:INFO:Checking exceptions
2023-07-23 01:04:06,425:INFO:Importing libraries
2023-07-23 01:04:06,425:INFO:Copying training dataset
2023-07-23 01:04:06,434:INFO:Defining folds
2023-07-23 01:04:06,435:INFO:Declaring metric variables
2023-07-23 01:04:06,440:INFO:Importing untrained model
2023-07-23 01:04:06,446:INFO:Gradient Boosting Classifier Imported successfully
2023-07-23 01:04:06,457:INFO:Starting cross validation
2023-07-23 01:04:06,560:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:04:17,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:18,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:18,433:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:18,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:18,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:18,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:19,172:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:44,757:INFO:Calculating mean and std
2023-07-23 01:04:44,758:INFO:Creating metrics dataframe
2023-07-23 01:04:47,959:INFO:Uploading results into container
2023-07-23 01:04:47,959:INFO:Uploading model into container now
2023-07-23 01:04:47,959:INFO:_master_model_container: 10
2023-07-23 01:04:47,959:INFO:_display_container: 2
2023-07-23 01:04:47,959:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-23 01:04:47,966:INFO:create_model() successfully completed......................................
2023-07-23 01:04:48,339:INFO:SubProcess create_model() end ==================================
2023-07-23 01:04:48,339:INFO:Creating metrics dataframe
2023-07-23 01:04:48,358:INFO:Initializing Linear Discriminant Analysis
2023-07-23 01:04:48,358:INFO:Total runtime is 6.0677924036979665 minutes
2023-07-23 01:04:48,363:INFO:SubProcess create_model() called ==================================
2023-07-23 01:04:48,363:INFO:Initializing create_model()
2023-07-23 01:04:48,363:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:04:48,363:INFO:Checking exceptions
2023-07-23 01:04:48,363:INFO:Importing libraries
2023-07-23 01:04:48,363:INFO:Copying training dataset
2023-07-23 01:04:48,379:INFO:Defining folds
2023-07-23 01:04:48,379:INFO:Declaring metric variables
2023-07-23 01:04:48,384:INFO:Importing untrained model
2023-07-23 01:04:48,389:INFO:Linear Discriminant Analysis Imported successfully
2023-07-23 01:04:48,401:INFO:Starting cross validation
2023-07-23 01:04:48,506:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:04:50,661:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:50,746:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:04:50,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:18,688:INFO:Calculating mean and std
2023-07-23 01:05:18,690:INFO:Creating metrics dataframe
2023-07-23 01:05:21,833:INFO:Uploading results into container
2023-07-23 01:05:21,834:INFO:Uploading model into container now
2023-07-23 01:05:21,836:INFO:_master_model_container: 11
2023-07-23 01:05:21,836:INFO:_display_container: 2
2023-07-23 01:05:21,837:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-23 01:05:21,837:INFO:create_model() successfully completed......................................
2023-07-23 01:05:22,210:INFO:SubProcess create_model() end ==================================
2023-07-23 01:05:22,210:INFO:Creating metrics dataframe
2023-07-23 01:05:22,226:INFO:Initializing Extra Trees Classifier
2023-07-23 01:05:22,226:INFO:Total runtime is 6.632256682713825 minutes
2023-07-23 01:05:22,231:INFO:SubProcess create_model() called ==================================
2023-07-23 01:05:22,232:INFO:Initializing create_model()
2023-07-23 01:05:22,232:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:05:22,232:INFO:Checking exceptions
2023-07-23 01:05:22,232:INFO:Importing libraries
2023-07-23 01:05:22,232:INFO:Copying training dataset
2023-07-23 01:05:22,245:INFO:Defining folds
2023-07-23 01:05:22,245:INFO:Declaring metric variables
2023-07-23 01:05:22,250:INFO:Importing untrained model
2023-07-23 01:05:22,255:INFO:Extra Trees Classifier Imported successfully
2023-07-23 01:05:22,267:INFO:Starting cross validation
2023-07-23 01:05:22,375:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:05:24,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 01:05:24,828:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 01:05:25,117:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 01:05:25,898:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 01:05:27,032:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:27,465:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:27,617:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:27,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:27,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:27,813:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:27,817:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:28,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:28,799:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:05:55,842:INFO:Calculating mean and std
2023-07-23 01:05:55,844:INFO:Creating metrics dataframe
2023-07-23 01:05:59,015:INFO:Uploading results into container
2023-07-23 01:05:59,016:INFO:Uploading model into container now
2023-07-23 01:05:59,016:INFO:_master_model_container: 12
2023-07-23 01:05:59,016:INFO:_display_container: 2
2023-07-23 01:05:59,016:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-23 01:05:59,016:INFO:create_model() successfully completed......................................
2023-07-23 01:05:59,372:INFO:SubProcess create_model() end ==================================
2023-07-23 01:05:59,372:INFO:Creating metrics dataframe
2023-07-23 01:05:59,390:INFO:Initializing Extreme Gradient Boosting
2023-07-23 01:05:59,390:INFO:Total runtime is 7.251664412021635 minutes
2023-07-23 01:05:59,396:INFO:SubProcess create_model() called ==================================
2023-07-23 01:05:59,396:INFO:Initializing create_model()
2023-07-23 01:05:59,397:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:05:59,397:INFO:Checking exceptions
2023-07-23 01:05:59,397:INFO:Importing libraries
2023-07-23 01:05:59,397:INFO:Copying training dataset
2023-07-23 01:05:59,411:INFO:Defining folds
2023-07-23 01:05:59,412:INFO:Declaring metric variables
2023-07-23 01:05:59,416:INFO:Importing untrained model
2023-07-23 01:05:59,423:INFO:Extreme Gradient Boosting Imported successfully
2023-07-23 01:05:59,434:INFO:Starting cross validation
2023-07-23 01:05:59,539:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:06:33,633:INFO:Calculating mean and std
2023-07-23 01:06:33,634:INFO:Creating metrics dataframe
2023-07-23 01:06:36,839:INFO:Uploading results into container
2023-07-23 01:06:36,840:INFO:Uploading model into container now
2023-07-23 01:06:36,841:INFO:_master_model_container: 13
2023-07-23 01:06:36,842:INFO:_display_container: 2
2023-07-23 01:06:36,843:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-23 01:06:36,843:INFO:create_model() successfully completed......................................
2023-07-23 01:06:37,185:INFO:SubProcess create_model() end ==================================
2023-07-23 01:06:37,186:INFO:Creating metrics dataframe
2023-07-23 01:06:37,201:INFO:Initializing Light Gradient Boosting Machine
2023-07-23 01:06:37,201:INFO:Total runtime is 7.881848788261412 minutes
2023-07-23 01:06:37,206:INFO:SubProcess create_model() called ==================================
2023-07-23 01:06:37,206:INFO:Initializing create_model()
2023-07-23 01:06:37,206:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:06:37,207:INFO:Checking exceptions
2023-07-23 01:06:37,207:INFO:Importing libraries
2023-07-23 01:06:37,207:INFO:Copying training dataset
2023-07-23 01:06:37,217:INFO:Defining folds
2023-07-23 01:06:37,218:INFO:Declaring metric variables
2023-07-23 01:06:37,219:INFO:Importing untrained model
2023-07-23 01:06:37,228:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-23 01:06:37,238:INFO:Starting cross validation
2023-07-23 01:06:37,345:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:07:08,198:INFO:Calculating mean and std
2023-07-23 01:07:08,199:INFO:Creating metrics dataframe
2023-07-23 01:07:11,622:INFO:Uploading results into container
2023-07-23 01:07:11,622:INFO:Uploading model into container now
2023-07-23 01:07:11,622:INFO:_master_model_container: 14
2023-07-23 01:07:11,622:INFO:_display_container: 2
2023-07-23 01:07:11,622:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-23 01:07:11,622:INFO:create_model() successfully completed......................................
2023-07-23 01:07:11,994:INFO:SubProcess create_model() end ==================================
2023-07-23 01:07:11,995:INFO:Creating metrics dataframe
2023-07-23 01:07:12,009:INFO:Initializing Dummy Classifier
2023-07-23 01:07:12,010:INFO:Total runtime is 8.461985504627226 minutes
2023-07-23 01:07:12,014:INFO:SubProcess create_model() called ==================================
2023-07-23 01:07:12,014:INFO:Initializing create_model()
2023-07-23 01:07:12,014:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143E7F68850>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:07:12,014:INFO:Checking exceptions
2023-07-23 01:07:12,014:INFO:Importing libraries
2023-07-23 01:07:12,014:INFO:Copying training dataset
2023-07-23 01:07:12,018:INFO:Defining folds
2023-07-23 01:07:12,018:INFO:Declaring metric variables
2023-07-23 01:07:12,029:INFO:Importing untrained model
2023-07-23 01:07:12,035:INFO:Dummy Classifier Imported successfully
2023-07-23 01:07:12,045:INFO:Starting cross validation
2023-07-23 01:07:12,143:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:07:14,378:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,396:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,412:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,514:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,532:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:14,702:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:07:42,473:INFO:Calculating mean and std
2023-07-23 01:07:42,475:INFO:Creating metrics dataframe
2023-07-23 01:07:45,753:INFO:Uploading results into container
2023-07-23 01:07:45,753:INFO:Uploading model into container now
2023-07-23 01:07:45,753:INFO:_master_model_container: 15
2023-07-23 01:07:45,755:INFO:_display_container: 2
2023-07-23 01:07:45,755:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-23 01:07:45,755:INFO:create_model() successfully completed......................................
2023-07-23 01:07:46,102:INFO:SubProcess create_model() end ==================================
2023-07-23 01:07:46,102:INFO:Creating metrics dataframe
2023-07-23 01:07:46,137:INFO:Initializing create_model()
2023-07-23 01:07:46,137:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:07:46,137:INFO:Checking exceptions
2023-07-23 01:07:46,141:INFO:Importing libraries
2023-07-23 01:07:46,141:INFO:Copying training dataset
2023-07-23 01:07:46,148:INFO:Defining folds
2023-07-23 01:07:46,148:INFO:Declaring metric variables
2023-07-23 01:07:46,149:INFO:Importing untrained model
2023-07-23 01:07:46,149:INFO:Declaring custom model
2023-07-23 01:07:46,149:INFO:Gradient Boosting Classifier Imported successfully
2023-07-23 01:07:46,247:INFO:Cross validation set to False
2023-07-23 01:07:46,248:INFO:Fitting Model
2023-07-23 01:07:57,804:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-23 01:07:57,804:INFO:create_model() successfully completed......................................
2023-07-23 01:07:58,195:INFO:Initializing create_model()
2023-07-23 01:07:58,196:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:07:58,196:INFO:Checking exceptions
2023-07-23 01:07:58,198:INFO:Importing libraries
2023-07-23 01:07:58,199:INFO:Copying training dataset
2023-07-23 01:07:58,208:INFO:Defining folds
2023-07-23 01:07:58,208:INFO:Declaring metric variables
2023-07-23 01:07:58,208:INFO:Importing untrained model
2023-07-23 01:07:58,208:INFO:Declaring custom model
2023-07-23 01:07:58,209:INFO:Random Forest Classifier Imported successfully
2023-07-23 01:07:58,314:INFO:Cross validation set to False
2023-07-23 01:07:58,314:INFO:Fitting Model
2023-07-23 01:08:01,949:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-23 01:08:01,950:INFO:create_model() successfully completed......................................
2023-07-23 01:08:02,317:INFO:Initializing create_model()
2023-07-23 01:08:02,317:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143E423EDD0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:08:02,317:INFO:Checking exceptions
2023-07-23 01:08:02,321:INFO:Importing libraries
2023-07-23 01:08:02,321:INFO:Copying training dataset
2023-07-23 01:08:02,332:INFO:Defining folds
2023-07-23 01:08:02,332:INFO:Declaring metric variables
2023-07-23 01:08:02,333:INFO:Importing untrained model
2023-07-23 01:08:02,333:INFO:Declaring custom model
2023-07-23 01:08:02,334:INFO:Extra Trees Classifier Imported successfully
2023-07-23 01:08:02,436:INFO:Cross validation set to False
2023-07-23 01:08:02,436:INFO:Fitting Model
2023-07-23 01:08:05,744:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-23 01:08:05,744:INFO:create_model() successfully completed......................................
2023-07-23 01:08:06,119:INFO:_master_model_container: 15
2023-07-23 01:08:06,119:INFO:_display_container: 2
2023-07-23 01:08:06,124:INFO:[GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)]
2023-07-23 01:08:06,124:INFO:compare_models() successfully completed......................................
2023-07-23 01:33:25,373:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 01:33:25,913:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 01:33:25,913:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 01:33:25,913:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 01:33:25,913:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 01:33:25,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 01:33:25,940:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 01:33:25,942:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 01:33:26,479:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning:

This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0


2023-07-23 01:33:26,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning:

'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680


2023-07-23 01:33:31,166:INFO:PyCaret ClassificationExperiment
2023-07-23 01:33:31,166:INFO:Logging name: clf-default-name
2023-07-23 01:33:31,166:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-23 01:33:31,166:INFO:version 3.0.4
2023-07-23 01:33:31,166:INFO:Initializing setup()
2023-07-23 01:33:31,166:INFO:self.USI: 31fb
2023-07-23 01:33:31,166:INFO:self._variable_keys: {'seed', 'fold_shuffle_param', 'exp_name_log', 'data', 'gpu_n_jobs_param', 'y', '_available_plots', 'X', 'pipeline', 'exp_id', 'n_jobs_param', 'gpu_param', 'idx', 'target_param', 'log_plots_param', 'X_train', 'html_param', 'fix_imbalance', 'memory', 'is_multiclass', 'y_train', 'fold_groups_param', 'y_test', 'USI', 'logging_param', '_ml_usecase', 'X_test', 'fold_generator'}
2023-07-23 01:33:31,166:INFO:Checking environment
2023-07-23 01:33:31,166:INFO:python_version: 3.10.8
2023-07-23 01:33:31,167:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-23 01:33:31,167:INFO:machine: AMD64
2023-07-23 01:33:31,167:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-23 01:33:31,172:INFO:Memory: svmem(total=16505966592, available=3738443776, percent=77.4, used=12767522816, free=3738443776)
2023-07-23 01:33:31,172:INFO:Physical Core: 6
2023-07-23 01:33:31,172:INFO:Logical Core: 12
2023-07-23 01:33:31,172:INFO:Checking libraries
2023-07-23 01:33:31,172:INFO:System:
2023-07-23 01:33:31,173:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-23 01:33:31,173:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-23 01:33:31,173:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-23 01:33:31,173:INFO:PyCaret required dependencies:
2023-07-23 01:33:31,173:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 01:33:31,176:INFO:                 pip: 22.2.2
2023-07-23 01:33:31,176:INFO:          setuptools: 63.2.0
2023-07-23 01:33:31,176:INFO:             pycaret: 3.0.4
2023-07-23 01:33:31,176:INFO:             IPython: 8.11.0
2023-07-23 01:33:31,176:INFO:          ipywidgets: 8.0.7
2023-07-23 01:33:31,176:INFO:                tqdm: 4.64.1
2023-07-23 01:33:31,176:INFO:               numpy: 1.23.5
2023-07-23 01:33:31,176:INFO:              pandas: 1.5.3
2023-07-23 01:33:31,176:INFO:              jinja2: 3.1.2
2023-07-23 01:33:31,176:INFO:               scipy: 1.9.3
2023-07-23 01:33:31,176:INFO:              joblib: 1.2.0
2023-07-23 01:33:31,176:INFO:             sklearn: 1.2.2
2023-07-23 01:33:31,177:INFO:                pyod: 1.1.0
2023-07-23 01:33:31,177:INFO:            imblearn: 0.10.1
2023-07-23 01:33:31,177:INFO:   category_encoders: 2.6.1
2023-07-23 01:33:31,177:INFO:            lightgbm: 3.3.5
2023-07-23 01:33:31,177:INFO:               numba: 0.57.0
2023-07-23 01:33:31,177:INFO:            requests: 2.28.2
2023-07-23 01:33:31,177:INFO:          matplotlib: 3.7.1
2023-07-23 01:33:31,177:INFO:          scikitplot: 0.3.7
2023-07-23 01:33:31,177:INFO:         yellowbrick: 1.5
2023-07-23 01:33:31,177:INFO:              plotly: 5.15.0
2023-07-23 01:33:31,177:INFO:    plotly-resampler: Not installed
2023-07-23 01:33:31,177:INFO:             kaleido: 0.2.1
2023-07-23 01:33:31,177:INFO:           schemdraw: 0.15
2023-07-23 01:33:31,177:INFO:         statsmodels: 0.13.5
2023-07-23 01:33:31,177:INFO:              sktime: 0.21.0
2023-07-23 01:33:31,177:INFO:               tbats: 1.1.3
2023-07-23 01:33:31,177:INFO:            pmdarima: 2.0.3
2023-07-23 01:33:31,177:INFO:              psutil: 5.9.4
2023-07-23 01:33:31,177:INFO:          markupsafe: 2.1.2
2023-07-23 01:33:31,177:INFO:             pickle5: Not installed
2023-07-23 01:33:31,177:INFO:         cloudpickle: 2.2.1
2023-07-23 01:33:31,177:INFO:         deprecation: 2.1.0
2023-07-23 01:33:31,177:INFO:              xxhash: 3.2.0
2023-07-23 01:33:31,177:INFO:           wurlitzer: Not installed
2023-07-23 01:33:31,177:INFO:PyCaret optional dependencies:
2023-07-23 01:33:32,082:INFO:                shap: 0.41.0
2023-07-23 01:33:32,082:INFO:           interpret: 0.4.2
2023-07-23 01:33:32,082:INFO:                umap: 0.5.3
2023-07-23 01:33:32,082:INFO:    pandas_profiling: 4.1.2
2023-07-23 01:33:32,082:INFO:  explainerdashboard: Not installed
2023-07-23 01:33:32,082:INFO:             autoviz: Not installed
2023-07-23 01:33:32,082:INFO:           fairlearn: Not installed
2023-07-23 01:33:32,082:INFO:          deepchecks: Not installed
2023-07-23 01:33:32,082:INFO:             xgboost: 1.7.6
2023-07-23 01:33:32,082:INFO:            catboost: Not installed
2023-07-23 01:33:32,082:INFO:              kmodes: Not installed
2023-07-23 01:33:32,082:INFO:             mlxtend: Not installed
2023-07-23 01:33:32,082:INFO:       statsforecast: Not installed
2023-07-23 01:33:32,082:INFO:        tune_sklearn: Not installed
2023-07-23 01:33:32,082:INFO:                 ray: Not installed
2023-07-23 01:33:32,082:INFO:            hyperopt: Not installed
2023-07-23 01:33:32,082:INFO:              optuna: 3.2.0
2023-07-23 01:33:32,082:INFO:               skopt: Not installed
2023-07-23 01:33:32,082:INFO:              mlflow: 2.4.2
2023-07-23 01:33:32,082:INFO:              gradio: Not installed
2023-07-23 01:33:32,084:INFO:             fastapi: 0.95.2
2023-07-23 01:33:32,084:INFO:             uvicorn: 0.22.0
2023-07-23 01:33:32,084:INFO:              m2cgen: Not installed
2023-07-23 01:33:32,084:INFO:           evidently: Not installed
2023-07-23 01:33:32,084:INFO:               fugue: Not installed
2023-07-23 01:33:32,084:INFO:           streamlit: Not installed
2023-07-23 01:33:32,084:INFO:             prophet: Not installed
2023-07-23 01:33:32,084:INFO:None
2023-07-23 01:33:32,084:INFO:Set up data.
2023-07-23 01:33:32,110:INFO:Set up train/test split.
2023-07-23 01:33:32,110:INFO:Set up data.
2023-07-23 01:33:32,130:INFO:Set up index.
2023-07-23 01:33:32,131:INFO:Set up folding strategy.
2023-07-23 01:33:32,131:INFO:Assigning column types.
2023-07-23 01:33:32,138:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-23 01:33:32,198:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,202:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,255:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:32,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:32,326:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,327:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,371:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:32,374:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:32,376:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-23 01:33:32,440:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,480:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:32,484:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:32,548:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,597:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:32,602:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:32,602:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-23 01:33:32,722:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:32,726:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:32,848:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:32,852:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:32,854:INFO:Preparing preprocessing pipeline...
2023-07-23 01:33:32,856:INFO:Set up iterative imputation.
2023-07-23 01:33:32,856:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,861:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,866:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-23 01:33:32,969:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-23 01:33:33,023:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:33,026:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:33,112:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:33,117:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:33,145:INFO:Set up encoding of categorical features.
2023-07-23 01:33:33,145:INFO:Set up imbalanced handling.
2023-07-23 01:33:33,145:INFO:Set up feature selection.
2023-07-23 01:33:33,235:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:33,239:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:33,287:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:33,462:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:33,617:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:33,805:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:33,962:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:34,121:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:34,285:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:34,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:34,594:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:34,767:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:34,924:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:35,081:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:35,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:35,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 01:33:36,542:INFO:Finished creating preprocessing pipeline.
2023-07-23 01:33:36,569:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-23 01:33:36,569:INFO:Creating final display dataframe.
2023-07-23 01:33:37,654:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             52.3%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              31fb
2023-07-23 01:33:37,757:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:37,760:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:37,845:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 01:33:37,848:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 01:33:37,849:INFO:setup() successfully completed in 9.34s...............
2023-07-23 01:33:37,917:INFO:Initializing compare_models()
2023-07-23 01:33:37,917:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-23 01:33:37,917:INFO:Checking exceptions
2023-07-23 01:33:37,924:INFO:Preparing display monitor
2023-07-23 01:33:37,954:INFO:Initializing Logistic Regression
2023-07-23 01:33:37,954:INFO:Total runtime is 0.0 minutes
2023-07-23 01:33:37,959:INFO:SubProcess create_model() called ==================================
2023-07-23 01:33:37,959:INFO:Initializing create_model()
2023-07-23 01:33:37,960:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:33:37,960:INFO:Checking exceptions
2023-07-23 01:33:37,960:INFO:Importing libraries
2023-07-23 01:33:37,960:INFO:Copying training dataset
2023-07-23 01:33:37,966:INFO:Defining folds
2023-07-23 01:33:37,966:INFO:Declaring metric variables
2023-07-23 01:33:37,970:INFO:Importing untrained model
2023-07-23 01:33:37,977:INFO:Logistic Regression Imported successfully
2023-07-23 01:33:37,985:INFO:Starting cross validation
2023-07-23 01:33:38,086:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:33:40,190:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,208:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,213:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,218:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,309:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,385:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,403:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,434:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,599:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,602:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,610:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,706:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,977:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:40,995:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,019:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,045:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,083:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,163:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,326:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,399:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,400:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,402:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,415:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,447:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,478:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,547:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,679:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,720:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,725:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,744:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,754:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,769:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,828:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,857:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,887:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:41,996:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,040:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,085:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,116:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,153:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,192:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,213:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,226:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,389:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,449:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,475:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,510:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,555:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,580:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,600:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,644:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,714:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,732:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,764:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,796:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,810:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,838:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,917:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,922:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:42,957:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,032:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,050:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,126:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,207:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,269:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,305:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,407:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,472:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,512:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,526:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,607:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,658:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,697:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,790:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,866:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,915:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,987:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:43,990:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,040:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,081:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,084:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,156:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,169:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,220:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,254:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,268:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,349:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,440:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,513:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,519:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,634:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,646:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,729:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,747:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,784:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,925:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:44,987:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,043:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,044:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,121:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,137:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,185:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,222:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,234:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,304:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,394:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:33:45,430:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 01:34:15,196:INFO:Calculating mean and std
2023-07-23 01:34:15,197:INFO:Creating metrics dataframe
2023-07-23 01:34:18,410:INFO:Uploading results into container
2023-07-23 01:34:18,411:INFO:Uploading model into container now
2023-07-23 01:34:18,412:INFO:_master_model_container: 1
2023-07-23 01:34:18,412:INFO:_display_container: 2
2023-07-23 01:34:18,413:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-23 01:34:18,413:INFO:create_model() successfully completed......................................
2023-07-23 01:34:18,536:INFO:SubProcess create_model() end ==================================
2023-07-23 01:34:18,537:INFO:Creating metrics dataframe
2023-07-23 01:34:18,545:INFO:Initializing K Neighbors Classifier
2023-07-23 01:34:18,545:INFO:Total runtime is 0.6765129685401916 minutes
2023-07-23 01:34:18,550:INFO:SubProcess create_model() called ==================================
2023-07-23 01:34:18,550:INFO:Initializing create_model()
2023-07-23 01:34:18,550:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:34:18,550:INFO:Checking exceptions
2023-07-23 01:34:18,551:INFO:Importing libraries
2023-07-23 01:34:18,551:INFO:Copying training dataset
2023-07-23 01:34:18,559:INFO:Defining folds
2023-07-23 01:34:18,560:INFO:Declaring metric variables
2023-07-23 01:34:18,564:INFO:Importing untrained model
2023-07-23 01:34:18,568:INFO:K Neighbors Classifier Imported successfully
2023-07-23 01:34:18,577:INFO:Starting cross validation
2023-07-23 01:34:18,668:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:34:20,829:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 01:34:20,845:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:20,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:20,870:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:20,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:20,914:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:20,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:21,515:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:48,203:INFO:Calculating mean and std
2023-07-23 01:34:48,205:INFO:Creating metrics dataframe
2023-07-23 01:34:51,827:INFO:Uploading results into container
2023-07-23 01:34:51,828:INFO:Uploading model into container now
2023-07-23 01:34:51,829:INFO:_master_model_container: 2
2023-07-23 01:34:51,829:INFO:_display_container: 2
2023-07-23 01:34:51,829:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-23 01:34:51,830:INFO:create_model() successfully completed......................................
2023-07-23 01:34:51,957:INFO:SubProcess create_model() end ==================================
2023-07-23 01:34:51,957:INFO:Creating metrics dataframe
2023-07-23 01:34:51,970:INFO:Initializing Naive Bayes
2023-07-23 01:34:51,970:INFO:Total runtime is 1.2335959196090698 minutes
2023-07-23 01:34:51,976:INFO:SubProcess create_model() called ==================================
2023-07-23 01:34:51,976:INFO:Initializing create_model()
2023-07-23 01:34:51,977:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:34:51,977:INFO:Checking exceptions
2023-07-23 01:34:51,977:INFO:Importing libraries
2023-07-23 01:34:51,977:INFO:Copying training dataset
2023-07-23 01:34:51,988:INFO:Defining folds
2023-07-23 01:34:51,988:INFO:Declaring metric variables
2023-07-23 01:34:51,993:INFO:Importing untrained model
2023-07-23 01:34:51,998:INFO:Naive Bayes Imported successfully
2023-07-23 01:34:52,007:INFO:Starting cross validation
2023-07-23 01:34:52,110:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:34:54,291:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:54,451:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:54,585:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:54,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:34:54,757:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:35:23,167:INFO:Calculating mean and std
2023-07-23 01:35:23,168:INFO:Creating metrics dataframe
2023-07-23 01:35:26,364:INFO:Uploading results into container
2023-07-23 01:35:26,365:INFO:Uploading model into container now
2023-07-23 01:35:26,366:INFO:_master_model_container: 3
2023-07-23 01:35:26,366:INFO:_display_container: 2
2023-07-23 01:35:26,366:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-23 01:35:26,367:INFO:create_model() successfully completed......................................
2023-07-23 01:35:26,485:INFO:SubProcess create_model() end ==================================
2023-07-23 01:35:26,485:INFO:Creating metrics dataframe
2023-07-23 01:35:26,496:INFO:Initializing Decision Tree Classifier
2023-07-23 01:35:26,496:INFO:Total runtime is 1.8090197046597798 minutes
2023-07-23 01:35:26,500:INFO:SubProcess create_model() called ==================================
2023-07-23 01:35:26,501:INFO:Initializing create_model()
2023-07-23 01:35:26,501:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:35:26,501:INFO:Checking exceptions
2023-07-23 01:35:26,501:INFO:Importing libraries
2023-07-23 01:35:26,501:INFO:Copying training dataset
2023-07-23 01:35:26,510:INFO:Defining folds
2023-07-23 01:35:26,510:INFO:Declaring metric variables
2023-07-23 01:35:26,515:INFO:Importing untrained model
2023-07-23 01:35:26,521:INFO:Decision Tree Classifier Imported successfully
2023-07-23 01:35:26,531:INFO:Starting cross validation
2023-07-23 01:35:26,636:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:35:29,005:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:35:56,026:INFO:Calculating mean and std
2023-07-23 01:35:56,027:INFO:Creating metrics dataframe
2023-07-23 01:35:59,062:INFO:Uploading results into container
2023-07-23 01:35:59,063:INFO:Uploading model into container now
2023-07-23 01:35:59,064:INFO:_master_model_container: 4
2023-07-23 01:35:59,064:INFO:_display_container: 2
2023-07-23 01:35:59,064:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-23 01:35:59,064:INFO:create_model() successfully completed......................................
2023-07-23 01:35:59,183:INFO:SubProcess create_model() end ==================================
2023-07-23 01:35:59,183:INFO:Creating metrics dataframe
2023-07-23 01:35:59,196:INFO:Initializing SVM - Linear Kernel
2023-07-23 01:35:59,196:INFO:Total runtime is 2.354032564163208 minutes
2023-07-23 01:35:59,201:INFO:SubProcess create_model() called ==================================
2023-07-23 01:35:59,201:INFO:Initializing create_model()
2023-07-23 01:35:59,201:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:35:59,201:INFO:Checking exceptions
2023-07-23 01:35:59,201:INFO:Importing libraries
2023-07-23 01:35:59,201:INFO:Copying training dataset
2023-07-23 01:35:59,211:INFO:Defining folds
2023-07-23 01:35:59,212:INFO:Declaring metric variables
2023-07-23 01:35:59,216:INFO:Importing untrained model
2023-07-23 01:35:59,222:INFO:SVM - Linear Kernel Imported successfully
2023-07-23 01:35:59,230:INFO:Starting cross validation
2023-07-23 01:35:59,330:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:36:01,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:36:01,434:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,434:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,449:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,452:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,461:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,475:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,493:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:01,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 01:36:27,944:INFO:Calculating mean and std
2023-07-23 01:36:27,946:INFO:Creating metrics dataframe
2023-07-23 01:36:31,675:INFO:Uploading results into container
2023-07-23 01:36:31,677:INFO:Uploading model into container now
2023-07-23 01:36:31,677:INFO:_master_model_container: 5
2023-07-23 01:36:31,678:INFO:_display_container: 2
2023-07-23 01:36:31,679:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-23 01:36:31,680:INFO:create_model() successfully completed......................................
2023-07-23 01:36:31,822:INFO:SubProcess create_model() end ==================================
2023-07-23 01:36:31,822:INFO:Creating metrics dataframe
2023-07-23 01:36:31,837:INFO:Initializing Ridge Classifier
2023-07-23 01:36:31,837:INFO:Total runtime is 2.8980387608210245 minutes
2023-07-23 01:36:31,843:INFO:SubProcess create_model() called ==================================
2023-07-23 01:36:31,843:INFO:Initializing create_model()
2023-07-23 01:36:31,843:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:36:31,843:INFO:Checking exceptions
2023-07-23 01:36:31,843:INFO:Importing libraries
2023-07-23 01:36:31,843:INFO:Copying training dataset
2023-07-23 01:36:31,851:INFO:Defining folds
2023-07-23 01:36:31,851:INFO:Declaring metric variables
2023-07-23 01:36:31,857:INFO:Importing untrained model
2023-07-23 01:36:31,863:INFO:Ridge Classifier Imported successfully
2023-07-23 01:36:31,871:INFO:Starting cross validation
2023-07-23 01:36:31,972:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:36:33,800:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:33,821:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:33,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:33,846:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:33,848:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:33,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:33,890:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:33,902:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:33,986:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:36:34,017:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 01:37:00,766:INFO:Calculating mean and std
2023-07-23 01:37:00,767:INFO:Creating metrics dataframe
2023-07-23 01:37:04,526:INFO:Uploading results into container
2023-07-23 01:37:04,527:INFO:Uploading model into container now
2023-07-23 01:37:04,527:INFO:_master_model_container: 6
2023-07-23 01:37:04,528:INFO:_display_container: 2
2023-07-23 01:37:04,529:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-23 01:37:04,529:INFO:create_model() successfully completed......................................
2023-07-23 01:37:04,650:INFO:SubProcess create_model() end ==================================
2023-07-23 01:37:04,650:INFO:Creating metrics dataframe
2023-07-23 01:37:04,660:INFO:Initializing Random Forest Classifier
2023-07-23 01:37:04,660:INFO:Total runtime is 3.4450964252154033 minutes
2023-07-23 01:37:04,665:INFO:SubProcess create_model() called ==================================
2023-07-23 01:37:04,665:INFO:Initializing create_model()
2023-07-23 01:37:04,666:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:37:04,666:INFO:Checking exceptions
2023-07-23 01:37:04,666:INFO:Importing libraries
2023-07-23 01:37:04,666:INFO:Copying training dataset
2023-07-23 01:37:04,675:INFO:Defining folds
2023-07-23 01:37:04,676:INFO:Declaring metric variables
2023-07-23 01:37:04,680:INFO:Importing untrained model
2023-07-23 01:37:04,685:INFO:Random Forest Classifier Imported successfully
2023-07-23 01:37:04,693:INFO:Starting cross validation
2023-07-23 01:37:04,786:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:37:08,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 01:37:12,336:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:12,348:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:12,373:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:12,374:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:12,408:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:12,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:13,590:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:13,714:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:13,849:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:41,660:INFO:Calculating mean and std
2023-07-23 01:37:41,662:INFO:Creating metrics dataframe
2023-07-23 01:37:45,168:INFO:Uploading results into container
2023-07-23 01:37:45,169:INFO:Uploading model into container now
2023-07-23 01:37:45,170:INFO:_master_model_container: 7
2023-07-23 01:37:45,170:INFO:_display_container: 2
2023-07-23 01:37:45,170:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-23 01:37:45,171:INFO:create_model() successfully completed......................................
2023-07-23 01:37:45,300:INFO:SubProcess create_model() end ==================================
2023-07-23 01:37:45,300:INFO:Creating metrics dataframe
2023-07-23 01:37:45,313:INFO:Initializing Quadratic Discriminant Analysis
2023-07-23 01:37:45,314:INFO:Total runtime is 4.122661832968394 minutes
2023-07-23 01:37:45,318:INFO:SubProcess create_model() called ==================================
2023-07-23 01:37:45,319:INFO:Initializing create_model()
2023-07-23 01:37:45,319:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:37:45,319:INFO:Checking exceptions
2023-07-23 01:37:45,319:INFO:Importing libraries
2023-07-23 01:37:45,320:INFO:Copying training dataset
2023-07-23 01:37:45,330:INFO:Defining folds
2023-07-23 01:37:45,330:INFO:Declaring metric variables
2023-07-23 01:37:45,335:INFO:Importing untrained model
2023-07-23 01:37:45,341:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-23 01:37:45,351:INFO:Starting cross validation
2023-07-23 01:37:45,454:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:37:47,522:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:47,524:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:47,809:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:48,033:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:37:48,124:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:38:20,320:INFO:Calculating mean and std
2023-07-23 01:38:20,321:INFO:Creating metrics dataframe
2023-07-23 01:38:24,040:INFO:Uploading results into container
2023-07-23 01:38:24,041:INFO:Uploading model into container now
2023-07-23 01:38:24,043:INFO:_master_model_container: 8
2023-07-23 01:38:24,043:INFO:_display_container: 2
2023-07-23 01:38:24,044:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-23 01:38:24,045:INFO:create_model() successfully completed......................................
2023-07-23 01:38:24,195:INFO:SubProcess create_model() end ==================================
2023-07-23 01:38:24,195:INFO:Creating metrics dataframe
2023-07-23 01:38:24,212:INFO:Initializing Ada Boost Classifier
2023-07-23 01:38:24,213:INFO:Total runtime is 4.77096908489863 minutes
2023-07-23 01:38:24,217:INFO:SubProcess create_model() called ==================================
2023-07-23 01:38:24,217:INFO:Initializing create_model()
2023-07-23 01:38:24,217:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:38:24,218:INFO:Checking exceptions
2023-07-23 01:38:24,218:INFO:Importing libraries
2023-07-23 01:38:24,218:INFO:Copying training dataset
2023-07-23 01:38:24,229:INFO:Defining folds
2023-07-23 01:38:24,229:INFO:Declaring metric variables
2023-07-23 01:38:24,235:INFO:Importing untrained model
2023-07-23 01:38:24,241:INFO:Ada Boost Classifier Imported successfully
2023-07-23 01:38:24,253:INFO:Starting cross validation
2023-07-23 01:38:24,359:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:38:28,942:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:38:29,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:38:29,449:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:38:29,477:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:38:29,879:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:38:30,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:38:57,713:INFO:Calculating mean and std
2023-07-23 01:38:57,714:INFO:Creating metrics dataframe
2023-07-23 01:39:01,075:INFO:Uploading results into container
2023-07-23 01:39:01,076:INFO:Uploading model into container now
2023-07-23 01:39:01,077:INFO:_master_model_container: 9
2023-07-23 01:39:01,077:INFO:_display_container: 2
2023-07-23 01:39:01,078:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-23 01:39:01,078:INFO:create_model() successfully completed......................................
2023-07-23 01:39:01,192:INFO:SubProcess create_model() end ==================================
2023-07-23 01:39:01,192:INFO:Creating metrics dataframe
2023-07-23 01:39:01,205:INFO:Initializing Gradient Boosting Classifier
2023-07-23 01:39:01,205:INFO:Total runtime is 5.387511964639027 minutes
2023-07-23 01:39:01,211:INFO:SubProcess create_model() called ==================================
2023-07-23 01:39:01,212:INFO:Initializing create_model()
2023-07-23 01:39:01,212:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:39:01,212:INFO:Checking exceptions
2023-07-23 01:39:01,212:INFO:Importing libraries
2023-07-23 01:39:01,212:INFO:Copying training dataset
2023-07-23 01:39:01,221:INFO:Defining folds
2023-07-23 01:39:01,221:INFO:Declaring metric variables
2023-07-23 01:39:01,226:INFO:Importing untrained model
2023-07-23 01:39:01,232:INFO:Gradient Boosting Classifier Imported successfully
2023-07-23 01:39:01,243:INFO:Starting cross validation
2023-07-23 01:39:01,348:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:39:12,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:12,948:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:12,969:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:13,198:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:13,249:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:13,355:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:13,368:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:13,635:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:13,803:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:14,006:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:46,460:INFO:Calculating mean and std
2023-07-23 01:39:46,461:INFO:Creating metrics dataframe
2023-07-23 01:39:50,248:INFO:Uploading results into container
2023-07-23 01:39:50,249:INFO:Uploading model into container now
2023-07-23 01:39:50,250:INFO:_master_model_container: 10
2023-07-23 01:39:50,250:INFO:_display_container: 2
2023-07-23 01:39:50,251:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-23 01:39:50,251:INFO:create_model() successfully completed......................................
2023-07-23 01:39:50,375:INFO:SubProcess create_model() end ==================================
2023-07-23 01:39:50,375:INFO:Creating metrics dataframe
2023-07-23 01:39:50,390:INFO:Initializing Linear Discriminant Analysis
2023-07-23 01:39:50,390:INFO:Total runtime is 6.2072611053784685 minutes
2023-07-23 01:39:50,397:INFO:SubProcess create_model() called ==================================
2023-07-23 01:39:50,398:INFO:Initializing create_model()
2023-07-23 01:39:50,398:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:39:50,398:INFO:Checking exceptions
2023-07-23 01:39:50,398:INFO:Importing libraries
2023-07-23 01:39:50,398:INFO:Copying training dataset
2023-07-23 01:39:50,409:INFO:Defining folds
2023-07-23 01:39:50,409:INFO:Declaring metric variables
2023-07-23 01:39:50,415:INFO:Importing untrained model
2023-07-23 01:39:50,421:INFO:Linear Discriminant Analysis Imported successfully
2023-07-23 01:39:50,431:INFO:Starting cross validation
2023-07-23 01:39:50,589:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:39:52,943:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:53,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:39:53,318:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:25,549:INFO:Calculating mean and std
2023-07-23 01:40:25,550:INFO:Creating metrics dataframe
2023-07-23 01:40:29,104:INFO:Uploading results into container
2023-07-23 01:40:29,106:INFO:Uploading model into container now
2023-07-23 01:40:29,106:INFO:_master_model_container: 11
2023-07-23 01:40:29,106:INFO:_display_container: 2
2023-07-23 01:40:29,107:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-23 01:40:29,108:INFO:create_model() successfully completed......................................
2023-07-23 01:40:29,236:INFO:SubProcess create_model() end ==================================
2023-07-23 01:40:29,236:INFO:Creating metrics dataframe
2023-07-23 01:40:29,249:INFO:Initializing Extra Trees Classifier
2023-07-23 01:40:29,249:INFO:Total runtime is 6.854912086327871 minutes
2023-07-23 01:40:29,256:INFO:SubProcess create_model() called ==================================
2023-07-23 01:40:29,257:INFO:Initializing create_model()
2023-07-23 01:40:29,257:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:40:29,258:INFO:Checking exceptions
2023-07-23 01:40:29,258:INFO:Importing libraries
2023-07-23 01:40:29,258:INFO:Copying training dataset
2023-07-23 01:40:29,269:INFO:Defining folds
2023-07-23 01:40:29,270:INFO:Declaring metric variables
2023-07-23 01:40:29,275:INFO:Importing untrained model
2023-07-23 01:40:29,281:INFO:Extra Trees Classifier Imported successfully
2023-07-23 01:40:29,295:INFO:Starting cross validation
2023-07-23 01:40:29,460:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:40:32,084:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 01:40:32,554:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 01:40:33,109:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 01:40:33,261:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 01:40:34,560:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:34,692:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:34,701:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:35,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:35,160:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:36,611:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:36,804:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:37,054:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:37,056:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:40:37,085:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:41:06,210:INFO:Calculating mean and std
2023-07-23 01:41:06,211:INFO:Creating metrics dataframe
2023-07-23 01:41:09,640:INFO:Uploading results into container
2023-07-23 01:41:09,641:INFO:Uploading model into container now
2023-07-23 01:41:09,642:INFO:_master_model_container: 12
2023-07-23 01:41:09,642:INFO:_display_container: 2
2023-07-23 01:41:09,643:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-23 01:41:09,643:INFO:create_model() successfully completed......................................
2023-07-23 01:41:09,777:INFO:SubProcess create_model() end ==================================
2023-07-23 01:41:09,777:INFO:Creating metrics dataframe
2023-07-23 01:41:09,792:INFO:Initializing Extreme Gradient Boosting
2023-07-23 01:41:09,792:INFO:Total runtime is 7.530627989768982 minutes
2023-07-23 01:41:09,797:INFO:SubProcess create_model() called ==================================
2023-07-23 01:41:09,797:INFO:Initializing create_model()
2023-07-23 01:41:09,797:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:41:09,797:INFO:Checking exceptions
2023-07-23 01:41:09,797:INFO:Importing libraries
2023-07-23 01:41:09,797:INFO:Copying training dataset
2023-07-23 01:41:09,808:INFO:Defining folds
2023-07-23 01:41:09,808:INFO:Declaring metric variables
2023-07-23 01:41:09,814:INFO:Importing untrained model
2023-07-23 01:41:09,820:INFO:Extreme Gradient Boosting Imported successfully
2023-07-23 01:41:09,831:INFO:Starting cross validation
2023-07-23 01:41:09,939:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:41:17,540:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:41:17,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:41:18,265:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:41:48,362:INFO:Calculating mean and std
2023-07-23 01:41:48,363:INFO:Creating metrics dataframe
2023-07-23 01:41:51,795:INFO:Uploading results into container
2023-07-23 01:41:51,796:INFO:Uploading model into container now
2023-07-23 01:41:51,797:INFO:_master_model_container: 13
2023-07-23 01:41:51,798:INFO:_display_container: 2
2023-07-23 01:41:51,799:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=100, n_jobs=-1, num_parallel_tree=None,
              objective='binary:logistic', predictor=None, ...)
2023-07-23 01:41:51,799:INFO:create_model() successfully completed......................................
2023-07-23 01:41:51,925:INFO:SubProcess create_model() end ==================================
2023-07-23 01:41:51,925:INFO:Creating metrics dataframe
2023-07-23 01:41:51,943:INFO:Initializing Light Gradient Boosting Machine
2023-07-23 01:41:51,943:INFO:Total runtime is 8.2331409573555 minutes
2023-07-23 01:41:51,948:INFO:SubProcess create_model() called ==================================
2023-07-23 01:41:51,948:INFO:Initializing create_model()
2023-07-23 01:41:51,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:41:51,949:INFO:Checking exceptions
2023-07-23 01:41:51,949:INFO:Importing libraries
2023-07-23 01:41:51,949:INFO:Copying training dataset
2023-07-23 01:41:51,960:INFO:Defining folds
2023-07-23 01:41:51,960:INFO:Declaring metric variables
2023-07-23 01:41:51,965:INFO:Importing untrained model
2023-07-23 01:41:51,971:INFO:Light Gradient Boosting Machine Imported successfully
2023-07-23 01:41:51,981:INFO:Starting cross validation
2023-07-23 01:41:52,105:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:41:55,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:41:55,213:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:41:55,254:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:42:26,203:INFO:Calculating mean and std
2023-07-23 01:42:26,204:INFO:Creating metrics dataframe
2023-07-23 01:42:29,651:INFO:Uploading results into container
2023-07-23 01:42:29,652:INFO:Uploading model into container now
2023-07-23 01:42:29,653:INFO:_master_model_container: 14
2023-07-23 01:42:29,653:INFO:_display_container: 2
2023-07-23 01:42:29,654:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn',
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-07-23 01:42:29,654:INFO:create_model() successfully completed......................................
2023-07-23 01:42:29,777:INFO:SubProcess create_model() end ==================================
2023-07-23 01:42:29,778:INFO:Creating metrics dataframe
2023-07-23 01:42:29,793:INFO:Initializing Dummy Classifier
2023-07-23 01:42:29,793:INFO:Total runtime is 8.863974018891653 minutes
2023-07-23 01:42:29,799:INFO:SubProcess create_model() called ==================================
2023-07-23 01:42:29,800:INFO:Initializing create_model()
2023-07-23 01:42:29,801:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000221028CCCA0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:42:29,802:INFO:Checking exceptions
2023-07-23 01:42:29,802:INFO:Importing libraries
2023-07-23 01:42:29,802:INFO:Copying training dataset
2023-07-23 01:42:29,810:INFO:Defining folds
2023-07-23 01:42:29,811:INFO:Declaring metric variables
2023-07-23 01:42:29,816:INFO:Importing untrained model
2023-07-23 01:42:29,821:INFO:Dummy Classifier Imported successfully
2023-07-23 01:42:29,831:INFO:Starting cross validation
2023-07-23 01:42:29,940:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 01:42:32,301:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,408:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,408:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:42:32,447:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,447:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:42:32,489:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,561:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 01:42:32,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,793:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,809:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:42:32,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-07-23 01:43:03,103:INFO:Calculating mean and std
2023-07-23 01:43:03,104:INFO:Creating metrics dataframe
2023-07-23 01:43:06,490:INFO:Uploading results into container
2023-07-23 01:43:06,491:INFO:Uploading model into container now
2023-07-23 01:43:06,492:INFO:_master_model_container: 15
2023-07-23 01:43:06,492:INFO:_display_container: 2
2023-07-23 01:43:06,493:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2023-07-23 01:43:06,493:INFO:create_model() successfully completed......................................
2023-07-23 01:43:06,641:INFO:SubProcess create_model() end ==================================
2023-07-23 01:43:06,642:INFO:Creating metrics dataframe
2023-07-23 01:43:06,671:INFO:Initializing create_model()
2023-07-23 01:43:06,671:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:43:06,671:INFO:Checking exceptions
2023-07-23 01:43:06,673:INFO:Importing libraries
2023-07-23 01:43:06,673:INFO:Copying training dataset
2023-07-23 01:43:06,681:INFO:Defining folds
2023-07-23 01:43:06,681:INFO:Declaring metric variables
2023-07-23 01:43:06,682:INFO:Importing untrained model
2023-07-23 01:43:06,682:INFO:Declaring custom model
2023-07-23 01:43:06,682:INFO:Gradient Boosting Classifier Imported successfully
2023-07-23 01:43:06,787:INFO:Cross validation set to False
2023-07-23 01:43:06,787:INFO:Fitting Model
2023-07-23 01:43:18,447:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-23 01:43:18,447:INFO:create_model() successfully completed......................................
2023-07-23 01:43:18,600:INFO:Initializing create_model()
2023-07-23 01:43:18,601:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:43:18,601:INFO:Checking exceptions
2023-07-23 01:43:18,604:INFO:Importing libraries
2023-07-23 01:43:18,604:INFO:Copying training dataset
2023-07-23 01:43:18,615:INFO:Defining folds
2023-07-23 01:43:18,615:INFO:Declaring metric variables
2023-07-23 01:43:18,615:INFO:Importing untrained model
2023-07-23 01:43:18,616:INFO:Declaring custom model
2023-07-23 01:43:18,616:INFO:Random Forest Classifier Imported successfully
2023-07-23 01:43:18,723:INFO:Cross validation set to False
2023-07-23 01:43:18,723:INFO:Fitting Model
2023-07-23 01:43:22,750:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-23 01:43:22,750:INFO:create_model() successfully completed......................................
2023-07-23 01:43:22,888:INFO:Initializing create_model()
2023-07-23 01:43:22,888:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002217A1EA080>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-07-23 01:43:22,889:INFO:Checking exceptions
2023-07-23 01:43:22,891:INFO:Importing libraries
2023-07-23 01:43:22,892:INFO:Copying training dataset
2023-07-23 01:43:22,900:INFO:Defining folds
2023-07-23 01:43:22,900:INFO:Declaring metric variables
2023-07-23 01:43:22,900:INFO:Importing untrained model
2023-07-23 01:43:22,900:INFO:Declaring custom model
2023-07-23 01:43:22,902:INFO:Extra Trees Classifier Imported successfully
2023-07-23 01:43:23,010:INFO:Cross validation set to False
2023-07-23 01:43:23,010:INFO:Fitting Model
2023-07-23 01:43:26,782:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-23 01:43:26,782:INFO:create_model() successfully completed......................................
2023-07-23 01:43:27,059:INFO:_master_model_container: 15
2023-07-23 01:43:27,059:INFO:_display_container: 2
2023-07-23 01:43:27,061:INFO:[GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)]
2023-07-23 01:43:27,061:INFO:compare_models() successfully completed......................................
2023-07-23 01:43:56,721:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:35: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:54: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:63: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:69: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,728:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:77: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,732:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:5: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,733:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:10: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,733:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:15: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,733:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:20: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,734:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:363: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,734:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:385: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,736:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:428: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,737:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:439: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,752:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:186: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,752:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:197: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,756:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_image.py:175: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\explainers\_partition.py:676: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 01:43:56,806:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 01:43:56,806:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 01:43:56,892:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 01:43:56,898:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 01:43:56,907:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 03:02:10,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 03:02:11,077:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 03:02:11,077:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 03:02:11,077:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 03:02:11,077:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 03:02:11,091:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 03:02:11,091:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 03:02:11,094:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 03:02:11,549:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning:

This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0


2023-07-23 03:02:11,810:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning:

'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680


2023-07-23 03:02:14,970:INFO:PyCaret ClassificationExperiment
2023-07-23 03:02:14,970:INFO:Logging name: clf-default-name
2023-07-23 03:02:14,971:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-23 03:02:14,971:INFO:version 3.0.4
2023-07-23 03:02:14,971:INFO:Initializing setup()
2023-07-23 03:02:14,971:INFO:self.USI: dc06
2023-07-23 03:02:14,971:INFO:self._variable_keys: {'X_train', 'fix_imbalance', 'y_train', 'USI', 'is_multiclass', '_ml_usecase', 'X', 'seed', 'X_test', 'exp_id', 'gpu_param', 'fold_groups_param', 'gpu_n_jobs_param', 'fold_shuffle_param', '_available_plots', 'logging_param', 'log_plots_param', 'y_test', 'target_param', 'memory', 'y', 'fold_generator', 'pipeline', 'exp_name_log', 'idx', 'data', 'n_jobs_param', 'html_param'}
2023-07-23 03:02:14,971:INFO:Checking environment
2023-07-23 03:02:14,971:INFO:python_version: 3.10.8
2023-07-23 03:02:14,971:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-23 03:02:14,971:INFO:machine: AMD64
2023-07-23 03:02:14,971:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-23 03:02:14,976:INFO:Memory: svmem(total=16505966592, available=4375121920, percent=73.5, used=12130844672, free=4375121920)
2023-07-23 03:02:14,976:INFO:Physical Core: 6
2023-07-23 03:02:14,976:INFO:Logical Core: 12
2023-07-23 03:02:14,976:INFO:Checking libraries
2023-07-23 03:02:14,976:INFO:System:
2023-07-23 03:02:14,976:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-23 03:02:14,976:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-23 03:02:14,976:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-23 03:02:14,976:INFO:PyCaret required dependencies:
2023-07-23 03:02:14,976:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 03:02:14,979:INFO:                 pip: 22.2.2
2023-07-23 03:02:14,979:INFO:          setuptools: 63.2.0
2023-07-23 03:02:14,979:INFO:             pycaret: 3.0.4
2023-07-23 03:02:14,979:INFO:             IPython: 8.11.0
2023-07-23 03:02:14,979:INFO:          ipywidgets: 8.0.7
2023-07-23 03:02:14,979:INFO:                tqdm: 4.64.1
2023-07-23 03:02:14,979:INFO:               numpy: 1.23.5
2023-07-23 03:02:14,979:INFO:              pandas: 1.5.3
2023-07-23 03:02:14,979:INFO:              jinja2: 3.1.2
2023-07-23 03:02:14,979:INFO:               scipy: 1.9.3
2023-07-23 03:02:14,979:INFO:              joblib: 1.2.0
2023-07-23 03:02:14,979:INFO:             sklearn: 1.2.2
2023-07-23 03:02:14,979:INFO:                pyod: 1.1.0
2023-07-23 03:02:14,979:INFO:            imblearn: 0.10.1
2023-07-23 03:02:14,979:INFO:   category_encoders: 2.6.1
2023-07-23 03:02:14,980:INFO:            lightgbm: 3.3.5
2023-07-23 03:02:14,980:INFO:               numba: 0.57.0
2023-07-23 03:02:14,980:INFO:            requests: 2.28.2
2023-07-23 03:02:14,980:INFO:          matplotlib: 3.7.1
2023-07-23 03:02:14,980:INFO:          scikitplot: 0.3.7
2023-07-23 03:02:14,980:INFO:         yellowbrick: 1.5
2023-07-23 03:02:14,980:INFO:              plotly: 5.15.0
2023-07-23 03:02:14,980:INFO:    plotly-resampler: Not installed
2023-07-23 03:02:14,980:INFO:             kaleido: 0.2.1
2023-07-23 03:02:14,980:INFO:           schemdraw: 0.15
2023-07-23 03:02:14,980:INFO:         statsmodels: 0.13.5
2023-07-23 03:02:14,980:INFO:              sktime: 0.21.0
2023-07-23 03:02:14,980:INFO:               tbats: 1.1.3
2023-07-23 03:02:14,980:INFO:            pmdarima: 2.0.3
2023-07-23 03:02:14,980:INFO:              psutil: 5.9.4
2023-07-23 03:02:14,980:INFO:          markupsafe: 2.1.2
2023-07-23 03:02:14,980:INFO:             pickle5: Not installed
2023-07-23 03:02:14,980:INFO:         cloudpickle: 2.2.1
2023-07-23 03:02:14,980:INFO:         deprecation: 2.1.0
2023-07-23 03:02:14,980:INFO:              xxhash: 3.2.0
2023-07-23 03:02:14,980:INFO:           wurlitzer: Not installed
2023-07-23 03:02:14,980:INFO:PyCaret optional dependencies:
2023-07-23 03:02:15,281:INFO:                shap: 0.41.0
2023-07-23 03:02:15,281:INFO:           interpret: 0.4.2
2023-07-23 03:02:15,281:INFO:                umap: 0.5.3
2023-07-23 03:02:15,281:INFO:    pandas_profiling: 4.1.2
2023-07-23 03:02:15,281:INFO:  explainerdashboard: Not installed
2023-07-23 03:02:15,281:INFO:             autoviz: Not installed
2023-07-23 03:02:15,281:INFO:           fairlearn: Not installed
2023-07-23 03:02:15,281:INFO:          deepchecks: Not installed
2023-07-23 03:02:15,281:INFO:             xgboost: 1.7.6
2023-07-23 03:02:15,281:INFO:            catboost: Not installed
2023-07-23 03:02:15,282:INFO:              kmodes: Not installed
2023-07-23 03:02:15,282:INFO:             mlxtend: Not installed
2023-07-23 03:02:15,282:INFO:       statsforecast: Not installed
2023-07-23 03:02:15,282:INFO:        tune_sklearn: Not installed
2023-07-23 03:02:15,282:INFO:                 ray: Not installed
2023-07-23 03:02:15,282:INFO:            hyperopt: Not installed
2023-07-23 03:02:15,282:INFO:              optuna: 3.2.0
2023-07-23 03:02:15,282:INFO:               skopt: Not installed
2023-07-23 03:02:15,282:INFO:              mlflow: 2.4.2
2023-07-23 03:02:15,282:INFO:              gradio: Not installed
2023-07-23 03:02:15,282:INFO:             fastapi: 0.95.2
2023-07-23 03:02:15,282:INFO:             uvicorn: 0.22.0
2023-07-23 03:02:15,282:INFO:              m2cgen: Not installed
2023-07-23 03:02:15,282:INFO:           evidently: Not installed
2023-07-23 03:02:15,282:INFO:               fugue: Not installed
2023-07-23 03:02:15,282:INFO:           streamlit: Not installed
2023-07-23 03:02:15,282:INFO:             prophet: Not installed
2023-07-23 03:02:15,282:INFO:None
2023-07-23 03:02:15,282:INFO:Set up data.
2023-07-23 03:02:15,304:INFO:Set up train/test split.
2023-07-23 03:02:15,305:INFO:Set up data.
2023-07-23 03:02:15,323:INFO:Set up index.
2023-07-23 03:02:15,324:INFO:Set up folding strategy.
2023-07-23 03:02:15,324:INFO:Assigning column types.
2023-07-23 03:02:15,331:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-23 03:02:15,387:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,390:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,429:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:15,433:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:15,493:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,494:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,529:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:15,532:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:15,532:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-23 03:02:15,583:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,615:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:15,618:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:15,673:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,704:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:15,707:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:15,707:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-23 03:02:15,794:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:15,799:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:15,888:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:15,891:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:15,893:INFO:Preparing preprocessing pipeline...
2023-07-23 03:02:15,895:INFO:Set up iterative imputation.
2023-07-23 03:02:15,895:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,900:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,905:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-23 03:02:15,987:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-23 03:02:16,043:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:16,048:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:16,132:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:16,134:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:16,166:INFO:Set up encoding of categorical features.
2023-07-23 03:02:16,166:INFO:Set up imbalanced handling.
2023-07-23 03:02:16,166:INFO:Set up feature selection.
2023-07-23 03:02:16,259:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:16,261:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:16,303:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:16,506:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:16,660:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:16,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:16,966:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:17,123:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:17,289:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:17,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:17,592:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:17,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:17,891:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:18,047:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:18,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:18,349:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:18,516:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:18,689:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:18,869:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:19,030:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:19,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:19,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:19,510:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:19,671:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:19,826:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:20,001:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:20,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:20,317:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:20,476:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:20,636:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:20,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:20,964:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:21,122:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:21,281:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:21,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:21,615:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:21,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:21,945:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:22,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:22,262:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:22,427:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:22,589:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:22,759:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:22,948:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:23,101:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:23,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:23,413:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:23,573:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:23,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:23,892:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:24,049:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:24,204:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:24,385:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:24,547:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:24,701:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:24,874:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:25,059:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:25,222:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:25,383:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:25,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:25,794:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:25,992:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:26,152:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:26,310:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:26,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:26,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:26,799:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:26,957:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:27,114:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:27,263:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:27,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:27,585:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:27,736:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:27,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:28,056:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:28,218:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:28,398:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 03:02:28,554:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning:

[IterativeImputer] Early stopping criterion not reached.


2023-07-23 03:02:31,149:INFO:Finished creating preprocessing pipeline.
2023-07-23 03:02:31,178:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-23 03:02:31,180:INFO:Creating final display dataframe.
2023-07-23 03:02:35,774:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             50.6%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              dc06
2023-07-23 03:02:35,880:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:35,883:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:35,964:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 03:02:35,967:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 03:02:35,968:INFO:setup() successfully completed in 23.33s...............
2023-07-23 03:02:36,038:INFO:Initializing compare_models()
2023-07-23 03:02:36,039:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-23 03:02:36,039:INFO:Checking exceptions
2023-07-23 03:02:36,047:INFO:Preparing display monitor
2023-07-23 03:02:36,078:INFO:Initializing Logistic Regression
2023-07-23 03:02:36,078:INFO:Total runtime is 0.0 minutes
2023-07-23 03:02:36,082:INFO:SubProcess create_model() called ==================================
2023-07-23 03:02:36,083:INFO:Initializing create_model()
2023-07-23 03:02:36,083:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:02:36,083:INFO:Checking exceptions
2023-07-23 03:02:36,083:INFO:Importing libraries
2023-07-23 03:02:36,083:INFO:Copying training dataset
2023-07-23 03:02:36,092:INFO:Defining folds
2023-07-23 03:02:36,092:INFO:Declaring metric variables
2023-07-23 03:02:36,096:INFO:Importing untrained model
2023-07-23 03:02:36,100:INFO:Logistic Regression Imported successfully
2023-07-23 03:02:36,109:INFO:Starting cross validation
2023-07-23 03:02:36,572:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:02:38,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,592:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,626:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,637:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,644:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,676:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,692:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,984:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:38,999:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,011:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,016:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,019:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,023:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,043:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,048:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,381:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,397:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,401:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,408:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,423:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,443:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,752:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,754:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,758:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,775:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,797:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,797:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:39,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,127:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,136:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,137:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,143:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,161:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,183:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,218:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,488:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,489:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,499:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,501:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,539:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,576:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,867:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,891:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,895:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,900:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,942:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:40,970:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,232:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,261:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,266:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,267:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,294:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,303:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,305:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,319:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,609:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,636:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,647:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,659:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,667:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,682:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,690:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,710:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,959:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,975:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:41,983:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,001:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,013:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,019:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,039:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,040:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,322:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,340:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,368:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,375:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,389:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,405:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,433:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,719:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,745:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,764:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,781:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,789:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,807:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,844:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,844:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:42,853:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,163:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,170:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,184:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,205:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,207:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,210:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,257:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,543:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,576:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,609:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,627:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,692:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:43,987:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,004:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,004:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,037:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,043:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,043:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,048:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,108:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,119:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,122:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,434:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,464:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,467:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,469:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,476:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,543:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,549:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,558:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,857:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,877:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,881:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,916:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,923:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:44,937:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,003:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,007:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,010:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,314:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,397:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,414:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,421:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,472:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,488:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,498:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,840:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,861:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,866:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,877:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,892:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,907:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,923:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:45,975:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,271:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,298:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,324:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,328:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,345:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,353:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,388:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,473:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,695:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,720:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,762:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,762:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,763:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,765:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,789:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,803:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:46,908:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,131:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,146:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,168:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,202:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,203:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,205:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,208:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,227:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,243:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,350:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,550:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,572:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,630:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,658:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,675:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,683:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:47,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,036:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,070:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,099:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,144:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,244:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,452:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,456:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,478:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,526:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,559:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,559:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,662:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,903:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,919:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,954:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,966:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:48,991:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,003:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,005:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,334:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,336:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,360:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,389:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,400:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,410:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,428:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,436:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,444:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,519:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,754:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,771:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,774:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,822:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,858:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,859:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,874:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:49,929:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,184:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,201:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,203:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,239:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,239:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,265:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,312:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,325:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,360:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,613:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,633:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,646:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,671:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,675:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,721:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,729:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,741:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:50,775:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,057:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,072:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,114:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,120:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,123:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,174:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,179:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,230:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,483:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,537:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,556:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,569:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,602:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,610:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,637:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,647:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,955:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:51,978:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,006:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,020:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,025:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,062:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,069:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,100:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,103:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,388:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,446:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,465:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,470:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,483:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,505:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,523:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,548:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,563:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,841:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,885:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,903:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,918:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,943:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,957:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,960:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:52,989:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,026:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,037:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,288:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,333:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,347:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,358:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,392:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,405:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,472:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,760:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,862:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,889:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,917:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,929:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,940:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:53,972:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,068:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,114:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,268:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,529:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,536:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,588:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,659:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,690:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,802:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:54,969:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,016:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,114:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,175:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,209:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,222:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,317:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,390:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,433:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,568:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,653:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,683:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,745:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,855:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:55,985:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,010:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,121:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,218:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,247:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,259:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,311:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,377:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,555:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,636:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,682:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,776:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,853:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,880:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,920:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:56,993:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,084:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,107:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,171:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,322:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,341:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,398:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,430:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,498:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,506:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,513:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,592:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,628:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,661:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,905:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,924:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:57,993:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,026:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,158:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,249:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,312:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,323:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,382:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,388:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,496:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,552:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,687:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,795:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,886:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,887:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,926:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,933:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:58,985:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,066:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,167:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,204:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,291:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,375:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,384:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,427:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,487:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,511:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,536:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,798:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,801:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,838:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,879:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:02:59,901:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,009:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,022:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,118:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,150:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,351:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,356:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,402:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,411:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,554:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,561:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,576:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,641:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,871:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,905:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,943:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:00,985:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,062:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,064:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,078:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,147:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,329:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,346:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,384:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,409:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,477:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,550:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,597:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,753:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,908:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,940:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,974:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:01,989:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,095:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,183:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,285:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,363:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,471:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,515:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,538:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,597:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,672:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,758:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,909:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:02,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,016:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,067:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,111:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,268:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,337:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,477:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,603:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,627:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,631:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,732:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,796:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,800:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,938:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:03,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,143:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,148:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,178:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,222:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,311:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,337:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,420:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,449:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,476:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,491:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,629:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,738:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,847:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:04,942:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,017:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,150:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,179:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,185:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,253:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,317:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,343:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,422:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,616:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,653:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,655:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,671:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,689:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,696:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,724:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,810:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,815:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:05,877:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,128:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,154:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,227:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,243:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,260:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,305:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,329:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,357:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,381:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,599:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,659:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,725:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,730:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,735:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,799:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,806:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,844:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:06,955:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,108:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,147:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,219:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,219:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,288:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,306:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,325:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,331:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,396:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,545:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,602:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,637:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,681:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,745:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,815:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,887:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:07,982:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,077:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,095:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,150:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,181:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,243:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,303:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,327:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,440:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,540:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,601:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,624:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,662:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,724:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,808:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:08,815:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,043:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,130:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,156:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,160:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,171:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,172:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,185:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,240:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,306:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,366:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,647:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,657:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,657:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,674:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,689:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,697:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,765:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,805:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,823:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:09,960:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,245:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,256:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,287:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,303:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,313:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,319:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,457:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,589:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,755:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,761:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,762:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,770:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,789:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,954:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:10,990:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,096:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,178:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,240:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,246:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,265:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,296:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,326:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,344:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,467:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,715:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,745:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,776:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,794:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,841:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,852:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:11,970:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,209:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,224:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,250:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,251:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,308:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,309:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,352:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,394:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,505:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,514:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,741:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,751:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,754:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,802:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,842:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,857:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:12,988:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,012:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,079:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,219:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,242:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,278:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,280:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,341:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,382:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,714:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,789:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,790:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,845:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:13,902:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,009:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,087:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,132:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,304:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,339:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,425:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,498:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:14,566:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,631:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,702:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:14,861:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,872:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,879:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:14,954:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:15,146:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:15,348:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:15,357:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:15,368:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:15,438:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:15,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:15,582:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:15,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:15,906:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:15,925:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:16,249:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:16,261:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:16,353:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:16,479:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:16,692:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:16,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 03:03:17,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:17,344:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 03:03:19,507:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:19,731:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:20,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:20,524:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.33s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:21,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:21,288:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:21,814:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:22,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:22,774:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.47s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:22,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 2.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 03:03:24,646:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:25,440:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:25,774:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.40s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:26,084:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:27,145:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:27,297:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:27,913:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:28,028:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.47s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:28,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.47s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:29,055:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 2.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 03:03:33,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-23 03:03:34,443:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:36,396:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:36,723:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:36,733:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:37,337:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:37,365:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:37,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:37,978:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:38,532:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:38,612:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:39,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.37s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:39,342:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:39,751:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:39,899:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.41s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:40,247:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:40,591:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:41,096:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 2.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 03:03:41,207:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:41,922:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:03:43,663:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.45s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:10,300:INFO:Calculating mean and std
2023-07-23 03:04:10,302:INFO:Creating metrics dataframe
2023-07-23 03:04:14,082:INFO:Uploading results into container
2023-07-23 03:04:14,083:INFO:Uploading model into container now
2023-07-23 03:04:14,083:INFO:_master_model_container: 1
2023-07-23 03:04:14,083:INFO:_display_container: 2
2023-07-23 03:04:14,084:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-23 03:04:14,084:INFO:create_model() successfully completed......................................
2023-07-23 03:04:14,211:INFO:SubProcess create_model() end ==================================
2023-07-23 03:04:14,212:INFO:Creating metrics dataframe
2023-07-23 03:04:14,221:INFO:Initializing K Neighbors Classifier
2023-07-23 03:04:14,222:INFO:Total runtime is 1.6357112050056457 minutes
2023-07-23 03:04:14,226:INFO:SubProcess create_model() called ==================================
2023-07-23 03:04:14,227:INFO:Initializing create_model()
2023-07-23 03:04:14,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:04:14,227:INFO:Checking exceptions
2023-07-23 03:04:14,227:INFO:Importing libraries
2023-07-23 03:04:14,227:INFO:Copying training dataset
2023-07-23 03:04:14,237:INFO:Defining folds
2023-07-23 03:04:14,238:INFO:Declaring metric variables
2023-07-23 03:04:14,243:INFO:Importing untrained model
2023-07-23 03:04:14,248:INFO:K Neighbors Classifier Imported successfully
2023-07-23 03:04:14,257:INFO:Starting cross validation
2023-07-23 03:04:14,790:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:04:25,809:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:26,573:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:26,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:26,879:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:27,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:27,344:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:27,493:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.32s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:27,704:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:30,502:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:04:31,266:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:03,732:INFO:Calculating mean and std
2023-07-23 03:05:03,733:INFO:Creating metrics dataframe
2023-07-23 03:05:07,694:INFO:Uploading results into container
2023-07-23 03:05:07,695:INFO:Uploading model into container now
2023-07-23 03:05:07,696:INFO:_master_model_container: 2
2023-07-23 03:05:07,696:INFO:_display_container: 2
2023-07-23 03:05:07,697:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-23 03:05:07,697:INFO:create_model() successfully completed......................................
2023-07-23 03:05:07,826:INFO:SubProcess create_model() end ==================================
2023-07-23 03:05:07,827:INFO:Creating metrics dataframe
2023-07-23 03:05:07,837:INFO:Initializing Naive Bayes
2023-07-23 03:05:07,837:INFO:Total runtime is 2.529315741856893 minutes
2023-07-23 03:05:07,841:INFO:SubProcess create_model() called ==================================
2023-07-23 03:05:07,842:INFO:Initializing create_model()
2023-07-23 03:05:07,842:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:05:07,842:INFO:Checking exceptions
2023-07-23 03:05:07,842:INFO:Importing libraries
2023-07-23 03:05:07,842:INFO:Copying training dataset
2023-07-23 03:05:07,852:INFO:Defining folds
2023-07-23 03:05:07,852:INFO:Declaring metric variables
2023-07-23 03:05:07,858:INFO:Importing untrained model
2023-07-23 03:05:07,863:INFO:Naive Bayes Imported successfully
2023-07-23 03:05:07,875:INFO:Starting cross validation
2023-07-23 03:05:08,505:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:05:17,875:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:17,875:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:17,908:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:18,081:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:18,139:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:18,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:18,251:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:18,923:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:18,940:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:19,057:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:05:51,597:INFO:Calculating mean and std
2023-07-23 03:05:51,599:INFO:Creating metrics dataframe
2023-07-23 03:05:55,342:INFO:Uploading results into container
2023-07-23 03:05:55,343:INFO:Uploading model into container now
2023-07-23 03:05:55,344:INFO:_master_model_container: 3
2023-07-23 03:05:55,344:INFO:_display_container: 2
2023-07-23 03:05:55,344:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-23 03:05:55,345:INFO:create_model() successfully completed......................................
2023-07-23 03:05:55,513:INFO:SubProcess create_model() end ==================================
2023-07-23 03:05:55,514:INFO:Creating metrics dataframe
2023-07-23 03:05:55,525:INFO:Initializing Decision Tree Classifier
2023-07-23 03:05:55,526:INFO:Total runtime is 3.324131409327189 minutes
2023-07-23 03:05:55,530:INFO:SubProcess create_model() called ==================================
2023-07-23 03:05:55,530:INFO:Initializing create_model()
2023-07-23 03:05:55,531:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:05:55,531:INFO:Checking exceptions
2023-07-23 03:05:55,531:INFO:Importing libraries
2023-07-23 03:05:55,531:INFO:Copying training dataset
2023-07-23 03:05:55,539:INFO:Defining folds
2023-07-23 03:05:55,540:INFO:Declaring metric variables
2023-07-23 03:05:55,545:INFO:Importing untrained model
2023-07-23 03:05:55,549:INFO:Decision Tree Classifier Imported successfully
2023-07-23 03:05:55,559:INFO:Starting cross validation
2023-07-23 03:05:56,100:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:06:05,606:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:05,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:05,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:05,747:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:05,799:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:05,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:05,916:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:06,112:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:06,318:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:06,380:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:39,814:INFO:Calculating mean and std
2023-07-23 03:06:39,816:INFO:Creating metrics dataframe
2023-07-23 03:06:43,654:INFO:Uploading results into container
2023-07-23 03:06:43,655:INFO:Uploading model into container now
2023-07-23 03:06:43,655:INFO:_master_model_container: 4
2023-07-23 03:06:43,655:INFO:_display_container: 2
2023-07-23 03:06:43,656:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-23 03:06:43,656:INFO:create_model() successfully completed......................................
2023-07-23 03:06:43,782:INFO:SubProcess create_model() end ==================================
2023-07-23 03:06:43,783:INFO:Creating metrics dataframe
2023-07-23 03:06:43,798:INFO:Initializing SVM - Linear Kernel
2023-07-23 03:06:43,798:INFO:Total runtime is 4.128660929203034 minutes
2023-07-23 03:06:43,805:INFO:SubProcess create_model() called ==================================
2023-07-23 03:06:43,805:INFO:Initializing create_model()
2023-07-23 03:06:43,805:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:06:43,805:INFO:Checking exceptions
2023-07-23 03:06:43,807:INFO:Importing libraries
2023-07-23 03:06:43,807:INFO:Copying training dataset
2023-07-23 03:06:43,819:INFO:Defining folds
2023-07-23 03:06:43,820:INFO:Declaring metric variables
2023-07-23 03:06:43,825:INFO:Importing untrained model
2023-07-23 03:06:43,832:INFO:SVM - Linear Kernel Imported successfully
2023-07-23 03:06:43,842:INFO:Starting cross validation
2023-07-23 03:06:44,388:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:06:55,456:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:55,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:55,863:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:55,871:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:56,293:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:56,302:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:56,518:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:56,538:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:56,631:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:56,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:56,711:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:56,719:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:56,798:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:56,806:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:56,835:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:56,856:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:56,953:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:56,962:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:06:56,996:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:06:57,006:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 03:07:32,092:INFO:Calculating mean and std
2023-07-23 03:07:32,094:INFO:Creating metrics dataframe
2023-07-23 03:07:36,993:INFO:Uploading results into container
2023-07-23 03:07:36,993:INFO:Uploading model into container now
2023-07-23 03:07:36,994:INFO:_master_model_container: 5
2023-07-23 03:07:36,994:INFO:_display_container: 2
2023-07-23 03:07:36,995:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-23 03:07:36,995:INFO:create_model() successfully completed......................................
2023-07-23 03:07:37,134:INFO:SubProcess create_model() end ==================================
2023-07-23 03:07:37,134:INFO:Creating metrics dataframe
2023-07-23 03:07:37,149:INFO:Initializing Ridge Classifier
2023-07-23 03:07:37,149:INFO:Total runtime is 5.017838025093079 minutes
2023-07-23 03:07:37,155:INFO:SubProcess create_model() called ==================================
2023-07-23 03:07:37,155:INFO:Initializing create_model()
2023-07-23 03:07:37,155:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:07:37,155:INFO:Checking exceptions
2023-07-23 03:07:37,156:INFO:Importing libraries
2023-07-23 03:07:37,156:INFO:Copying training dataset
2023-07-23 03:07:37,165:INFO:Defining folds
2023-07-23 03:07:37,166:INFO:Declaring metric variables
2023-07-23 03:07:37,171:INFO:Importing untrained model
2023-07-23 03:07:37,178:INFO:Ridge Classifier Imported successfully
2023-07-23 03:07:37,190:INFO:Starting cross validation
2023-07-23 03:07:37,892:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:07:48,727:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.47s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:48,733:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:48,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:48,797:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:49,492:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:49,500:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:49,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:49,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:49,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:49,738:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:50,339:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:50,354:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:50,769:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:50,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:50,972:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:50,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:51,891:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:51,902:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:07:51,922:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:07:51,928:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 03:08:25,188:INFO:Calculating mean and std
2023-07-23 03:08:25,191:INFO:Creating metrics dataframe
2023-07-23 03:08:28,843:INFO:Uploading results into container
2023-07-23 03:08:28,843:INFO:Uploading model into container now
2023-07-23 03:08:28,845:INFO:_master_model_container: 6
2023-07-23 03:08:28,845:INFO:_display_container: 2
2023-07-23 03:08:28,846:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-23 03:08:28,846:INFO:create_model() successfully completed......................................
2023-07-23 03:08:28,978:INFO:SubProcess create_model() end ==================================
2023-07-23 03:08:28,979:INFO:Creating metrics dataframe
2023-07-23 03:08:28,993:INFO:Initializing Random Forest Classifier
2023-07-23 03:08:28,993:INFO:Total runtime is 5.881914599736532 minutes
2023-07-23 03:08:29,001:INFO:SubProcess create_model() called ==================================
2023-07-23 03:08:29,002:INFO:Initializing create_model()
2023-07-23 03:08:29,002:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:08:29,002:INFO:Checking exceptions
2023-07-23 03:08:29,002:INFO:Importing libraries
2023-07-23 03:08:29,002:INFO:Copying training dataset
2023-07-23 03:08:29,010:INFO:Defining folds
2023-07-23 03:08:29,011:INFO:Declaring metric variables
2023-07-23 03:08:29,017:INFO:Importing untrained model
2023-07-23 03:08:29,022:INFO:Random Forest Classifier Imported successfully
2023-07-23 03:08:29,031:INFO:Starting cross validation
2023-07-23 03:08:29,582:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:08:37,079:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 03:08:37,217:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 03:08:39,130:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 03:08:40,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 03:08:46,771:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:46,946:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:47,103:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:47,700:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:48,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:48,830:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:49,094:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:49,799:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:49,990:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:08:50,073:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:23,541:INFO:Calculating mean and std
2023-07-23 03:09:23,543:INFO:Creating metrics dataframe
2023-07-23 03:09:27,505:INFO:Uploading results into container
2023-07-23 03:09:27,506:INFO:Uploading model into container now
2023-07-23 03:09:27,508:INFO:_master_model_container: 7
2023-07-23 03:09:27,508:INFO:_display_container: 2
2023-07-23 03:09:27,509:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-23 03:09:27,509:INFO:create_model() successfully completed......................................
2023-07-23 03:09:27,644:INFO:SubProcess create_model() end ==================================
2023-07-23 03:09:27,644:INFO:Creating metrics dataframe
2023-07-23 03:09:27,658:INFO:Initializing Quadratic Discriminant Analysis
2023-07-23 03:09:27,658:INFO:Total runtime is 6.85965272585551 minutes
2023-07-23 03:09:27,663:INFO:SubProcess create_model() called ==================================
2023-07-23 03:09:27,664:INFO:Initializing create_model()
2023-07-23 03:09:27,664:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:09:27,664:INFO:Checking exceptions
2023-07-23 03:09:27,664:INFO:Importing libraries
2023-07-23 03:09:27,665:INFO:Copying training dataset
2023-07-23 03:09:27,675:INFO:Defining folds
2023-07-23 03:09:27,676:INFO:Declaring metric variables
2023-07-23 03:09:27,681:INFO:Importing untrained model
2023-07-23 03:09:27,686:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-23 03:09:27,696:INFO:Starting cross validation
2023-07-23 03:09:28,334:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:09:38,937:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:39,019:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:39,251:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:39,645:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:39,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:39,957:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.29s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:40,304:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:40,790:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:41,888:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:09:41,977:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:12,081:INFO:Calculating mean and std
2023-07-23 03:10:12,083:INFO:Creating metrics dataframe
2023-07-23 03:10:18,313:INFO:Uploading results into container
2023-07-23 03:10:18,316:INFO:Uploading model into container now
2023-07-23 03:10:18,317:INFO:_master_model_container: 8
2023-07-23 03:10:18,317:INFO:_display_container: 2
2023-07-23 03:10:18,318:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-23 03:10:18,318:INFO:create_model() successfully completed......................................
2023-07-23 03:10:18,498:INFO:SubProcess create_model() end ==================================
2023-07-23 03:10:18,499:INFO:Creating metrics dataframe
2023-07-23 03:10:18,532:INFO:Initializing Ada Boost Classifier
2023-07-23 03:10:18,532:INFO:Total runtime is 7.707566130161285 minutes
2023-07-23 03:10:18,542:INFO:SubProcess create_model() called ==================================
2023-07-23 03:10:18,543:INFO:Initializing create_model()
2023-07-23 03:10:18,543:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:10:18,543:INFO:Checking exceptions
2023-07-23 03:10:18,543:INFO:Importing libraries
2023-07-23 03:10:18,543:INFO:Copying training dataset
2023-07-23 03:10:18,562:INFO:Defining folds
2023-07-23 03:10:18,563:INFO:Declaring metric variables
2023-07-23 03:10:18,574:INFO:Importing untrained model
2023-07-23 03:10:18,584:INFO:Ada Boost Classifier Imported successfully
2023-07-23 03:10:18,604:INFO:Starting cross validation
2023-07-23 03:10:19,679:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:10:45,736:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:45,951:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:45,979:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:46,181:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:46,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:46,532:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:46,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:47,676:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:47,680:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:10:48,083:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.37s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:00,660:INFO:Calculating mean and std
2023-07-23 03:12:00,664:INFO:Creating metrics dataframe
2023-07-23 03:12:09,162:INFO:Uploading results into container
2023-07-23 03:12:09,163:INFO:Uploading model into container now
2023-07-23 03:12:09,165:INFO:_master_model_container: 9
2023-07-23 03:12:09,165:INFO:_display_container: 2
2023-07-23 03:12:09,166:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-23 03:12:09,166:INFO:create_model() successfully completed......................................
2023-07-23 03:12:09,367:INFO:SubProcess create_model() end ==================================
2023-07-23 03:12:09,368:INFO:Creating metrics dataframe
2023-07-23 03:12:09,402:INFO:Initializing Gradient Boosting Classifier
2023-07-23 03:12:09,403:INFO:Total runtime is 9.555414418379465 minutes
2023-07-23 03:12:09,416:INFO:SubProcess create_model() called ==================================
2023-07-23 03:12:09,417:INFO:Initializing create_model()
2023-07-23 03:12:09,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E279FBFD0>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E2847AFE0>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 03:12:09,417:INFO:Checking exceptions
2023-07-23 03:12:09,418:INFO:Importing libraries
2023-07-23 03:12:09,418:INFO:Copying training dataset
2023-07-23 03:12:09,441:INFO:Defining folds
2023-07-23 03:12:09,455:INFO:Declaring metric variables
2023-07-23 03:12:09,465:INFO:Importing untrained model
2023-07-23 03:12:09,477:INFO:Gradient Boosting Classifier Imported successfully
2023-07-23 03:12:09,498:INFO:Starting cross validation
2023-07-23 03:12:10,740:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 03:12:52,642:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 3.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:52,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.49s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:52,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:53,012:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.37s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:53,300:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:53,388:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:54,908:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:56,658:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 4.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:58,039:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 5.44s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:12:59,705:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 5.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 03:14:20,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\generic.py:831: DeprecationWarning:

invalid escape sequence '\P'


2023-07-23 03:17:17,117:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:35: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,133:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:54: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,134:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:63: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,134:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:69: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,135:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:77: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,143:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:5: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,144:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:10: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,144:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:15: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,144:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:20: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,145:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:363: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,146:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:385: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,147:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:428: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,147:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:439: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,192:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:186: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,192:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:197: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,202:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_image.py:175: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,240:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\explainers\_partition.py:676: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 03:17:17,258:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 03:17:17,259:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 03:17:17,419:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 03:17:17,427:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 03:17:17,451:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 03:17:21,712:WARNING:[IterativeImputer] Early stopping criterion not reached.

2023-07-23 10:10:05,444:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 10:10:09,296:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 10:10:09,296:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 10:10:09,296:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 10:10:09,296:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 10:10:09,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 10:10:09,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 10:10:09,319:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 10:10:09,939:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning:

This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0


2023-07-23 10:10:10,215:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning:

'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680


2023-07-23 10:10:14,399:INFO:PyCaret ClassificationExperiment
2023-07-23 10:10:14,399:INFO:Logging name: clf-default-name
2023-07-23 10:10:14,399:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-23 10:10:14,399:INFO:version 3.0.4
2023-07-23 10:10:14,399:INFO:Initializing setup()
2023-07-23 10:10:14,399:INFO:self.USI: 554a
2023-07-23 10:10:14,399:INFO:self._variable_keys: {'_available_plots', 'idx', 'memory', '_ml_usecase', 'fold_shuffle_param', 'gpu_param', 'X_train', 'exp_id', 'data', 'seed', 'logging_param', 'X', 'y_test', 'gpu_n_jobs_param', 'exp_name_log', 'y_train', 'is_multiclass', 'log_plots_param', 'html_param', 'USI', 'fold_generator', 'fix_imbalance', 'y', 'X_test', 'target_param', 'pipeline', 'fold_groups_param', 'n_jobs_param'}
2023-07-23 10:10:14,399:INFO:Checking environment
2023-07-23 10:10:14,399:INFO:python_version: 3.10.8
2023-07-23 10:10:14,400:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-23 10:10:14,400:INFO:machine: AMD64
2023-07-23 10:10:14,400:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-23 10:10:14,404:INFO:Memory: svmem(total=16505966592, available=4596760576, percent=72.2, used=11909206016, free=4596760576)
2023-07-23 10:10:14,404:INFO:Physical Core: 6
2023-07-23 10:10:14,404:INFO:Logical Core: 12
2023-07-23 10:10:14,404:INFO:Checking libraries
2023-07-23 10:10:14,404:INFO:System:
2023-07-23 10:10:14,404:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-23 10:10:14,404:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-23 10:10:14,405:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-23 10:10:14,405:INFO:PyCaret required dependencies:
2023-07-23 10:10:14,405:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 10:10:14,407:INFO:                 pip: 22.2.2
2023-07-23 10:10:14,407:INFO:          setuptools: 63.2.0
2023-07-23 10:10:14,407:INFO:             pycaret: 3.0.4
2023-07-23 10:10:14,407:INFO:             IPython: 8.11.0
2023-07-23 10:10:14,407:INFO:          ipywidgets: 8.0.7
2023-07-23 10:10:14,407:INFO:                tqdm: 4.64.1
2023-07-23 10:10:14,407:INFO:               numpy: 1.23.5
2023-07-23 10:10:14,407:INFO:              pandas: 1.5.3
2023-07-23 10:10:14,407:INFO:              jinja2: 3.1.2
2023-07-23 10:10:14,407:INFO:               scipy: 1.9.3
2023-07-23 10:10:14,407:INFO:              joblib: 1.2.0
2023-07-23 10:10:14,408:INFO:             sklearn: 1.2.2
2023-07-23 10:10:14,408:INFO:                pyod: 1.1.0
2023-07-23 10:10:14,408:INFO:            imblearn: 0.10.1
2023-07-23 10:10:14,408:INFO:   category_encoders: 2.6.1
2023-07-23 10:10:14,408:INFO:            lightgbm: 3.3.5
2023-07-23 10:10:14,408:INFO:               numba: 0.57.0
2023-07-23 10:10:14,408:INFO:            requests: 2.28.2
2023-07-23 10:10:14,408:INFO:          matplotlib: 3.7.1
2023-07-23 10:10:14,408:INFO:          scikitplot: 0.3.7
2023-07-23 10:10:14,408:INFO:         yellowbrick: 1.5
2023-07-23 10:10:14,408:INFO:              plotly: 5.15.0
2023-07-23 10:10:14,408:INFO:    plotly-resampler: Not installed
2023-07-23 10:10:14,408:INFO:             kaleido: 0.2.1
2023-07-23 10:10:14,408:INFO:           schemdraw: 0.15
2023-07-23 10:10:14,408:INFO:         statsmodels: 0.13.5
2023-07-23 10:10:14,408:INFO:              sktime: 0.21.0
2023-07-23 10:10:14,408:INFO:               tbats: 1.1.3
2023-07-23 10:10:14,408:INFO:            pmdarima: 2.0.3
2023-07-23 10:10:14,408:INFO:              psutil: 5.9.4
2023-07-23 10:10:14,408:INFO:          markupsafe: 2.1.2
2023-07-23 10:10:14,408:INFO:             pickle5: Not installed
2023-07-23 10:10:14,408:INFO:         cloudpickle: 2.2.1
2023-07-23 10:10:14,408:INFO:         deprecation: 2.1.0
2023-07-23 10:10:14,408:INFO:              xxhash: 3.2.0
2023-07-23 10:10:14,408:INFO:           wurlitzer: Not installed
2023-07-23 10:10:14,408:INFO:PyCaret optional dependencies:
2023-07-23 10:10:15,089:INFO:                shap: 0.41.0
2023-07-23 10:10:15,089:INFO:           interpret: 0.4.2
2023-07-23 10:10:15,089:INFO:                umap: 0.5.3
2023-07-23 10:10:15,089:INFO:    pandas_profiling: 4.1.2
2023-07-23 10:10:15,089:INFO:  explainerdashboard: Not installed
2023-07-23 10:10:15,089:INFO:             autoviz: Not installed
2023-07-23 10:10:15,089:INFO:           fairlearn: Not installed
2023-07-23 10:10:15,089:INFO:          deepchecks: Not installed
2023-07-23 10:10:15,090:INFO:             xgboost: 1.7.6
2023-07-23 10:10:15,090:INFO:            catboost: Not installed
2023-07-23 10:10:15,090:INFO:              kmodes: Not installed
2023-07-23 10:10:15,090:INFO:             mlxtend: Not installed
2023-07-23 10:10:15,090:INFO:       statsforecast: Not installed
2023-07-23 10:10:15,090:INFO:        tune_sklearn: Not installed
2023-07-23 10:10:15,090:INFO:                 ray: Not installed
2023-07-23 10:10:15,090:INFO:            hyperopt: Not installed
2023-07-23 10:10:15,090:INFO:              optuna: 3.2.0
2023-07-23 10:10:15,090:INFO:               skopt: Not installed
2023-07-23 10:10:15,090:INFO:              mlflow: 2.4.2
2023-07-23 10:10:15,090:INFO:              gradio: Not installed
2023-07-23 10:10:15,090:INFO:             fastapi: 0.95.2
2023-07-23 10:10:15,090:INFO:             uvicorn: 0.22.0
2023-07-23 10:10:15,090:INFO:              m2cgen: Not installed
2023-07-23 10:10:15,090:INFO:           evidently: Not installed
2023-07-23 10:10:15,090:INFO:               fugue: Not installed
2023-07-23 10:10:15,090:INFO:           streamlit: Not installed
2023-07-23 10:10:15,090:INFO:             prophet: Not installed
2023-07-23 10:10:15,090:INFO:None
2023-07-23 10:10:15,090:INFO:Set up data.
2023-07-23 10:10:15,114:INFO:Set up train/test split.
2023-07-23 10:10:15,114:INFO:Set up data.
2023-07-23 10:10:15,132:INFO:Set up index.
2023-07-23 10:10:15,133:INFO:Set up folding strategy.
2023-07-23 10:10:15,133:INFO:Assigning column types.
2023-07-23 10:10:15,138:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-23 10:10:15,199:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,202:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,248:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:15,252:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:15,311:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,312:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,348:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:15,351:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:15,352:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-23 10:10:15,411:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,448:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:15,452:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:15,511:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,547:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:15,550:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:15,551:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-23 10:10:15,647:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:15,650:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:15,747:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:15,750:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:15,806:INFO:Preparing preprocessing pipeline...
2023-07-23 10:10:15,807:INFO:Set up iterative imputation.
2023-07-23 10:10:15,808:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,812:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,817:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,877:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-23 10:10:15,937:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:15,940:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:16,037:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:16,041:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:16,067:INFO:Set up encoding of categorical features.
2023-07-23 10:10:16,067:INFO:Set up imbalanced handling.
2023-07-23 10:10:16,067:INFO:Set up feature selection.
2023-07-23 10:10:16,163:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:16,167:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:16,210:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:16,344:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:16,496:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:16,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:16,775:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:16,911:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:17,060:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:17,199:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:17,346:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:17,489:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:17,625:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:17,773:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:17,914:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:18,060:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:18,222:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:18,370:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:18,515:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:18,671:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:18,823:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:18,974:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:19,123:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:19,272:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:19,414:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:19,567:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:19,714:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:19,861:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:20,013:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:20,155:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:20,297:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:20,450:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:20,592:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:20,728:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:20,880:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:21,028:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:21,170:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:21,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:21,462:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:21,604:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:21,761:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:21,902:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:22,048:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:22,210:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:22,351:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:22,498:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:22,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:22,802:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:22,943:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:23,108:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:23,285:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:23,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:23,601:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:23,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:23,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:24,092:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:24,236:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:24,397:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:24,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:24,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:25,068:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:25,228:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:25,413:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:25,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:25,921:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:26,121:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:26,383:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:26,549:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:26,703:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:26,853:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:27,017:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:27,163:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:27,312:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:27,490:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:27,651:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:27,818:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:27,991:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning:

Using categorical_feature in Dataset.


2023-07-23 10:10:28,158:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning:

[IterativeImputer] Early stopping criterion not reached.


2023-07-23 10:10:30,616:INFO:Finished creating preprocessing pipeline.
2023-07-23 10:10:30,643:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-23 10:10:30,644:INFO:Creating final display dataframe.
2023-07-23 10:10:34,548:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:271: UserWarning:

Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.


2023-07-23 10:10:35,582:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             50.9%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              554a
2023-07-23 10:10:35,684:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:35,687:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:35,773:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 10:10:35,777:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 10:10:35,778:INFO:setup() successfully completed in 24.33s...............
2023-07-23 10:10:51,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:35: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,656:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:54: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,657:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:63: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,657:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:69: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,657:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:77: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,661:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:5: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,661:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:10: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,662:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:15: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,663:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:20: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,663:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:363: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:385: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:428: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:439: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,678:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:186: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,679:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:197: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,681:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_image.py:175: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,700:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\explainers\_partition.py:676: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 10:10:51,725:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 10:10:51,726:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 10:10:51,800:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 10:10:51,805:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 10:10:51,812:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 20:40:12,219:INFO:PyCaret ClassificationExperiment
2023-07-23 20:40:12,219:INFO:Logging name: clf-default-name
2023-07-23 20:40:12,219:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-23 20:40:12,219:INFO:version 3.0.4
2023-07-23 20:40:12,219:INFO:Initializing setup()
2023-07-23 20:40:12,220:INFO:self.USI: 6455
2023-07-23 20:40:12,220:INFO:self._variable_keys: {'_available_plots', 'idx', 'memory', '_ml_usecase', 'fold_shuffle_param', 'gpu_param', 'X_train', 'exp_id', 'data', 'seed', 'logging_param', 'X', 'y_test', 'gpu_n_jobs_param', 'exp_name_log', 'y_train', 'is_multiclass', 'log_plots_param', 'html_param', 'USI', 'fold_generator', 'fix_imbalance', 'y', 'X_test', 'target_param', 'pipeline', 'fold_groups_param', 'n_jobs_param'}
2023-07-23 20:40:12,220:INFO:Checking environment
2023-07-23 20:40:12,220:INFO:python_version: 3.10.8
2023-07-23 20:40:12,220:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-23 20:40:12,220:INFO:machine: AMD64
2023-07-23 20:40:12,220:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-23 20:40:12,226:INFO:Memory: svmem(total=16505966592, available=4551856128, percent=72.4, used=11954110464, free=4551856128)
2023-07-23 20:40:12,226:INFO:Physical Core: 6
2023-07-23 20:40:12,226:INFO:Logical Core: 12
2023-07-23 20:40:12,227:INFO:Checking libraries
2023-07-23 20:40:12,227:INFO:System:
2023-07-23 20:40:12,227:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-23 20:40:12,227:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-23 20:40:12,227:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-23 20:40:12,227:INFO:PyCaret required dependencies:
2023-07-23 20:40:12,227:INFO:                 pip: 22.2.2
2023-07-23 20:40:12,227:INFO:          setuptools: 63.2.0
2023-07-23 20:40:12,227:INFO:             pycaret: 3.0.4
2023-07-23 20:40:12,227:INFO:             IPython: 8.11.0
2023-07-23 20:40:12,227:INFO:          ipywidgets: 8.0.7
2023-07-23 20:40:12,227:INFO:                tqdm: 4.64.1
2023-07-23 20:40:12,227:INFO:               numpy: 1.23.5
2023-07-23 20:40:12,227:INFO:              pandas: 1.5.3
2023-07-23 20:40:12,227:INFO:              jinja2: 3.1.2
2023-07-23 20:40:12,227:INFO:               scipy: 1.9.3
2023-07-23 20:40:12,227:INFO:              joblib: 1.2.0
2023-07-23 20:40:12,227:INFO:             sklearn: 1.2.2
2023-07-23 20:40:12,227:INFO:                pyod: 1.1.0
2023-07-23 20:40:12,227:INFO:            imblearn: 0.10.1
2023-07-23 20:40:12,227:INFO:   category_encoders: 2.6.1
2023-07-23 20:40:12,227:INFO:            lightgbm: 3.3.5
2023-07-23 20:40:12,227:INFO:               numba: 0.57.0
2023-07-23 20:40:12,227:INFO:            requests: 2.28.2
2023-07-23 20:40:12,227:INFO:          matplotlib: 3.7.1
2023-07-23 20:40:12,228:INFO:          scikitplot: 0.3.7
2023-07-23 20:40:12,228:INFO:         yellowbrick: 1.5
2023-07-23 20:40:12,228:INFO:              plotly: 5.15.0
2023-07-23 20:40:12,228:INFO:    plotly-resampler: Not installed
2023-07-23 20:40:12,228:INFO:             kaleido: 0.2.1
2023-07-23 20:40:12,228:INFO:           schemdraw: 0.15
2023-07-23 20:40:12,228:INFO:         statsmodels: 0.13.5
2023-07-23 20:40:12,228:INFO:              sktime: 0.21.0
2023-07-23 20:40:12,228:INFO:               tbats: 1.1.3
2023-07-23 20:40:12,228:INFO:            pmdarima: 2.0.3
2023-07-23 20:40:12,228:INFO:              psutil: 5.9.4
2023-07-23 20:40:12,228:INFO:          markupsafe: 2.1.2
2023-07-23 20:40:12,228:INFO:             pickle5: Not installed
2023-07-23 20:40:12,228:INFO:         cloudpickle: 2.2.1
2023-07-23 20:40:12,228:INFO:         deprecation: 2.1.0
2023-07-23 20:40:12,228:INFO:              xxhash: 3.2.0
2023-07-23 20:40:12,228:INFO:           wurlitzer: Not installed
2023-07-23 20:40:12,228:INFO:PyCaret optional dependencies:
2023-07-23 20:40:12,228:INFO:                shap: 0.41.0
2023-07-23 20:40:12,228:INFO:           interpret: 0.4.2
2023-07-23 20:40:12,228:INFO:                umap: 0.5.3
2023-07-23 20:40:12,228:INFO:    pandas_profiling: 4.1.2
2023-07-23 20:40:12,228:INFO:  explainerdashboard: Not installed
2023-07-23 20:40:12,228:INFO:             autoviz: Not installed
2023-07-23 20:40:12,228:INFO:           fairlearn: Not installed
2023-07-23 20:40:12,228:INFO:          deepchecks: Not installed
2023-07-23 20:40:12,228:INFO:             xgboost: 1.7.6
2023-07-23 20:40:12,228:INFO:            catboost: Not installed
2023-07-23 20:40:12,228:INFO:              kmodes: Not installed
2023-07-23 20:40:12,228:INFO:             mlxtend: Not installed
2023-07-23 20:40:12,228:INFO:       statsforecast: Not installed
2023-07-23 20:40:12,228:INFO:        tune_sklearn: Not installed
2023-07-23 20:40:12,228:INFO:                 ray: Not installed
2023-07-23 20:40:12,228:INFO:            hyperopt: Not installed
2023-07-23 20:40:12,228:INFO:              optuna: 3.2.0
2023-07-23 20:40:12,228:INFO:               skopt: Not installed
2023-07-23 20:40:12,228:INFO:              mlflow: 2.4.2
2023-07-23 20:40:12,228:INFO:              gradio: Not installed
2023-07-23 20:40:12,228:INFO:             fastapi: 0.95.2
2023-07-23 20:40:12,228:INFO:             uvicorn: 0.22.0
2023-07-23 20:40:12,228:INFO:              m2cgen: Not installed
2023-07-23 20:40:12,228:INFO:           evidently: Not installed
2023-07-23 20:40:12,228:INFO:               fugue: Not installed
2023-07-23 20:40:12,230:INFO:           streamlit: Not installed
2023-07-23 20:40:12,230:INFO:             prophet: Not installed
2023-07-23 20:40:12,230:INFO:None
2023-07-23 20:40:12,230:INFO:Set up data.
2023-07-23 20:40:12,254:INFO:Set up train/test split.
2023-07-23 20:40:12,254:INFO:Set up data.
2023-07-23 20:40:12,283:INFO:Set up index.
2023-07-23 20:40:12,284:INFO:Set up folding strategy.
2023-07-23 20:40:12,284:INFO:Assigning column types.
2023-07-23 20:40:12,294:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-23 20:40:12,341:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,342:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,372:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:12,375:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:12,422:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,423:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,454:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:12,457:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:12,457:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-23 20:40:12,512:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,542:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:12,545:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:12,595:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,625:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:12,628:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:12,629:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-23 20:40:12,714:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:12,717:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:12,798:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:12,801:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:12,803:INFO:Preparing preprocessing pipeline...
2023-07-23 20:40:12,804:INFO:Set up iterative imputation.
2023-07-23 20:40:12,804:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,809:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,814:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,879:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-23 20:40:12,935:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:12,938:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:13,015:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:13,018:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:13,055:INFO:Set up encoding of categorical features.
2023-07-23 20:40:13,055:INFO:Set up imbalanced handling.
2023-07-23 20:40:13,055:INFO:Set up feature selection.
2023-07-23 20:40:13,139:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:13,142:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:14,055:INFO:Finished creating preprocessing pipeline.
2023-07-23 20:40:14,087:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-23 20:40:14,087:INFO:Creating final display dataframe.
2023-07-23 20:40:16,102:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             50.9%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              6455
2023-07-23 20:40:16,217:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:16,222:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:16,315:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 20:40:16,319:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 20:40:16,320:INFO:setup() successfully completed in 7.51s...............
2023-07-23 22:39:20,661:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 22:39:25,188:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 22:39:25,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 22:39:25,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 22:39:25,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 22:39:25,210:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 22:39:25,210:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 22:39:25,213:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 22:39:25,767:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning:

This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0


2023-07-23 22:39:26,081:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning:

'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680


2023-07-23 22:39:30,340:INFO:PyCaret ClassificationExperiment
2023-07-23 22:39:30,342:INFO:Logging name: clf-default-name
2023-07-23 22:39:30,342:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-23 22:39:30,342:INFO:version 3.0.4
2023-07-23 22:39:30,342:INFO:Initializing setup()
2023-07-23 22:39:30,342:INFO:self.USI: 35bb
2023-07-23 22:39:30,342:INFO:self._variable_keys: {'data', 'log_plots_param', 'html_param', 'X_train', 'idx', 'target_param', 'y_test', 'fix_imbalance', 'gpu_param', '_ml_usecase', 'X', 'y_train', 'n_jobs_param', 'logging_param', 'exp_id', 'y', '_available_plots', 'seed', 'fold_groups_param', 'fold_shuffle_param', 'X_test', 'gpu_n_jobs_param', 'fold_generator', 'USI', 'exp_name_log', 'memory', 'pipeline', 'is_multiclass'}
2023-07-23 22:39:30,342:INFO:Checking environment
2023-07-23 22:39:30,342:INFO:python_version: 3.10.8
2023-07-23 22:39:30,342:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-23 22:39:30,342:INFO:machine: AMD64
2023-07-23 22:39:30,342:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-23 22:39:30,347:INFO:Memory: svmem(total=16505966592, available=2424307712, percent=85.3, used=14081658880, free=2424307712)
2023-07-23 22:39:30,347:INFO:Physical Core: 6
2023-07-23 22:39:30,347:INFO:Logical Core: 12
2023-07-23 22:39:30,347:INFO:Checking libraries
2023-07-23 22:39:30,349:INFO:System:
2023-07-23 22:39:30,349:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-23 22:39:30,349:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-23 22:39:30,349:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-23 22:39:30,349:INFO:PyCaret required dependencies:
2023-07-23 22:39:30,349:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 22:39:30,351:INFO:                 pip: 22.2.2
2023-07-23 22:39:30,351:INFO:          setuptools: 63.2.0
2023-07-23 22:39:30,351:INFO:             pycaret: 3.0.4
2023-07-23 22:39:30,351:INFO:             IPython: 8.11.0
2023-07-23 22:39:30,352:INFO:          ipywidgets: 8.0.7
2023-07-23 22:39:30,352:INFO:                tqdm: 4.64.1
2023-07-23 22:39:30,352:INFO:               numpy: 1.23.5
2023-07-23 22:39:30,352:INFO:              pandas: 1.5.3
2023-07-23 22:39:30,352:INFO:              jinja2: 3.1.2
2023-07-23 22:39:30,352:INFO:               scipy: 1.9.3
2023-07-23 22:39:30,352:INFO:              joblib: 1.2.0
2023-07-23 22:39:30,352:INFO:             sklearn: 1.2.2
2023-07-23 22:39:30,352:INFO:                pyod: 1.1.0
2023-07-23 22:39:30,352:INFO:            imblearn: 0.10.1
2023-07-23 22:39:30,352:INFO:   category_encoders: 2.6.1
2023-07-23 22:39:30,352:INFO:            lightgbm: 3.3.5
2023-07-23 22:39:30,352:INFO:               numba: 0.57.0
2023-07-23 22:39:30,352:INFO:            requests: 2.28.2
2023-07-23 22:39:30,352:INFO:          matplotlib: 3.7.1
2023-07-23 22:39:30,352:INFO:          scikitplot: 0.3.7
2023-07-23 22:39:30,352:INFO:         yellowbrick: 1.5
2023-07-23 22:39:30,352:INFO:              plotly: 5.15.0
2023-07-23 22:39:30,352:INFO:    plotly-resampler: Not installed
2023-07-23 22:39:30,352:INFO:             kaleido: 0.2.1
2023-07-23 22:39:30,352:INFO:           schemdraw: 0.15
2023-07-23 22:39:30,352:INFO:         statsmodels: 0.13.5
2023-07-23 22:39:30,352:INFO:              sktime: 0.21.0
2023-07-23 22:39:30,352:INFO:               tbats: 1.1.3
2023-07-23 22:39:30,353:INFO:            pmdarima: 2.0.3
2023-07-23 22:39:30,353:INFO:              psutil: 5.9.4
2023-07-23 22:39:30,353:INFO:          markupsafe: 2.1.2
2023-07-23 22:39:30,353:INFO:             pickle5: Not installed
2023-07-23 22:39:30,353:INFO:         cloudpickle: 2.2.1
2023-07-23 22:39:30,353:INFO:         deprecation: 2.1.0
2023-07-23 22:39:30,353:INFO:              xxhash: 3.2.0
2023-07-23 22:39:30,353:INFO:           wurlitzer: Not installed
2023-07-23 22:39:30,353:INFO:PyCaret optional dependencies:
2023-07-23 22:39:31,144:INFO:                shap: 0.41.0
2023-07-23 22:39:31,144:INFO:           interpret: 0.4.2
2023-07-23 22:39:31,144:INFO:                umap: 0.5.3
2023-07-23 22:39:31,145:INFO:    pandas_profiling: 4.1.2
2023-07-23 22:39:31,145:INFO:  explainerdashboard: Not installed
2023-07-23 22:39:31,145:INFO:             autoviz: Not installed
2023-07-23 22:39:31,145:INFO:           fairlearn: Not installed
2023-07-23 22:39:31,145:INFO:          deepchecks: Not installed
2023-07-23 22:39:31,145:INFO:             xgboost: 1.7.6
2023-07-23 22:39:31,145:INFO:            catboost: Not installed
2023-07-23 22:39:31,145:INFO:              kmodes: Not installed
2023-07-23 22:39:31,145:INFO:             mlxtend: Not installed
2023-07-23 22:39:31,145:INFO:       statsforecast: Not installed
2023-07-23 22:39:31,145:INFO:        tune_sklearn: Not installed
2023-07-23 22:39:31,145:INFO:                 ray: Not installed
2023-07-23 22:39:31,145:INFO:            hyperopt: Not installed
2023-07-23 22:39:31,145:INFO:              optuna: 3.2.0
2023-07-23 22:39:31,145:INFO:               skopt: Not installed
2023-07-23 22:39:31,145:INFO:              mlflow: 2.4.2
2023-07-23 22:39:31,145:INFO:              gradio: Not installed
2023-07-23 22:39:31,145:INFO:             fastapi: 0.95.2
2023-07-23 22:39:31,145:INFO:             uvicorn: 0.22.0
2023-07-23 22:39:31,145:INFO:              m2cgen: Not installed
2023-07-23 22:39:31,145:INFO:           evidently: Not installed
2023-07-23 22:39:31,145:INFO:               fugue: Not installed
2023-07-23 22:39:31,145:INFO:           streamlit: Not installed
2023-07-23 22:39:31,145:INFO:             prophet: Not installed
2023-07-23 22:39:31,145:INFO:None
2023-07-23 22:39:31,145:INFO:Set up data.
2023-07-23 22:39:31,172:INFO:Set up train/test split.
2023-07-23 22:39:31,172:INFO:Set up data.
2023-07-23 22:39:31,190:INFO:Set up index.
2023-07-23 22:39:31,191:INFO:Set up folding strategy.
2023-07-23 22:39:31,192:INFO:Assigning column types.
2023-07-23 22:39:31,202:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-23 22:39:31,251:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,256:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,304:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:31,307:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:31,356:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,357:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,386:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:31,389:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:31,389:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-23 22:39:31,433:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,463:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:31,466:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:31,515:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,547:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:31,549:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:31,550:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-23 22:39:31,627:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:31,630:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:31,711:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:31,711:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:31,724:INFO:Preparing preprocessing pipeline...
2023-07-23 22:39:31,726:INFO:Set up iterative imputation.
2023-07-23 22:39:31,726:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,730:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,732:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,800:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-23 22:39:31,849:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:31,852:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:31,935:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:31,940:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:31,977:INFO:Set up encoding of categorical features.
2023-07-23 22:39:31,977:INFO:Set up imbalanced handling.
2023-07-23 22:39:31,977:INFO:Set up feature selection.
2023-07-23 22:39:32,057:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:32,060:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:32,902:INFO:Finished creating preprocessing pipeline.
2023-07-23 22:39:32,932:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-23 22:39:32,932:INFO:Creating final display dataframe.
2023-07-23 22:39:34,809:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             50.9%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              35bb
2023-07-23 22:39:34,920:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:34,923:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:35,010:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 22:39:35,012:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 22:39:35,013:INFO:setup() successfully completed in 7.46s...............
2023-07-23 22:39:42,918:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:35: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,924:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:54: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,924:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:63: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,926:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:69: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,926:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:77: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,929:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:5: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:10: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:15: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:20: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:363: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:385: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:428: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:439: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,946:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:186: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,946:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:197: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,949:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_image.py:175: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:42,976:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\explainers\_partition.py:676: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 22:39:43,005:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 22:39:43,005:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 22:39:43,086:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 22:39:43,090:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 22:39:43,101:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 22:39:45,110:WARNING:X does not have valid feature names, but QuantileTransformer was fitted with feature names

2023-07-23 23:05:40,769:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:60: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 23:05:41,182:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 23:05:41,183:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 23:05:41,183:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 23:05:41,183:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-07-23 23:05:41,196:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\colors.py:35: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 23:05:41,196:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\setuptools\_distutils\version.py:345: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 23:05:41,199:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\yellowbrick\style\rcmod.py:31: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 23:05:41,485:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\scikitplot\plotters.py:33: DeprecationWarning:

This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0


2023-07-23 23:05:41,944:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\requests\__init__.py:43: DeprecationWarning:

'urllib3[secure]' extra is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680


2023-07-23 23:05:45,172:INFO:PyCaret ClassificationExperiment
2023-07-23 23:05:45,172:INFO:Logging name: clf-default-name
2023-07-23 23:05:45,172:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-07-23 23:05:45,172:INFO:version 3.0.4
2023-07-23 23:05:45,172:INFO:Initializing setup()
2023-07-23 23:05:45,172:INFO:self.USI: d86e
2023-07-23 23:05:45,172:INFO:self._variable_keys: {'n_jobs_param', 'exp_name_log', 'gpu_n_jobs_param', '_available_plots', 'gpu_param', 'pipeline', 'y', 'fold_shuffle_param', 'log_plots_param', 'fix_imbalance', 'is_multiclass', 'fold_generator', '_ml_usecase', 'target_param', 'seed', 'memory', 'USI', 'html_param', 'idx', 'fold_groups_param', 'logging_param', 'y_test', 'data', 'X_test', 'X', 'exp_id', 'X_train', 'y_train'}
2023-07-23 23:05:45,172:INFO:Checking environment
2023-07-23 23:05:45,173:INFO:python_version: 3.10.8
2023-07-23 23:05:45,173:INFO:python_build: ('tags/v3.10.8:aaaf517', 'Oct 11 2022 16:50:30')
2023-07-23 23:05:45,173:INFO:machine: AMD64
2023-07-23 23:05:45,173:INFO:platform: Windows-10-10.0.22621-SP0
2023-07-23 23:05:45,177:INFO:Memory: svmem(total=16505966592, available=2775916544, percent=83.2, used=13730050048, free=2775916544)
2023-07-23 23:05:45,177:INFO:Physical Core: 6
2023-07-23 23:05:45,177:INFO:Logical Core: 12
2023-07-23 23:05:45,178:INFO:Checking libraries
2023-07-23 23:05:45,178:INFO:System:
2023-07-23 23:05:45,178:INFO:    python: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]
2023-07-23 23:05:45,178:INFO:executable: c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\Scripts\python.exe
2023-07-23 23:05:45,178:INFO:   machine: Windows-10-10.0.22621-SP0
2023-07-23 23:05:45,178:INFO:PyCaret required dependencies:
2023-07-23 23:05:45,178:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\_dependencies.py:39: DeprecationWarning:

distutils Version classes are deprecated. Use packaging.version instead.


2023-07-23 23:05:45,180:INFO:                 pip: 22.2.2
2023-07-23 23:05:45,181:INFO:          setuptools: 63.2.0
2023-07-23 23:05:45,181:INFO:             pycaret: 3.0.4
2023-07-23 23:05:45,181:INFO:             IPython: 8.11.0
2023-07-23 23:05:45,181:INFO:          ipywidgets: 8.0.7
2023-07-23 23:05:45,181:INFO:                tqdm: 4.64.1
2023-07-23 23:05:45,181:INFO:               numpy: 1.23.5
2023-07-23 23:05:45,181:INFO:              pandas: 1.5.3
2023-07-23 23:05:45,181:INFO:              jinja2: 3.1.2
2023-07-23 23:05:45,181:INFO:               scipy: 1.9.3
2023-07-23 23:05:45,181:INFO:              joblib: 1.2.0
2023-07-23 23:05:45,181:INFO:             sklearn: 1.2.2
2023-07-23 23:05:45,181:INFO:                pyod: 1.1.0
2023-07-23 23:05:45,181:INFO:            imblearn: 0.10.1
2023-07-23 23:05:45,181:INFO:   category_encoders: 2.6.1
2023-07-23 23:05:45,181:INFO:            lightgbm: 3.3.5
2023-07-23 23:05:45,181:INFO:               numba: 0.57.0
2023-07-23 23:05:45,181:INFO:            requests: 2.28.2
2023-07-23 23:05:45,181:INFO:          matplotlib: 3.7.1
2023-07-23 23:05:45,181:INFO:          scikitplot: 0.3.7
2023-07-23 23:05:45,181:INFO:         yellowbrick: 1.5
2023-07-23 23:05:45,181:INFO:              plotly: 5.15.0
2023-07-23 23:05:45,181:INFO:    plotly-resampler: Not installed
2023-07-23 23:05:45,181:INFO:             kaleido: 0.2.1
2023-07-23 23:05:45,181:INFO:           schemdraw: 0.15
2023-07-23 23:05:45,181:INFO:         statsmodels: 0.13.5
2023-07-23 23:05:45,181:INFO:              sktime: 0.21.0
2023-07-23 23:05:45,181:INFO:               tbats: 1.1.3
2023-07-23 23:05:45,181:INFO:            pmdarima: 2.0.3
2023-07-23 23:05:45,182:INFO:              psutil: 5.9.4
2023-07-23 23:05:45,182:INFO:          markupsafe: 2.1.2
2023-07-23 23:05:45,182:INFO:             pickle5: Not installed
2023-07-23 23:05:45,182:INFO:         cloudpickle: 2.2.1
2023-07-23 23:05:45,182:INFO:         deprecation: 2.1.0
2023-07-23 23:05:45,182:INFO:              xxhash: 3.2.0
2023-07-23 23:05:45,182:INFO:           wurlitzer: Not installed
2023-07-23 23:05:45,182:INFO:PyCaret optional dependencies:
2023-07-23 23:05:45,433:INFO:                shap: 0.41.0
2023-07-23 23:05:45,433:INFO:           interpret: 0.4.2
2023-07-23 23:05:45,433:INFO:                umap: 0.5.3
2023-07-23 23:05:45,433:INFO:    pandas_profiling: 4.1.2
2023-07-23 23:05:45,434:INFO:  explainerdashboard: Not installed
2023-07-23 23:05:45,434:INFO:             autoviz: Not installed
2023-07-23 23:05:45,434:INFO:           fairlearn: Not installed
2023-07-23 23:05:45,434:INFO:          deepchecks: Not installed
2023-07-23 23:05:45,434:INFO:             xgboost: 1.7.6
2023-07-23 23:05:45,434:INFO:            catboost: Not installed
2023-07-23 23:05:45,434:INFO:              kmodes: Not installed
2023-07-23 23:05:45,434:INFO:             mlxtend: Not installed
2023-07-23 23:05:45,434:INFO:       statsforecast: Not installed
2023-07-23 23:05:45,434:INFO:        tune_sklearn: Not installed
2023-07-23 23:05:45,434:INFO:                 ray: Not installed
2023-07-23 23:05:45,434:INFO:            hyperopt: Not installed
2023-07-23 23:05:45,434:INFO:              optuna: 3.2.0
2023-07-23 23:05:45,434:INFO:               skopt: Not installed
2023-07-23 23:05:45,434:INFO:              mlflow: 2.4.2
2023-07-23 23:05:45,434:INFO:              gradio: Not installed
2023-07-23 23:05:45,434:INFO:             fastapi: 0.95.2
2023-07-23 23:05:45,434:INFO:             uvicorn: 0.22.0
2023-07-23 23:05:45,434:INFO:              m2cgen: Not installed
2023-07-23 23:05:45,434:INFO:           evidently: Not installed
2023-07-23 23:05:45,434:INFO:               fugue: Not installed
2023-07-23 23:05:45,434:INFO:           streamlit: Not installed
2023-07-23 23:05:45,434:INFO:             prophet: Not installed
2023-07-23 23:05:45,434:INFO:None
2023-07-23 23:05:45,434:INFO:Set up data.
2023-07-23 23:05:45,456:INFO:Set up train/test split.
2023-07-23 23:05:45,456:INFO:Set up data.
2023-07-23 23:05:45,475:INFO:Set up index.
2023-07-23 23:05:45,475:INFO:Set up folding strategy.
2023-07-23 23:05:45,476:INFO:Assigning column types.
2023-07-23 23:05:45,483:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-07-23 23:05:45,538:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 23:05:45,541:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 23:05:45,578:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:45,582:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:45,627:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-07-23 23:05:45,628:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 23:05:45,657:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:45,660:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:45,660:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-07-23 23:05:45,707:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 23:05:45,743:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:45,746:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:45,802:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-07-23 23:05:45,831:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:45,834:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:45,835:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-07-23 23:05:45,915:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:45,918:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:45,996:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:45,999:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:46,001:INFO:Preparing preprocessing pipeline...
2023-07-23 23:05:46,002:INFO:Set up iterative imputation.
2023-07-23 23:05:46,002:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-07-23 23:05:46,007:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-07-23 23:05:46,012:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-07-23 23:05:46,076:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-07-23 23:05:46,127:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:46,131:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:46,206:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:46,209:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:46,247:INFO:Set up encoding of categorical features.
2023-07-23 23:05:46,247:INFO:Set up imbalanced handling.
2023-07-23 23:05:46,247:INFO:Set up feature selection.
2023-07-23 23:05:46,322:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:46,325:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:47,141:INFO:Finished creating preprocessing pipeline.
2023-07-23 23:05:47,168:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\TEMP\joblib),
         steps=[('iterative_imputer',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=IterativeImputer(add_indicator=False,
                                                                 cat_estimator=LGBMClassifier(boosting_type='gbdt',
                                                                                              class_weight=None,
                                                                                              colsample_bytree=1.0,
                                                                                              importance_type='split',
                                                                                              learning_rate=0.1,
                                                                                              max_depth=-1,
                                                                                              min_child_samples=20,
                                                                                              min_child_w...
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=-1,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         silent='warn',
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=20,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2023-07-23 23:05:47,168:INFO:Creating final display dataframe.
2023-07-23 23:05:48,928:INFO:Setup _display_container:                         Description             Value
0                        Session id                42
1                            Target      credit_score
2                       Target type            Binary
3               Original data shape       (12500, 22)
4            Transformed data shape       (16736, 21)
5       Transformed train set shape       (14236, 21)
6        Transformed test set shape        (2500, 21)
7                   Ignore features                 1
8                  Numeric features                17
9              Categorical features                 3
10         Rows with missing values             50.9%
11                       Preprocess              True
12                  Imputation type         iterative
13  Iterative imputation iterations                 5
14        Numeric iterative imputer          lightgbm
15    Categorical iterative imputer          lightgbm
16         Maximum one-hot encoding                25
17                  Encoding method              None
18                    Fix imbalance              True
19             Fix imbalance method             SMOTE
20                Feature selection              True
21         Feature selection method           classic
22      Feature selection estimator          lightgbm
23      Number of features selected                20
24                   Fold Generator             KFold
25                      Fold Number                10
26                         CPU Jobs                -1
27                          Use GPU             False
28                   Log Experiment             False
29                  Experiment Name  clf-default-name
30                              USI              d86e
2023-07-23 23:05:49,038:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:49,041:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:49,124:INFO:Soft dependency imported: xgboost: 1.7.6
2023-07-23 23:05:49,127:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-07-23 23:05:49,128:INFO:setup() successfully completed in 6.43s...............
2023-07-23 23:05:49,200:INFO:Initializing compare_models()
2023-07-23 23:05:49,200:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, include=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-07-23 23:05:49,202:INFO:Checking exceptions
2023-07-23 23:05:49,207:INFO:Preparing display monitor
2023-07-23 23:05:49,237:INFO:Initializing Logistic Regression
2023-07-23 23:05:49,237:INFO:Total runtime is 0.0 minutes
2023-07-23 23:05:49,242:INFO:SubProcess create_model() called ==================================
2023-07-23 23:05:49,243:INFO:Initializing create_model()
2023-07-23 23:05:49,243:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:05:49,243:INFO:Checking exceptions
2023-07-23 23:05:49,244:INFO:Importing libraries
2023-07-23 23:05:49,244:INFO:Copying training dataset
2023-07-23 23:05:49,259:INFO:Defining folds
2023-07-23 23:05:49,259:INFO:Declaring metric variables
2023-07-23 23:05:49,263:INFO:Importing untrained model
2023-07-23 23:05:49,268:INFO:Logistic Regression Imported successfully
2023-07-23 23:05:49,279:INFO:Starting cross validation
2023-07-23 23:05:49,787:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:05:52,101:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,103:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,103:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,111:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,156:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,378:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,421:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,452:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,458:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,510:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,516:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,565:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,596:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,624:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,704:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,768:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,815:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,818:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,852:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,955:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:52,992:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,027:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,039:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,072:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,174:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,176:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,179:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,213:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,400:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,413:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,438:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,456:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,456:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,524:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,535:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,551:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,571:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,783:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,861:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,863:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,873:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,876:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,894:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,920:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:53,976:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,124:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,211:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,229:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,232:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,261:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,301:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,351:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,393:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,409:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,463:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,559:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,575:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,612:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,724:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,784:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,816:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,823:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,841:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,843:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,923:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,925:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,944:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:54,974:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,170:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,173:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,240:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,286:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,304:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,314:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,323:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,538:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,561:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,698:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,703:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,705:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,729:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,737:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:55,737:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,006:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,008:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,045:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,050:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,142:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,161:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,165:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,174:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,186:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,399:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,470:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,523:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,551:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,560:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,627:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,657:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,796:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,804:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,812:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,913:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,952:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:56,967:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,094:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,105:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,215:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,219:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,318:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,334:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,363:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,391:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,584:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,585:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,594:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,614:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,630:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,737:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,747:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,778:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,859:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:57,982:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,033:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,033:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,075:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,078:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,144:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,185:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,191:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,326:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,411:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,463:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,552:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,598:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,623:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,631:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,809:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,872:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:58,960:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,008:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,012:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,036:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,047:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,071:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,099:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,245:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,300:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,307:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,362:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,452:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,456:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,467:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,481:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,521:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,525:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,731:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,747:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,784:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,851:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,853:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,861:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,896:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:05:59,975:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,120:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,173:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,185:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,202:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,263:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,267:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,287:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,373:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,387:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,429:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,591:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,608:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,655:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,656:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,656:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,660:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,674:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,843:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:00,869:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,021:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,021:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,061:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,085:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,105:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,119:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,190:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,353:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,428:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,459:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,467:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,488:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,499:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,503:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,620:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,742:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,816:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,852:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,892:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,895:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:01,906:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,016:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,179:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,257:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,261:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,288:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,316:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,324:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,351:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,415:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,450:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,606:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,672:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,679:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,683:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,711:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,716:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,754:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:02,895:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,056:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,074:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,075:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,094:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,151:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,203:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,223:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,442:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,486:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,488:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,501:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,531:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,576:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,597:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,625:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,650:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,837:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,873:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,896:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,902:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,938:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:03,984:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,019:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,031:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,085:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,228:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,248:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,267:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,297:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,306:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,345:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,382:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,436:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,464:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,558:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,651:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,659:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,680:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,709:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,734:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,756:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,809:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,830:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,889:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:04,994:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,051:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,087:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,121:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,128:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,161:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,247:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,266:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,424:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,473:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,521:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,534:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,544:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,569:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,575:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,683:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,839:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,879:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,936:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,951:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,962:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:05,991:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,003:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,060:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,094:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,295:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,344:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,347:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,374:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,413:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,422:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,461:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,492:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,725:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,735:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,766:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,774:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,827:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,831:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,883:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,895:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:06,904:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,170:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,200:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,212:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,271:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,288:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,309:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,329:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,363:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,557:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,588:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,615:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,619:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,706:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,736:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,736:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,823:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:07,938:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,011:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,025:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,031:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,118:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,119:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,213:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,264:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,364:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,460:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,490:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,528:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,604:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,735:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,885:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,957:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,959:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:08,964:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,037:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,063:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,128:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,139:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,144:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,222:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,371:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,391:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,421:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,444:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,543:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,575:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,634:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,685:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,837:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,873:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,895:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:09,965:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,006:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,015:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,187:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,249:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,274:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,374:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,415:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,436:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,503:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,507:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,585:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,673:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,673:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,689:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,815:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,824:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,869:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,912:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:10,941:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,108:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,131:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,209:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,218:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,240:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,275:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,395:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,474:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,510:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,554:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,591:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,602:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,636:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,638:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,670:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,761:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,830:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,889:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:11,955:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,008:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,030:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,053:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,086:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,098:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,186:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,257:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,293:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,394:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,403:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,454:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,468:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,515:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,574:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,646:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,662:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,705:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,820:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,856:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,868:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:12,927:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,017:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,059:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,095:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,208:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,273:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,298:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,374:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,452:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,458:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,470:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,525:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,662:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,690:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,752:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,772:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,874:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,894:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,911:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,922:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:13,932:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,074:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,091:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,173:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,192:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,193:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,330:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,464:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,507:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,566:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,609:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,637:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,665:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,747:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,762:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,892:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,923:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:14,986:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,051:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,125:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,145:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,189:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,243:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,285:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,294:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,325:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,368:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,487:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,517:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,562:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,564:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,585:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,662:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,681:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,736:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,748:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,914:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:15,953:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,000:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,030:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,044:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,072:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,159:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,161:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,187:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,328:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,372:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,391:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,465:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,479:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,526:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,573:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,579:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,639:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,675:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,848:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,872:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:16,906:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,011:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,034:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,045:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,101:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,231:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,235:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,450:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,484:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,504:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,591:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,645:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,688:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,741:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,874:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,927:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,935:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:17,991:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,013:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,084:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,143:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,157:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,272:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,298:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,353:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,368:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,435:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,468:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,482:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,501:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,587:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,622:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,682:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,713:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,824:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,853:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,872:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,887:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,893:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:18,992:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,080:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,109:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,116:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,263:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,271:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,278:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,283:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,298:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,307:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,415:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,534:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,542:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,666:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,707:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,710:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,715:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,744:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,744:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,852:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,954:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,967:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:19,999:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,099:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,119:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,195:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,227:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,253:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,277:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,305:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,363:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,370:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,509:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,686:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,718:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,739:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,883:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,911:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:20,972:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,029:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,103:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,123:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,132:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,141:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,155:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,223:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,326:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,339:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,365:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,431:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,543:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,553:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,565:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,594:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,606:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,621:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,728:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,743:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,825:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,850:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,947:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:21,966:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,013:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,041:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,059:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,075:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,129:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,131:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,245:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,290:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,348:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,363:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,394:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,469:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,519:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,520:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,542:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,556:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,668:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,764:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,777:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:22,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:22,811:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,930:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,948:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:22,956:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,970:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:22,980:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,063:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:23,135:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:23,240:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,298:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,332:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,383:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,395:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,680:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,682:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,694:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,833:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:23,842:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,056:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,084:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,138:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,266:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,284:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,418:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,446:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:24,584:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:24,667:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,691:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:24,800:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:25,015:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:25,085:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:25,219:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:25,400:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\lightgbm\basic.py:2065: UserWarning: Using categorical_feature in Dataset.
  _log_warning('Using categorical_feature in Dataset.')

2023-07-23 23:06:25,452:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:25,787:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\preprocess\iterative_imputer.py:402: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.
  warnings.warn(

2023-07-23 23:06:26,002:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:26,194:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:26,201:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:26,300:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:26,559:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:27,932:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:27,969:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:28,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:29,009:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:29,202:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 1.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-07-23 23:06:30,126:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:30,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:30,333:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:30,500:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:30,937:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:32,035:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:32,292:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:32,791:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:33,437:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:33,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 1.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:06:36,050:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-07-23 23:06:37,804:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:37,815:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:38,270:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:38,486:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:38,530:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:39,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:39,629:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:39,719:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:40,064:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:40,319:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:40,365:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:40,624:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:41,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:41,570:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:41,782:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:41,918:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 1.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-07-23 23:06:42,350:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:43,241:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:43,664:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:06:43,838:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:08,984:INFO:Calculating mean and std
2023-07-23 23:07:08,985:INFO:Creating metrics dataframe
2023-07-23 23:07:12,225:INFO:Uploading results into container
2023-07-23 23:07:12,226:INFO:Uploading model into container now
2023-07-23 23:07:12,227:INFO:_master_model_container: 1
2023-07-23 23:07:12,227:INFO:_display_container: 2
2023-07-23 23:07:12,228:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-07-23 23:07:12,228:INFO:create_model() successfully completed......................................
2023-07-23 23:07:12,343:INFO:SubProcess create_model() end ==================================
2023-07-23 23:07:12,343:INFO:Creating metrics dataframe
2023-07-23 23:07:12,353:INFO:Initializing K Neighbors Classifier
2023-07-23 23:07:12,353:INFO:Total runtime is 1.3852621992429097 minutes
2023-07-23 23:07:12,358:INFO:SubProcess create_model() called ==================================
2023-07-23 23:07:12,358:INFO:Initializing create_model()
2023-07-23 23:07:12,358:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:07:12,358:INFO:Checking exceptions
2023-07-23 23:07:12,358:INFO:Importing libraries
2023-07-23 23:07:12,358:INFO:Copying training dataset
2023-07-23 23:07:12,366:INFO:Defining folds
2023-07-23 23:07:12,366:INFO:Declaring metric variables
2023-07-23 23:07:12,370:INFO:Importing untrained model
2023-07-23 23:07:12,374:INFO:K Neighbors Classifier Imported successfully
2023-07-23 23:07:12,382:INFO:Starting cross validation
2023-07-23 23:07:12,851:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:07:20,276:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:20,340:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:20,426:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:20,438:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:20,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:20,656:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:21,053:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:21,054:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:22,093:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:22,848:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:07:53,808:INFO:Calculating mean and std
2023-07-23 23:07:53,810:INFO:Creating metrics dataframe
2023-07-23 23:07:57,299:INFO:Uploading results into container
2023-07-23 23:07:57,300:INFO:Uploading model into container now
2023-07-23 23:07:57,301:INFO:_master_model_container: 2
2023-07-23 23:07:57,301:INFO:_display_container: 2
2023-07-23 23:07:57,302:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-07-23 23:07:57,302:INFO:create_model() successfully completed......................................
2023-07-23 23:07:57,419:INFO:SubProcess create_model() end ==================================
2023-07-23 23:07:57,419:INFO:Creating metrics dataframe
2023-07-23 23:07:57,430:INFO:Initializing Naive Bayes
2023-07-23 23:07:57,430:INFO:Total runtime is 2.136539173126221 minutes
2023-07-23 23:07:57,434:INFO:SubProcess create_model() called ==================================
2023-07-23 23:07:57,434:INFO:Initializing create_model()
2023-07-23 23:07:57,434:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=nb, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:07:57,434:INFO:Checking exceptions
2023-07-23 23:07:57,434:INFO:Importing libraries
2023-07-23 23:07:57,435:INFO:Copying training dataset
2023-07-23 23:07:57,444:INFO:Defining folds
2023-07-23 23:07:57,444:INFO:Declaring metric variables
2023-07-23 23:07:57,450:INFO:Importing untrained model
2023-07-23 23:07:57,455:INFO:Naive Bayes Imported successfully
2023-07-23 23:07:57,464:INFO:Starting cross validation
2023-07-23 23:07:57,935:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:08:05,437:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:05,447:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:05,454:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:05,637:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:05,847:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:05,903:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:06,245:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:06,621:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:06,728:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:06,737:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:40,285:INFO:Calculating mean and std
2023-07-23 23:08:40,291:INFO:Creating metrics dataframe
2023-07-23 23:08:43,899:INFO:Uploading results into container
2023-07-23 23:08:43,901:INFO:Uploading model into container now
2023-07-23 23:08:43,902:INFO:_master_model_container: 3
2023-07-23 23:08:43,902:INFO:_display_container: 2
2023-07-23 23:08:43,903:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-07-23 23:08:43,903:INFO:create_model() successfully completed......................................
2023-07-23 23:08:44,034:INFO:SubProcess create_model() end ==================================
2023-07-23 23:08:44,034:INFO:Creating metrics dataframe
2023-07-23 23:08:44,045:INFO:Initializing Decision Tree Classifier
2023-07-23 23:08:44,045:INFO:Total runtime is 2.9134656071662905 minutes
2023-07-23 23:08:44,049:INFO:SubProcess create_model() called ==================================
2023-07-23 23:08:44,049:INFO:Initializing create_model()
2023-07-23 23:08:44,049:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:08:44,049:INFO:Checking exceptions
2023-07-23 23:08:44,050:INFO:Importing libraries
2023-07-23 23:08:44,050:INFO:Copying training dataset
2023-07-23 23:08:44,061:INFO:Defining folds
2023-07-23 23:08:44,062:INFO:Declaring metric variables
2023-07-23 23:08:44,067:INFO:Importing untrained model
2023-07-23 23:08:44,072:INFO:Decision Tree Classifier Imported successfully
2023-07-23 23:08:44,081:INFO:Starting cross validation
2023-07-23 23:08:44,576:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:08:52,546:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:52,597:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:52,866:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:52,873:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:53,056:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:53,149:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:53,232:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:53,606:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:53,692:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:08:54,078:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:31,669:INFO:Calculating mean and std
2023-07-23 23:09:31,671:INFO:Creating metrics dataframe
2023-07-23 23:09:35,212:INFO:Uploading results into container
2023-07-23 23:09:35,213:INFO:Uploading model into container now
2023-07-23 23:09:35,214:INFO:_master_model_container: 4
2023-07-23 23:09:35,214:INFO:_display_container: 2
2023-07-23 23:09:35,215:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=42, splitter='best')
2023-07-23 23:09:35,215:INFO:create_model() successfully completed......................................
2023-07-23 23:09:35,334:INFO:SubProcess create_model() end ==================================
2023-07-23 23:09:35,335:INFO:Creating metrics dataframe
2023-07-23 23:09:35,346:INFO:Initializing SVM - Linear Kernel
2023-07-23 23:09:35,346:INFO:Total runtime is 3.7684760689735413 minutes
2023-07-23 23:09:35,351:INFO:SubProcess create_model() called ==================================
2023-07-23 23:09:35,351:INFO:Initializing create_model()
2023-07-23 23:09:35,351:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=svm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:09:35,351:INFO:Checking exceptions
2023-07-23 23:09:35,351:INFO:Importing libraries
2023-07-23 23:09:35,351:INFO:Copying training dataset
2023-07-23 23:09:35,360:INFO:Defining folds
2023-07-23 23:09:35,360:INFO:Declaring metric variables
2023-07-23 23:09:35,365:INFO:Importing untrained model
2023-07-23 23:09:35,369:INFO:SVM - Linear Kernel Imported successfully
2023-07-23 23:09:35,377:INFO:Starting cross validation
2023-07-23 23:09:35,843:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:09:44,075:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,081:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,088:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,162:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:09:44,165:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:09:44,330:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,339:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:09:44,361:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,369:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:09:44,466:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,476:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:09:44,551:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.04s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,558:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:09:44,823:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,832:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:09:44,861:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,870:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:09:44,917:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:09:44,926:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-07-23 23:10:18,115:INFO:Calculating mean and std
2023-07-23 23:10:18,116:INFO:Creating metrics dataframe
2023-07-23 23:10:21,788:INFO:Uploading results into container
2023-07-23 23:10:21,789:INFO:Uploading model into container now
2023-07-23 23:10:21,790:INFO:_master_model_container: 5
2023-07-23 23:10:21,790:INFO:_display_container: 2
2023-07-23 23:10:21,790:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-07-23 23:10:21,791:INFO:create_model() successfully completed......................................
2023-07-23 23:10:21,911:INFO:SubProcess create_model() end ==================================
2023-07-23 23:10:21,911:INFO:Creating metrics dataframe
2023-07-23 23:10:21,925:INFO:Initializing Ridge Classifier
2023-07-23 23:10:21,925:INFO:Total runtime is 4.544790518283844 minutes
2023-07-23 23:10:21,929:INFO:SubProcess create_model() called ==================================
2023-07-23 23:10:21,929:INFO:Initializing create_model()
2023-07-23 23:10:21,930:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:10:21,930:INFO:Checking exceptions
2023-07-23 23:10:21,930:INFO:Importing libraries
2023-07-23 23:10:21,930:INFO:Copying training dataset
2023-07-23 23:10:21,939:INFO:Defining folds
2023-07-23 23:10:21,939:INFO:Declaring metric variables
2023-07-23 23:10:21,945:INFO:Importing untrained model
2023-07-23 23:10:21,949:INFO:Ridge Classifier Imported successfully
2023-07-23 23:10:21,957:INFO:Starting cross validation
2023-07-23 23:10:22,583:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:10:29,878:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:29,883:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:30,104:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:30,105:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:30,108:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:30,108:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:30,251:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:30,258:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:30,693:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:30,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:30,702:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:30,707:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:30,710:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:30,717:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:30,916:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:30,920:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:31,077:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:31,082:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:10:31,279:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:10:31,287:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py:138: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\metrics.py", line 130, in _score
    return super()._score(
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py", line 121, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-07-23 23:11:03,690:INFO:Calculating mean and std
2023-07-23 23:11:03,692:INFO:Creating metrics dataframe
2023-07-23 23:11:07,051:INFO:Uploading results into container
2023-07-23 23:11:07,052:INFO:Uploading model into container now
2023-07-23 23:11:07,052:INFO:_master_model_container: 6
2023-07-23 23:11:07,053:INFO:_display_container: 2
2023-07-23 23:11:07,053:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2023-07-23 23:11:07,054:INFO:create_model() successfully completed......................................
2023-07-23 23:11:07,170:INFO:SubProcess create_model() end ==================================
2023-07-23 23:11:07,170:INFO:Creating metrics dataframe
2023-07-23 23:11:07,181:INFO:Initializing Random Forest Classifier
2023-07-23 23:11:07,181:INFO:Total runtime is 5.299066710472107 minutes
2023-07-23 23:11:07,186:INFO:SubProcess create_model() called ==================================
2023-07-23 23:11:07,186:INFO:Initializing create_model()
2023-07-23 23:11:07,186:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:11:07,186:INFO:Checking exceptions
2023-07-23 23:11:07,186:INFO:Importing libraries
2023-07-23 23:11:07,187:INFO:Copying training dataset
2023-07-23 23:11:07,194:INFO:Defining folds
2023-07-23 23:11:07,194:INFO:Declaring metric variables
2023-07-23 23:11:07,199:INFO:Importing untrained model
2023-07-23 23:11:07,199:INFO:Random Forest Classifier Imported successfully
2023-07-23 23:11:07,211:INFO:Starting cross validation
2023-07-23 23:11:07,658:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:11:12,730:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 23:11:12,931:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 23:11:20,792:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:20,836:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:21,343:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:21,346:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:21,624:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:21,660:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:21,848:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:21,938:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:22,072:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:22,452:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:11:53,480:INFO:Calculating mean and std
2023-07-23 23:11:53,481:INFO:Creating metrics dataframe
2023-07-23 23:11:56,756:INFO:Uploading results into container
2023-07-23 23:11:56,756:INFO:Uploading model into container now
2023-07-23 23:11:56,756:INFO:_master_model_container: 7
2023-07-23 23:11:56,756:INFO:_display_container: 2
2023-07-23 23:11:56,756:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2023-07-23 23:11:56,756:INFO:create_model() successfully completed......................................
2023-07-23 23:11:56,875:INFO:SubProcess create_model() end ==================================
2023-07-23 23:11:56,875:INFO:Creating metrics dataframe
2023-07-23 23:11:56,885:INFO:Initializing Quadratic Discriminant Analysis
2023-07-23 23:11:56,886:INFO:Total runtime is 6.127475146452586 minutes
2023-07-23 23:11:56,890:INFO:SubProcess create_model() called ==================================
2023-07-23 23:11:56,890:INFO:Initializing create_model()
2023-07-23 23:11:56,890:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=qda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:11:56,890:INFO:Checking exceptions
2023-07-23 23:11:56,890:INFO:Importing libraries
2023-07-23 23:11:56,891:INFO:Copying training dataset
2023-07-23 23:11:56,900:INFO:Defining folds
2023-07-23 23:11:56,900:INFO:Declaring metric variables
2023-07-23 23:11:56,904:INFO:Importing untrained model
2023-07-23 23:11:56,908:INFO:Quadratic Discriminant Analysis Imported successfully
2023-07-23 23:11:56,917:INFO:Starting cross validation
2023-07-23 23:11:57,389:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:12:04,630:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:04,696:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:04,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:04,738:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:04,764:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:04,973:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:05,017:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:05,018:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:05,059:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:05,123:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:36,580:INFO:Calculating mean and std
2023-07-23 23:12:36,581:INFO:Creating metrics dataframe
2023-07-23 23:12:40,089:INFO:Uploading results into container
2023-07-23 23:12:40,090:INFO:Uploading model into container now
2023-07-23 23:12:40,090:INFO:_master_model_container: 8
2023-07-23 23:12:40,090:INFO:_display_container: 2
2023-07-23 23:12:40,091:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-07-23 23:12:40,091:INFO:create_model() successfully completed......................................
2023-07-23 23:12:40,207:INFO:SubProcess create_model() end ==================================
2023-07-23 23:12:40,207:INFO:Creating metrics dataframe
2023-07-23 23:12:40,218:INFO:Initializing Ada Boost Classifier
2023-07-23 23:12:40,219:INFO:Total runtime is 6.849692114194234 minutes
2023-07-23 23:12:40,223:INFO:SubProcess create_model() called ==================================
2023-07-23 23:12:40,223:INFO:Initializing create_model()
2023-07-23 23:12:40,223:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:12:40,223:INFO:Checking exceptions
2023-07-23 23:12:40,223:INFO:Importing libraries
2023-07-23 23:12:40,223:INFO:Copying training dataset
2023-07-23 23:12:40,232:INFO:Defining folds
2023-07-23 23:12:40,232:INFO:Declaring metric variables
2023-07-23 23:12:40,236:INFO:Importing untrained model
2023-07-23 23:12:40,242:INFO:Ada Boost Classifier Imported successfully
2023-07-23 23:12:40,249:INFO:Starting cross validation
2023-07-23 23:12:40,712:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:12:50,226:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:50,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:50,412:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:50,596:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:50,699:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:50,703:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:50,749:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:51,004:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:51,009:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:12:51,023:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:21,682:INFO:Calculating mean and std
2023-07-23 23:13:21,684:INFO:Creating metrics dataframe
2023-07-23 23:13:25,019:INFO:Uploading results into container
2023-07-23 23:13:25,020:INFO:Uploading model into container now
2023-07-23 23:13:25,021:INFO:_master_model_container: 9
2023-07-23 23:13:25,021:INFO:_display_container: 2
2023-07-23 23:13:25,021:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=42)
2023-07-23 23:13:25,022:INFO:create_model() successfully completed......................................
2023-07-23 23:13:25,137:INFO:SubProcess create_model() end ==================================
2023-07-23 23:13:25,137:INFO:Creating metrics dataframe
2023-07-23 23:13:25,149:INFO:Initializing Gradient Boosting Classifier
2023-07-23 23:13:25,150:INFO:Total runtime is 7.598545285065969 minutes
2023-07-23 23:13:25,155:INFO:SubProcess create_model() called ==================================
2023-07-23 23:13:25,155:INFO:Initializing create_model()
2023-07-23 23:13:25,156:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=gbc, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:13:25,156:INFO:Checking exceptions
2023-07-23 23:13:25,156:INFO:Importing libraries
2023-07-23 23:13:25,156:INFO:Copying training dataset
2023-07-23 23:13:25,164:INFO:Defining folds
2023-07-23 23:13:25,164:INFO:Declaring metric variables
2023-07-23 23:13:25,169:INFO:Importing untrained model
2023-07-23 23:13:25,173:INFO:Gradient Boosting Classifier Imported successfully
2023-07-23 23:13:25,182:INFO:Starting cross validation
2023-07-23 23:13:25,670:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:13:42,286:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:42,364:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:42,474:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:42,586:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:42,593:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:42,687:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:42,898:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:43,533:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:43,643:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:13:43,748:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:14,856:INFO:Calculating mean and std
2023-07-23 23:14:14,859:INFO:Creating metrics dataframe
2023-07-23 23:14:18,242:INFO:Uploading results into container
2023-07-23 23:14:18,243:INFO:Uploading model into container now
2023-07-23 23:14:18,243:INFO:_master_model_container: 10
2023-07-23 23:14:18,243:INFO:_display_container: 2
2023-07-23 23:14:18,245:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-07-23 23:14:18,245:INFO:create_model() successfully completed......................................
2023-07-23 23:14:18,361:INFO:SubProcess create_model() end ==================================
2023-07-23 23:14:18,362:INFO:Creating metrics dataframe
2023-07-23 23:14:18,374:INFO:Initializing Linear Discriminant Analysis
2023-07-23 23:14:18,374:INFO:Total runtime is 8.485614649454753 minutes
2023-07-23 23:14:18,378:INFO:SubProcess create_model() called ==================================
2023-07-23 23:14:18,379:INFO:Initializing create_model()
2023-07-23 23:14:18,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=lda, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:14:18,379:INFO:Checking exceptions
2023-07-23 23:14:18,379:INFO:Importing libraries
2023-07-23 23:14:18,379:INFO:Copying training dataset
2023-07-23 23:14:18,387:INFO:Defining folds
2023-07-23 23:14:18,388:INFO:Declaring metric variables
2023-07-23 23:14:18,392:INFO:Importing untrained model
2023-07-23 23:14:18,397:INFO:Linear Discriminant Analysis Imported successfully
2023-07-23 23:14:18,405:INFO:Starting cross validation
2023-07-23 23:14:18,874:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:14:26,148:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:26,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:26,373:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:26,479:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:26,552:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:26,788:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:27,058:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:27,428:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:27,510:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:14:57,275:INFO:Calculating mean and std
2023-07-23 23:14:57,277:INFO:Creating metrics dataframe
2023-07-23 23:15:00,764:INFO:Uploading results into container
2023-07-23 23:15:00,764:INFO:Uploading model into container now
2023-07-23 23:15:00,764:INFO:_master_model_container: 11
2023-07-23 23:15:00,764:INFO:_display_container: 2
2023-07-23 23:15:00,764:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-07-23 23:15:00,764:INFO:create_model() successfully completed......................................
2023-07-23 23:15:00,882:INFO:SubProcess create_model() end ==================================
2023-07-23 23:15:00,882:INFO:Creating metrics dataframe
2023-07-23 23:15:00,893:INFO:Initializing Extra Trees Classifier
2023-07-23 23:15:00,893:INFO:Total runtime is 9.194268409411112 minutes
2023-07-23 23:15:00,898:INFO:SubProcess create_model() called ==================================
2023-07-23 23:15:00,899:INFO:Initializing create_model()
2023-07-23 23:15:00,899:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:15:00,899:INFO:Checking exceptions
2023-07-23 23:15:00,899:INFO:Importing libraries
2023-07-23 23:15:00,899:INFO:Copying training dataset
2023-07-23 23:15:00,909:INFO:Defining folds
2023-07-23 23:15:00,909:INFO:Declaring metric variables
2023-07-23 23:15:00,913:INFO:Importing untrained model
2023-07-23 23:15:00,922:INFO:Extra Trees Classifier Imported successfully
2023-07-23 23:15:00,937:INFO:Starting cross validation
2023-07-23 23:15:01,484:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:15:04,539:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-07-23 23:15:05,917:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 23:15:06,315:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 23:15:06,891:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 23:15:08,252:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 23:15:08,425:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 23:15:08,613:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-07-23 23:15:11,150:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:11,150:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:11,527:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:12,541:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:12,669:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:13,453:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:14,478:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.43s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:14,954:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:15,271:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:15,299:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:15:49,885:INFO:Calculating mean and std
2023-07-23 23:15:49,885:INFO:Creating metrics dataframe
2023-07-23 23:15:53,810:INFO:Uploading results into container
2023-07-23 23:15:53,810:INFO:Uploading model into container now
2023-07-23 23:15:53,810:INFO:_master_model_container: 12
2023-07-23 23:15:53,810:INFO:_display_container: 2
2023-07-23 23:15:53,815:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=42, verbose=0, warm_start=False)
2023-07-23 23:15:53,815:INFO:create_model() successfully completed......................................
2023-07-23 23:15:53,983:INFO:SubProcess create_model() end ==================================
2023-07-23 23:15:53,983:INFO:Creating metrics dataframe
2023-07-23 23:15:54,003:INFO:Initializing Extreme Gradient Boosting
2023-07-23 23:15:54,003:INFO:Total runtime is 10.079422946770986 minutes
2023-07-23 23:15:54,003:INFO:SubProcess create_model() called ==================================
2023-07-23 23:15:54,003:INFO:Initializing create_model()
2023-07-23 23:15:54,003:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002043A3ABA30>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002041DEE2890>, model_only=True, return_train_score=False, kwargs={})
2023-07-23 23:15:54,003:INFO:Checking exceptions
2023-07-23 23:15:54,003:INFO:Importing libraries
2023-07-23 23:15:54,003:INFO:Copying training dataset
2023-07-23 23:15:54,015:INFO:Defining folds
2023-07-23 23:15:54,015:INFO:Declaring metric variables
2023-07-23 23:15:54,024:INFO:Importing untrained model
2023-07-23 23:15:54,035:INFO:Extreme Gradient Boosting Imported successfully
2023-07-23 23:15:54,046:INFO:Starting cross validation
2023-07-23 23:15:54,584:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-07-23 23:16:09,726:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:09,769:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:09,801:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:09,869:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:09,897:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:10,110:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:10,174:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:10,220:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:10,275:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:10,467:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 2.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-07-23 23:16:50,970:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\pycaret\utils\generic.py:831: DeprecationWarning:

invalid escape sequence '\P'


2023-07-23 23:17:17,574:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:35: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,628:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:54: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,630:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:63: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,630:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:69: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,630:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_clustering.py:77: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,633:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:5: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,634:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:10: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,634:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:15: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,634:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\links.py:20: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,634:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:363: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,635:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:385: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,635:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:428: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,635:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\utils\_masked_model.py:439: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:186: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,649:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_tabular.py:197: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,652:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\maskers\_image.py:175: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,676:WARNING:c:\Users\jigna\MyGitHub_Proyects\Laboratorios MDS7021\.venv\lib\site-packages\shap\explainers\_partition.py:676: NumbaDeprecationWarning:

[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m


2023-07-23 23:17:17,685:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 23:17:17,685:WARNING:[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m

2023-07-23 23:17:17,764:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 23:17:17,768:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 23:17:17,777:WARNING:Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display

2023-07-23 23:17:19,813:WARNING:X does not have valid feature names, but QuantileTransformer was fitted with feature names

